\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bepsilon{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Hierarchical Clustering

## Basic Concepts 

Suppose we have a set of six observations: 

```{r include = FALSE}
  par(mar=rep(0.3, 4))
```

```{r fig.dim = c(3, 3), out.width = "30%", echo = FALSE}
  set.seed(3)
  x = replicate(2, rnorm(6))
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61))
  text(x[,1], x[, 2], c(1:6))
```

The goal is to progressively group them together until there is only one group. During this process, we will always choose the closest two groups (some may be individuals) to merge. 

```{r include = FALSE}
  par(mfrow=c(1,4))
  par(mar=c(0.3, 0.3, 2, 0.3))
```

```{r, fig.dim = c(10, 2.5), out.width = "100%", echo = FALSE}
  library(plotrix)

  # Step 1
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), main = "Step 1")
  text(x[,1], x[, 2], c(1:6), lwd =2)
  draw.ellipse(0.1444561, -1.1750380, a = 0.2, b = 0.25, 
               angle = 60, border = "darkorange", lwd = 2)
  
  # Step 2
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5,
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), main = "Step 2")
  text(x[,1], x[, 2], c(1:6), lwd =2)
  draw.ellipse(0.1444561, -1.1750380, a = 0.2, b = 0.25, 
               angle = 60, border = "darkorange", lwd = 2)
  draw.circle(0.161565, -1.031619, 0.3, border = "darkorange", lwd = 2)
  
  # Step 3
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5,
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), main = "Step 3")
  text(x[,1], x[, 2], c(1:6), lwd =2)
  draw.ellipse(0.1444561, -1.1750380, a = 0.2, b = 0.25, 
               angle = 60, border = "darkorange", lwd = 2)
  draw.circle(0.161565, -1.031619, 0.3, border = "darkorange", lwd = 2)    
  draw.ellipse(-0.7223288, 1.1919895, a = 0.5, b = 0.25, 
               angle = 170, border = "darkorange", lwd = 2)

  # Step 4
  plot(x[,1], x[, 2], xaxt = 'n', yaxt = 'n', cex = 2.5, 
       xlim = c(-1.35, 0.45), ylim = c(-1.51, 1.61), main = "Step 4")
  text(x[,1], x[, 2], c(1:6), lwd =2)
  draw.ellipse(0.1444561, -1.1750380, a = 0.2, b = 0.25, 
               angle = 60, border = "darkorange", lwd = 2)
  draw.circle(0.161565, -1.031619, 0.3, border = "darkorange", lwd = 2)   
  draw.ellipse(-0.7223288, 1.1919895, a = 0.5, b = 0.25, 
               angle = 170, border = "darkorange", lwd = 2)    
  draw.circle(-0.74, 0.8331322, 0.57, border = "darkorange", lwd = 2)
```

If we evaluate the distance between two observations, that would be very easy. For example, the Euclidean distance and Hamming distance can be used. But what about the distance between two groups? Suppose we have two groups of observations $G$ and $H$, then several distance metric can be considered:

  * __Complete linkage__: the furthest pair 
  $$d(G, H) = \underset{i \in G, \, j \in G}{\max} d(x_i, x_j)$$
  * __Single linkage__: the closest pair
  $$d(G, H) = \underset{i \in G, \, j \in G}{\min} d(x_i, x_j)$$
  * __Average linkage__: average distance
  $$d(G, H) = \frac{1}{n_G n_H} \sum_{i \in G} \sum_{i \in H} d(x_i, x_j)$$

The `R` function `hclust()` uses the complete linkage as default. To perform a hierarchical clustering, we need to know all the pair-wise distances, i.e., $d(x_i, x_j)$. Let's consider the Euclidean distance. 

```{r}
  # the Euclidean distance can be computed using dist()
  as.matrix(dist(x))
```

We use this distance matrix in the hierarchical clustering algorithm `hclust()`. The `plot()` function will display the merging process. This should be exactly the same as we demonstrated previously. 

```{r include = FALSE}
  par(mfrow=c(1,1))
  par(mar=rep(2, 4))
```

```{r fig.dim = c(6, 6), out.width = "45%", echo = FALSE}
  # pass the distance matrix to hclust()
  # we use a complete link function
  hcfit <- hclust(dist(x), method = "complete")
  plot(hcfit)
```

The height of each split represents how separated the two subsets are (the distance when they are merged). Selecting the number of clusters is still a tricky problem. Usually, we pick a cutoff where the height of the next split is short. Hence, the above example fits well with two clusters.  

## Example 1: `iris` data

The `iris` data contains three clusters and four variables. We use all variables in the distance calculation and use the default complete linkage. 

```{r fig.width=8, fig.height=6, out.width = '40%'}
  iris_hc <- hclust(dist(iris[, 3:4]))
  plot(iris_hc)
```

This does not seem to perform very well, considering that we know the true number of classes is three. This shows that, in practice, the detected clusters can heavily depend on the variables you use. Let's try some other linkage functions. 

```{r fig.width=8, fig.height=6, out.width = '40%'}
  iris_hc <- hclust(dist(iris[, 3:4]), method = "average")
  plot(iris_hc, hang = -1)
```

This looks better, at least more consistent with the truth. Now we can also consider using other package to plot this result. For example, the `ape` package provides some interesting choices. 

```{r fig.width=5, fig.height=4, out.width = '45%'}
  library(ape)
  plot(as.phylo(iris_hc), type = "unrooted", cex = 0.6, no.margin = TRUE)
```

We can also add the true class colors to the plot. This plot is motivated by the `dendextend` package vignettes. Of course in a realistic situation, we wouldn't know what the true class is. 

```{r, fig.width=5, fig.height=4, out.width = '60%', echo = FALSE, message=FALSE}
  library(colorspace) 
  library(dendextend)
  
  dend <- as.dendrogram(iris_hc)
  # order it the closest we can to the order of the observations:
  dend <- rotate(dend, 1:150)
  
  # Color the branches based on the clusters:
  dend <- color_branches(dend, k=3) #, groupLabels=iris_species)
  
  # Manually match the labels, as much as possible, to the real classification of the flowers:
  labels_colors(dend) <-
     rainbow_hcl(3)[sort_levels_values(
        as.numeric(iris[,5])[order.dendrogram(dend)]
     )]
  
  # We shall add the flower type to the labels:
  labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                             "(",labels(dend),")", 
                             sep = "")
  # We hang the dendrogram a bit:
  dend <- hang.dendrogram(dend,hang_height=0.1)
  # reduce the size of the labels:
  # dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
  dend <- set(dend, "labels_cex", 0.5)
  # And plot:
  par(mar = c(3,1,1,5))
  plot(dend, horiz =  TRUE,  nodePar = list(cex = .007))
  legend("topleft", legend = levels(iris[,5]), fill = rainbow_hcl(3))
```

## Example 2: RNA Expression Data

We use a tissue gene expression dataset from the `tissuesGeneExpression` library, available from bioconductor. I prepared the data to include only 100 genes. You can download the data from the course website. In this first step, we simply plot the data using a heatmap. By default, a heatmap uses red to denote higher values, and yellow for lower values. Note that we first plot the data without organizing the columns or rows. The data is also standardized based on columns (genes). 

```{r}
    load("data/tissue.Rda")
    dim(expression)
    table(tissue)
    head(expression[, 1:3])
    heatmap(scale(expression), Rowv = NA, Colv = NA)
```

Hierarchical clustering may help us discover interesting patterns. If we reorganize the columns and rows based on the clusters, then it may reveal underlying subclasses of issues, or subgroups of genes. 

```{r}
    heatmap(scale(expression))
```

Note that there are many other `R` packages that produce more interesting plots. For example, you can try the [heatmaply](https://cran.r-project.org/web/packages/heatmaply/vignettes/heatmaply.html) package.
