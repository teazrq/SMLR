```{r, echo=FALSE, results='asis'}
  # import macros
  cat("<div style='display:none;'>")
  cat(paste(scan("mathjax_header.html", what = character(), sep = "\n", quiet = TRUE), collapse = "\n"))
  cat("</div>")
```

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Gradient Boosting Machines

The gradient boosting machine [GBM, @friedman2001greedy] is a powerful machine learning algorithm that can be used for any (generalized) regression task. It builds an ensemble of weak learners, typically decision trees, in a stage-wise manner to create a strong predictive model. The logic is similar to AdaBoost, but the way it defines the weak learners is different. The key idea behind GBM is to iteratively add models that correct the errors of the previous models by optimizing a specified loss function.

## Gradient Boosting

Let's take an alternative view of this problem: we use an additive structure to fit models
\[
F_T(x) = \sum_{t = 1}^T \alpha_t f(x; \btheta_t)
\]
by minimizing a loss function
\[
\underset{\{\alpha_t, \btheta_t\}_{t=1}^T}{\min} \sum_{i=1}^n L\big(y_i, F_T(x_i)\big).
\]
In this framework, we may choose a loss function \(L\) that is suitable for the problem, and also choose the base learner \(f(x; \btheta)\) with parameter \(\btheta\). Examples include linear functions, splines, trees, etc. While it may be difficult to minimize over all parameters \(\{\alpha_t, \btheta_t\}_{t=1}^T\), we may consider doing this in a stage-wise fashion. The algorithm could work in the following way:

- Set \(F_0(x) = 0\).
- For \(t = 1, \ldots, T\)
  + Choose \((\alpha_t, \btheta_t)\) to minimize the loss
    \[
    \underset{\alpha, \btheta}{\min}\ \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha\, f(x_i; \btheta)\big).
    \]
  + Update \(F_t(x) = F_{t-1}(x) + \alpha_t\, f(x; \btheta_t)\).
- Output \(F_T(x)\) as the final model.

The previous AdaBoost example is using the exponential loss function. Also, it doesn't pick an optimal \(f(x; \btheta)\) at each step; we just need a model that is better than random. The step size \(\alpha_t\) is optimized at each \(t\) given the fitted \(f(x; \btheta_t)\).

Another example is the forward stage-wise linear regression. In this case, we fit a single-variable linear model at each step \(t\):
\[
f(x; j) = \operatorname{sign}\!\big(\operatorname{Cor}(X_j, \br)\big)\, X_j,
\]
- \(\br\) is the residual, \(r_i = y_i - F_{t-1}(x_i)\).
- \(j\) is the index that has the largest absolute correlation with \(\br\).

Then we give a very small step size \(\alpha_t\) (say, \(\alpha_t = 10^{-5}\)) with sign equal to the correlation between \(X_j\) and \(\br\). In this case, \(F_t(x)\) is almost equivalent to the Lasso solution path as \(t\) increases.

We may notice that \(r_i\) is in fact the negative gradient of the squared-error loss as a function of the fitted function:
\[
r_{it} = - \left[ \frac{\partial \, \big(y_i - F(x_i)\big)^2 }{\partial \, F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)},
\]
and we are essentially fitting a weak learner \(f_t(x)\) to the residuals and updating the fitted model \(F_t(x)\). The following example shows the result of using a tree learner as \(f_t(x)\):

```{r include=FALSE}
par(mar=c(2,2,2,2))
par(mfrow=c(1,1))
set.seed(1)
```

```{r out.width="45%"}
library(gbm)

# a simple regression problem
x <- seq(0, 1, 0.001)
fx <- function(x) 2*sin(3*pi*x)
y <- fx(x) + rnorm(length(x))

plot(x, y, pch = 19, ylab = "y", col = "gray", cex = 0.5)
# plot the true regression line
lines(x, fx(x), lwd = 2, col = "deepskyblue")
```

We can see that the fitted model progressively approximates the true function.

```{r out.width="90%", fig.dim=c(12,8)}
# fit regression boosting
# (Using a large shrinkage here to visualize stagewise fits; in practice use 0.1 or smaller.)
gbm.fit <- gbm(y ~ x, data = data.frame(x, y), distribution = "gaussian",
               n.trees = 300, shrinkage = 0.5, bag.fraction = 0.8)

# plot the fitted regression function at several iterations
par(mfrow=c(2,3))
size <- c(1, 5, 10, 50, 100, 300)

for (i in 1:6) {
  par(mar=c(2,2,3,1))
  plot(x, y, pch = 19, ylab = "y", col = "gray", cex = 0.5)
  lines(x, fx(x), lwd = 2, col = "deepskyblue")
  # additive score F(x)
  Fx <- predict(gbm.fit, n.trees = size[i])
  lines(x, Fx, lwd = 3, col = "darkorange")
  title(paste("# of Iterations = ", size[i]))
}
```

This idea can be generalized to any loss function \(L\). This is the **gradient boosting** model:

- At each iteration \(t\), calculate “pseudo-residuals”, i.e., the negative gradient for each observation
  \[
  g_{it} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}.
  \]
- Fit \(f_t(x; \btheta_t)\) to pseudo-residuals \(g_{it}\).
- Search for the best step length
  \[
  \alpha_t = \underset{\alpha}{\arg\min}\ \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha\, f(x_i; \btheta_t)\big).
  \]
- Update \(F_t(x) = F_{t-1}(x) + \alpha_t\, f(x; \btheta_t)\).

Hence, the only change when modeling different outcomes is to choose the loss function \(L\) and derive the pseudo-residuals:

- For regression, the loss is \(\tfrac{1}{2}\big(y - F(x)\big)^2\), and the pseudo-residual is \(y_i - F(x_i)\).
- For quantile regression to model the median, the loss is \(|y - F(x)|\), and the pseudo-residual is \(\operatorname{sign}\!\big(y_i - F(x_i)\big)\).
- For classification, we can use the negative log-likelihood of a single observation \(-[ y\log(p) + (1-y)\log(1-p) ]\), and express \(p\) via the log-odds of a score \(F\), i.e., \(F = \log\!\big(p/(1-p)\big)\). Then the pseudo-residual is \(y_i - p(x_i)\).

## Gradient Boosting with Logistic Link

To see how the pseudo-residual of a classification model is derived, let's use the logistic link with the predicted probability \(p\) defined as:
\[
p = \frac{e^{F(x)}}{1 + e^{F(x)}} = \frac{1}{1 + e^{-F(x)}}.
\]

The negative log-likelihood for the Bernoulli distribution for a single instance is:
\[
L(y, p) = - \big[y \log(p) + (1 - y) \log(1 - p)\big].
\]

First, the partial derivative of this loss with respect to \(p\) is:
\[
\frac{\partial L(y, p)}{\partial p} = -\frac{y}{p} + \frac{1-y}{1-p}.
\]

The derivative of \(p\) with respect to \(F(x)\) is:
\[
\frac{\partial p}{\partial F(x)} = p(1-p).
\]

Hence, the partial derivative of the loss with respect to \(F\) is:
\[
\frac{\partial L(y, p)}{\partial F(x)}
= \left(-\frac{y}{p} + \frac{1-y}{1-p}\right)\, p(1-p)
= -y(1-p) + (1-y)p
= p - y.
\]

Gradient boosting uses the **negative** gradient as the working response, so the pseudo-residual is
\[
g_{it} = -\left.\frac{\partial L}{\partial F(x_i)}\right|_{F=F_{t-1}} = y_i - p_{t-1}(x_i),
\]
which we use to fit the next tree / linear booster. The sign determines the direction of the update and is handled by using the negative gradient.
