<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 20 Boosting | Statistical Machine Learning with R</title>
  <meta name="description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 20 Boosting | Statistical Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 20 Boosting | Statistical Machine Learning with R" />
  
  <meta name="twitter:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2025-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="random-forests.html"/>
<link rel="next" href="k-means.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visual-studio-code.html"><a href="visual-studio-code.html"><i class="fa fa-check"></i><b>3</b> Visual Studio Code</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visual-studio-code.html"><a href="visual-studio-code.html#basics-and-resources-1"><i class="fa fa-check"></i><b>3.1</b> Basics and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linear-regression"><i class="fa fa-check"></i><b>4.2</b> Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#matrix-inversion"><i class="fa fa-check"></i><b>4.3</b> Matrix Inversion</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linearalgebra-SM"><i class="fa fa-check"></i><b>4.3.1</b> Rank-one Update</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#rank-k-update"><i class="fa fa-check"></i><b>4.3.2</b> Rank-<span class="math inline">\(k\)</span> Update</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#times-2-block-matrix-inversion"><i class="fa fa-check"></i><b>4.3.3</b> 2 <span class="math inline">\(\times\)</span> 2 Block Matrix Inversion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>5</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>5.1</b> Basic Concept</a></li>
<li class="chapter" data-level="5.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>5.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="5.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>5.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="5.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>5.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="5.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>5.5</b> Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>5.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>5.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="5.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>5.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>5.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>5.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>5.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>5.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>6</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>6.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>6.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>6.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>6.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>6.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>6.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>6.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>6.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>6.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>6.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>6.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>6.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>7</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>7.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="7.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>7.2</b> Ridge Penalty and the Reduced Variation</a></li>
<li class="chapter" data-level="7.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>7.3</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="7.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>7.4</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="7.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>7.5</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>7.5.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="7.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>7.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>7.6</b> Cross-validation</a></li>
<li class="chapter" data-level="7.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.7</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>7.7.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>7.8</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>7.8.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8</b> Lasso</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>8.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>8.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="8.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>8.3</b> The Solution Path</a></li>
<li class="chapter" data-level="8.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>8.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="8.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>8.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="8.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>8.6</b> Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>9</b> Spline</a>
<ul>
<li class="chapter" data-level="9.1" data-path="spline.html"><a href="spline.html#using-linear-models-for-nonlinear-trends"><i class="fa fa-check"></i><b>9.1</b> Using Linear models for Nonlinear Trends</a></li>
<li class="chapter" data-level="9.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>9.2</b> A Motivating Example and Polynomials</a></li>
<li class="chapter" data-level="9.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>9.3</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="9.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>9.4</b> Splines</a></li>
<li class="chapter" data-level="9.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>9.5</b> Spline Basis</a></li>
<li class="chapter" data-level="9.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>9.6</b> Natural Cubic Spline</a></li>
<li class="chapter" data-level="9.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>9.7</b> Smoothing Spline</a></li>
<li class="chapter" data-level="9.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>9.8</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="9.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>9.9</b> Extending Splines to Multiple Varibles</a></li>
</ul></li>
<li class="part"><span><b>III Linear Classification Models</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>10.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>10.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>10.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>10.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>11</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>11.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="11.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>11.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="11.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>11.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="11.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>11.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="11.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>11.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>11.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Models</b></span></li>
<li class="chapter" data-level="12" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>12</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>12.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="12.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>12.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="12.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>12.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="12.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>12.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="12.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#degrees-of-freedom-1"><i class="fa fa-check"></i><b>12.6</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="12.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>12.7</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="12.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>12.8</b> Distance Measures</a></li>
<li class="chapter" data-level="12.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>12.9</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="12.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>12.10</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="12.11" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>12.11</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>13</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>13.1</b> KNN vs. Kernel</a></li>
<li class="chapter" data-level="13.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>13.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="13.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>13.3</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="13.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>13.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off-1"><i class="fa fa-check"></i><b>13.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>13.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="13.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>13.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="13.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>13.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="13.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>13.8</b> R Implementations</a></li>
</ul></li>
<li class="part"><span><b>V Kernel Machines</b></span></li>
<li class="chapter" data-level="14" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>14</b> Reproducing Kernel Hilbert Space</a>
<ul>
<li class="chapter" data-level="14.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-motivation"><i class="fa fa-check"></i><b>14.1</b> The Motivation</a></li>
<li class="chapter" data-level="14.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#hilbert-space-preliminaries"><i class="fa fa-check"></i><b>14.2</b> Hilbert Space Preliminaries</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-space-of-square-integrable-functions"><i class="fa fa-check"></i><b>14.2.1</b> The Space of Square-Integrable Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-kernel-function"><i class="fa fa-check"></i><b>14.3</b> A Kernel Function</a></li>
<li class="chapter" data-level="14.4" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-space-of-functions"><i class="fa fa-check"></i><b>14.4</b> A Space of Functions</a></li>
<li class="chapter" data-level="14.5" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-inner-product"><i class="fa fa-check"></i><b>14.5</b> The Inner Product</a></li>
<li class="chapter" data-level="14.6" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-rkhs"><i class="fa fa-check"></i><b>14.6</b> The RKHS</a></li>
<li class="chapter" data-level="14.7" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-reproducing-property"><i class="fa fa-check"></i><b>14.7</b> The Reproducing Property</a></li>
<li class="chapter" data-level="14.8" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#smoothness"><i class="fa fa-check"></i><b>14.8</b> Smoothness</a></li>
<li class="chapter" data-level="14.9" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-moorearonszajn-theorem"><i class="fa fa-check"></i><b>14.9</b> The Moore–Aronszajn Theorem</a></li>
<li class="chapter" data-level="14.10" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#examples"><i class="fa fa-check"></i><b>14.10</b> Examples</a>
<ul>
<li class="chapter" data-level="14.10.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#brownian-motion-kernel"><i class="fa fa-check"></i><b>14.10.1</b> Brownian Motion Kernel</a></li>
<li class="chapter" data-level="14.10.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#non-positive-definite-kernel"><i class="fa fa-check"></i><b>14.10.2</b> Non-positive Definite Kernel</a></li>
<li class="chapter" data-level="14.10.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#defining-new-kernels"><i class="fa fa-check"></i><b>14.10.3</b> Defining New Kernels</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>15</b> Kernel Ridge Regression</a>
<ul>
<li class="chapter" data-level="15.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#linear-regression-as-a-constraint-optimization"><i class="fa fa-check"></i><b>15.1</b> Linear Regression as a Constraint Optimization</a></li>
<li class="chapter" data-level="15.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#the-kernel-ridge-regression"><i class="fa fa-check"></i><b>15.2</b> The Kernel Ridge Regression</a></li>
<li class="chapter" data-level="15.3" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#ridge-regression-as-a-linear-kernel-model"><i class="fa fa-check"></i><b>15.3</b> Ridge Regression as a Linear Kernel Model</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>16</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="16.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>16.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="16.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>16.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>16.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>16.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="16.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>16.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="16.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>16.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="16.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>16.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="16.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>16.7</b> SVM as a Penalized Model</a></li>
<li class="chapter" data-level="16.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>16.8</b> Kernel and Feature Maps: Another Example</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html"><i class="fa fa-check"></i><b>17</b> The Representer Theorem</a>
<ul>
<li class="chapter" data-level="17.1" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#the-representer-theorem-1"><i class="fa fa-check"></i><b>17.1</b> The Representer Theorem</a></li>
<li class="chapter" data-level="17.2" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#notes-on-application"><i class="fa fa-check"></i><b>17.2</b> Notes on Application</a></li>
</ul></li>
<li class="part"><span><b>VI Trees and Ensembles</b></span></li>
<li class="chapter" data-level="18" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>18</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="18.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>18.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="18.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>18.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="18.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>18.3</b> Regression Trees</a></li>
<li class="chapter" data-level="18.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>18.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="18.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>18.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>19</b> Random Forests</a>
<ul>
<li class="chapter" data-level="19.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>19.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="19.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>19.2</b> Random Forests</a></li>
<li class="chapter" data-level="19.3" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forests"><i class="fa fa-check"></i><b>19.3</b> Kernel view of Random Forests</a></li>
<li class="chapter" data-level="19.4" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>19.4</b> Variable Importance</a></li>
<li class="chapter" data-level="19.5" data-path="random-forests.html"><a href="random-forests.html#adaptiveness-of-random-forest-kernel"><i class="fa fa-check"></i><b>19.5</b> Adaptiveness of Random Forest Kernel</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>20</b> Boosting</a>
<ul>
<li class="chapter" data-level="20.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>20.1</b> AdaBoost</a></li>
<li class="chapter" data-level="20.2" data-path="boosting.html"><a href="boosting.html#training-error-of-adaboost"><i class="fa fa-check"></i><b>20.2</b> Training Error of AdaBoost</a></li>
<li class="chapter" data-level="20.3" data-path="boosting.html"><a href="boosting.html#tuning-the-number-of-trees"><i class="fa fa-check"></i><b>20.3</b> Tuning the Number of Trees</a></li>
<li class="chapter" data-level="20.4" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>20.4</b> Gradient Boosting</a></li>
<li class="chapter" data-level="20.5" data-path="boosting.html"><a href="boosting.html#gradient-boosting-with-logistic-link"><i class="fa fa-check"></i><b>20.5</b> Gradient Boosting with Logistic Link</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Learning</b></span></li>
<li class="chapter" data-level="21" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>21</b> K-Means</a>
<ul>
<li class="chapter" data-level="21.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>21.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="21.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>21.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="21.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>21.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>22</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="22.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>22.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="22.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>22.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="22.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>22.3</b> Example 2: RNA Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>23</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="23.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>23.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>23.1.1</b> Note: Scaling</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>23.2</b> Example 1: <code>iris</code> Data</a></li>
<li class="chapter" data-level="23.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>23.3</b> Example 2: Handwritten Digits</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>24</b> Self-Organizing Map</a>
<ul>
<li class="chapter" data-level="24.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>24.1</b> Basic Concepts</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>25</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="25.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#an-example"><i class="fa fa-check"></i><b>25.1</b> An Example</a></li>
<li class="chapter" data-level="25.2" data-path="spectral-clustering.html"><a href="spectral-clustering.html#adjacency-matrix"><i class="fa fa-check"></i><b>25.2</b> Adjacency Matrix</a></li>
<li class="chapter" data-level="25.3" data-path="spectral-clustering.html"><a href="spectral-clustering.html#laplacian-matrix"><i class="fa fa-check"></i><b>25.3</b> Laplacian Matrix</a></li>
<li class="chapter" data-level="25.4" data-path="spectral-clustering.html"><a href="spectral-clustering.html#derivation-of-the-feature-embedding"><i class="fa fa-check"></i><b>25.4</b> Derivation of the Feature Embedding</a></li>
<li class="chapter" data-level="25.5" data-path="spectral-clustering.html"><a href="spectral-clustering.html#feature-embedding"><i class="fa fa-check"></i><b>25.5</b> Feature Embedding</a></li>
<li class="chapter" data-level="25.6" data-path="spectral-clustering.html"><a href="spectral-clustering.html#clustering-with-embedded-features"><i class="fa fa-check"></i><b>25.6</b> Clustering with Embedded Features</a></li>
<li class="chapter" data-level="25.7" data-path="spectral-clustering.html"><a href="spectral-clustering.html#normalized-graph-laplacian"><i class="fa fa-check"></i><b>25.7</b> Normalized Graph Laplacian</a></li>
<li class="chapter" data-level="25.8" data-path="spectral-clustering.html"><a href="spectral-clustering.html#using-a-different-adjacency-matrix"><i class="fa fa-check"></i><b>25.8</b> Using a Different Adjacency Matrix</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html"><i class="fa fa-check"></i><b>26</b> Uniform Manifold Approximation and Projection</a>
<ul>
<li class="chapter" data-level="26.1" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#an-example-1"><i class="fa fa-check"></i><b>26.1</b> An Example</a></li>
<li class="chapter" data-level="26.2" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#tuning"><i class="fa fa-check"></i><b>26.2</b> Tuning</a></li>
<li class="chapter" data-level="26.3" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#another-example"><i class="fa fa-check"></i><b>26.3</b> Another Example</a></li>
</ul></li>
<li class="part"><span><b>VIII Reference</b></span></li>
<li class="chapter" data-level="27" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>27</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2023 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="boosting" class="section level1 hasAnchor" number="20">
<h1><span class="header-section-number">Chapter 20</span> Boosting<a href="boosting.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Boosting is another ensemble model, created in the form of</p>
<p><span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)\]</span></p>
<p>However, it is different from random forest, in which each <span class="math inline">\(f_t(x)\)</span> is learned parallelly. These <span class="math inline">\(f_t(x)\)</span>’s are called weak learners and are constructed <strong>sequentially</strong>, with coefficients <span class="math inline">\(\alpha_t\)</span>’s to represent their weights. The most classical model, AdaBoost was proposed by <span class="citation">Freund and Schapire (<a href="#ref-freund1997decision">1997</a>)</span> for classification problems, and a more statically view of this type of model called gradient boosting machines <span class="citation">(<a href="#ref-friedman2001greedy">J. H. Friedman 2001</a>)</span> can handle any loss function we commonly use. We will first introduce AdaBoost and then discuss gradient boosting.</p>
<div id="adaboost" class="section level2 hasAnchor" number="20.1">
<h2><span class="header-section-number">20.1</span> AdaBoost<a href="boosting.html#adaboost" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Following our common notation, we observe a set of data <span class="math inline">\(\{\mathbf{x}_i, y_i\}_{i=1}^n\)</span>. Similar to SVM, we code <span class="math inline">\(y_i\)</span>s as <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>. The AdaBoost works by creating <span class="math inline">\(F_T(x)\)</span> sequentially and use <span class="math inline">\(\text{sign}(F_T(x))\)</span> as the classification rule. The algorithm is given in the following:</p>
<ul>
<li>Initiate weights <span class="math inline">\(w_i^{(1)} = 1/n\)</span>, for <span class="math inline">\(i = 1, \ldots, n\)</span></li>
<li>For <span class="math inline">\(t = 1, \ldots, T\)</span>, do
<ul>
<li>Fit a classifier <span class="math inline">\(f_t(x)\)</span> to the training data with subject weights <span class="math inline">\(w_i^{(t)}\)</span>’s.</li>
<li>Compute the weighed error rate
<span class="math display">\[\epsilon_t = \sum_{i=1}^n w_i^{(t)} \mathbf{1}\{y_i \neq f_t(x_i) \}\]</span></li>
<li>Compute
<span class="math display">\[\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}\]</span></li>
<li>Update subject weights
<span class="math display">\[w_i^{(t + 1)} = \frac{1}{Z_t} w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\]</span>
where <span class="math inline">\(Z_t\)</span> is a normalizing constant make <span class="math inline">\(w_i^{(t + 1)}\)</span>’s sum up to 1:
<span class="math display">\[Z_t = \sum_{i=1}^n w_i^{(t)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\]</span></li>
</ul></li>
<li>Output the final model
<span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f_t(x)\]</span>
and the decision rule is <span class="math inline">\(\text{sign}(F_T(x))\)</span>.</li>
</ul>
<p>An important mechanism in AdaBoost is the weight update step. We can notice that the weight is increased if <span class="math inline">\(\exp\big\{ - \alpha_t y_i f_t(x_i) \big\}\)</span> is larger than 1. This is simply when <span class="math inline">\(y_i f_t(x_i)\)</span> is negative, i.e., subject <span class="math inline">\(i\)</span> got mis-classified by <span class="math inline">\(f_t\)</span> at this iteration. Hence, during the next iteration <span class="math inline">\(t+1\)</span>, the model <span class="math inline">\(f_{(t+1)}\)</span> will more likely to address this subject. Here, <span class="math inline">\(f_t\)</span> can be any classification model, for example, we could use a tree model. The following figures demonstrate this idea of updating weights and aggregate the learners.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="boosting.html#cb168-1" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)</span>
<span id="cb168-2"><a href="boosting.html#cb168-2" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>,</span>
<span id="cb168-3"><a href="boosting.html#cb168-3" tabindex="-1"></a>         <span class="fl">0.8</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>)</span>
<span id="cb168-4"><a href="boosting.html#cb168-4" tabindex="-1"></a>  </span>
<span id="cb168-5"><a href="boosting.html#cb168-5" tabindex="-1"></a>  <span class="co"># the data</span></span>
<span id="cb168-6"><a href="boosting.html#cb168-6" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, </span>
<span id="cb168-7"><a href="boosting.html#cb168-7" tabindex="-1"></a>        <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb168-8"><a href="boosting.html#cb168-8" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">cbind</span>(<span class="st">&quot;x1&quot;</span> <span class="ot">=</span> x1, <span class="st">&quot;x2&quot;</span> <span class="ot">=</span> x2)</span>
<span id="cb168-9"><a href="boosting.html#cb168-9" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="st">&quot;x1&quot;</span> <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">1.1</span>, <span class="fl">0.01</span>), <span class="st">&quot;x2&quot;</span> <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.9</span>, <span class="fl">0.01</span>))</span>
<span id="cb168-10"><a href="boosting.html#cb168-10" tabindex="-1"></a>  </span>
<span id="cb168-11"><a href="boosting.html#cb168-11" tabindex="-1"></a>  <span class="co"># plot data</span></span>
<span id="cb168-12"><a href="boosting.html#cb168-12" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb168-13"><a href="boosting.html#cb168-13" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb168-14"><a href="boosting.html#cb168-14" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb168-15"><a href="boosting.html#cb168-15" tabindex="-1"></a>  </span>
<span id="cb168-16"><a href="boosting.html#cb168-16" tabindex="-1"></a>  <span class="co"># fit gbm with 3 trees</span></span>
<span id="cb168-17"><a href="boosting.html#cb168-17" tabindex="-1"></a>  <span class="fu">library</span>(gbm)</span>
<span id="cb168-18"><a href="boosting.html#cb168-18" tabindex="-1"></a><span class="do">## Loaded gbm 2.2.2</span></span>
<span id="cb168-19"><a href="boosting.html#cb168-19" tabindex="-1"></a><span class="do">## This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3</span></span>
<span id="cb168-20"><a href="boosting.html#cb168-20" tabindex="-1"></a>  gbm.fit <span class="ot">=</span> <span class="fu">gbm</span>(y <span class="sc">~</span>., <span class="fu">data.frame</span>(x1, x2, <span class="at">y=</span> <span class="fu">as.numeric</span>(y <span class="sc">==</span> <span class="dv">1</span>)), </span>
<span id="cb168-21"><a href="boosting.html#cb168-21" tabindex="-1"></a>                <span class="at">distribution=</span><span class="st">&quot;adaboost&quot;</span>, <span class="at">interaction.depth =</span> <span class="dv">1</span>, </span>
<span id="cb168-22"><a href="boosting.html#cb168-22" tabindex="-1"></a>                <span class="at">n.minobsinnode =</span> <span class="dv">1</span>, <span class="at">n.trees =</span> <span class="dv">3</span>, </span>
<span id="cb168-23"><a href="boosting.html#cb168-23" tabindex="-1"></a>                <span class="at">shrinkage =</span> <span class="dv">1</span>, <span class="at">bag.fraction =</span> <span class="dv">1</span>)</span>
<span id="cb168-24"><a href="boosting.html#cb168-24" tabindex="-1"></a>  </span>
<span id="cb168-25"><a href="boosting.html#cb168-25" tabindex="-1"></a>  <span class="co"># you may peek into each tree</span></span>
<span id="cb168-26"><a href="boosting.html#cb168-26" tabindex="-1"></a>  <span class="fu">pretty.gbm.tree</span>(gbm.fit, <span class="at">i.tree =</span> <span class="dv">1</span>)</span>
<span id="cb168-27"><a href="boosting.html#cb168-27" tabindex="-1"></a><span class="do">##   SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight Prediction</span></span>
<span id="cb168-28"><a href="boosting.html#cb168-28" tabindex="-1"></a><span class="do">## 0        0          0.25        1         2           3            2.5     10       0.00</span></span>
<span id="cb168-29"><a href="boosting.html#cb168-29" tabindex="-1"></a><span class="do">## 1       -1          1.00       -1        -1          -1            0.0      2       1.00</span></span>
<span id="cb168-30"><a href="boosting.html#cb168-30" tabindex="-1"></a><span class="do">## 2       -1         -0.25       -1        -1          -1            0.0      8      -0.25</span></span>
<span id="cb168-31"><a href="boosting.html#cb168-31" tabindex="-1"></a><span class="do">## 3       -1          0.00       -1        -1          -1            0.0     10       0.00</span></span>
<span id="cb168-32"><a href="boosting.html#cb168-32" tabindex="-1"></a>  </span>
<span id="cb168-33"><a href="boosting.html#cb168-33" tabindex="-1"></a>  <span class="co"># we can view the predicted decision rule</span></span>
<span id="cb168-34"><a href="boosting.html#cb168-34" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb168-35"><a href="boosting.html#cb168-35" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb168-36"><a href="boosting.html#cb168-36" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb168-37"><a href="boosting.html#cb168-37" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">predict</span>(gbm.fit, xgrid)</span>
<span id="cb168-38"><a href="boosting.html#cb168-38" tabindex="-1"></a><span class="do">## Using 3 trees...</span></span>
<span id="cb168-39"><a href="boosting.html#cb168-39" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb168-40"><a href="boosting.html#cb168-40" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-259-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Here is a rundown of the algorithm. Let’s initialize all weights as <span class="math inline">\(1/n\)</span>. We only used trees with a single split as weak learners. The first tree is splitting at <span class="math inline">\(X_1 = 0.25\)</span>. After the first split, we need to adjust the weights.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="boosting.html#cb169-1" tabindex="-1"></a>  w1 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb169-2"><a href="boosting.html#cb169-2" tabindex="-1"></a>  f1 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">1</span>] <span class="sc">&lt;</span> <span class="fl">0.25</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb169-3"><a href="boosting.html#cb169-3" tabindex="-1"></a>  e1 <span class="ot">=</span> <span class="fu">sum</span>(w1<span class="sc">*</span>(<span class="fu">f1</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb169-4"><a href="boosting.html#cb169-4" tabindex="-1"></a>  a1 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e1)<span class="sc">/</span>e1)</span>
<span id="cb169-5"><a href="boosting.html#cb169-5" tabindex="-1"></a>  </span>
<span id="cb169-6"><a href="boosting.html#cb169-6" tabindex="-1"></a>  w2 <span class="ot">=</span> w1<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span> a1<span class="sc">*</span>y<span class="sc">*</span><span class="fu">f1</span>(X))</span>
<span id="cb169-7"><a href="boosting.html#cb169-7" tabindex="-1"></a>  w2 <span class="ot">=</span> w2<span class="sc">/</span><span class="fu">sum</span>(w2)</span>
<span id="cb169-8"><a href="boosting.html#cb169-8" tabindex="-1"></a>  </span>
<span id="cb169-9"><a href="boosting.html#cb169-9" tabindex="-1"></a>  <span class="co"># the first tree</span></span>
<span id="cb169-10"><a href="boosting.html#cb169-10" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb169-11"><a href="boosting.html#cb169-11" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb169-12"><a href="boosting.html#cb169-12" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb169-13"><a href="boosting.html#cb169-13" tabindex="-1"></a>  </span>
<span id="cb169-14"><a href="boosting.html#cb169-14" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f1</span>(xgrid)</span>
<span id="cb169-15"><a href="boosting.html#cb169-15" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb169-16"><a href="boosting.html#cb169-16" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb169-17"><a href="boosting.html#cb169-17" tabindex="-1"></a>  </span>
<span id="cb169-18"><a href="boosting.html#cb169-18" tabindex="-1"></a>  <span class="co"># weights after the first tree</span></span>
<span id="cb169-19"><a href="boosting.html#cb169-19" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb169-20"><a href="boosting.html#cb169-20" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb169-21"><a href="boosting.html#cb169-21" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w2)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-261-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>We can notice that the observations got correctly classified will decrease their weights while those mis-classified will increase the weights.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="boosting.html#cb170-1" tabindex="-1"></a>  f2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">2</span>] <span class="sc">&gt;</span> <span class="fl">0.65</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb170-2"><a href="boosting.html#cb170-2" tabindex="-1"></a>  e2 <span class="ot">=</span> <span class="fu">sum</span>(w2<span class="sc">*</span>(<span class="fu">f2</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb170-3"><a href="boosting.html#cb170-3" tabindex="-1"></a>  a2 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e2)<span class="sc">/</span>e2)</span>
<span id="cb170-4"><a href="boosting.html#cb170-4" tabindex="-1"></a>  </span>
<span id="cb170-5"><a href="boosting.html#cb170-5" tabindex="-1"></a>  w3 <span class="ot">=</span> w2<span class="sc">*</span><span class="fu">exp</span>(<span class="sc">-</span> a2<span class="sc">*</span>y<span class="sc">*</span><span class="fu">f2</span>(X))</span>
<span id="cb170-6"><a href="boosting.html#cb170-6" tabindex="-1"></a>  w3 <span class="ot">=</span> w3<span class="sc">/</span><span class="fu">sum</span>(w3)</span>
<span id="cb170-7"><a href="boosting.html#cb170-7" tabindex="-1"></a>  </span>
<span id="cb170-8"><a href="boosting.html#cb170-8" tabindex="-1"></a>  <span class="co"># the second tree</span></span>
<span id="cb170-9"><a href="boosting.html#cb170-9" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb170-10"><a href="boosting.html#cb170-10" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb170-11"><a href="boosting.html#cb170-11" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w2)</span>
<span id="cb170-12"><a href="boosting.html#cb170-12" tabindex="-1"></a>  </span>
<span id="cb170-13"><a href="boosting.html#cb170-13" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f2</span>(xgrid)</span>
<span id="cb170-14"><a href="boosting.html#cb170-14" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb170-15"><a href="boosting.html#cb170-15" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb170-16"><a href="boosting.html#cb170-16" tabindex="-1"></a>  </span>
<span id="cb170-17"><a href="boosting.html#cb170-17" tabindex="-1"></a>  <span class="co"># weights after the second tree</span></span>
<span id="cb170-18"><a href="boosting.html#cb170-18" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb170-19"><a href="boosting.html#cb170-19" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb170-20"><a href="boosting.html#cb170-20" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w3)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-262-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>And then we have the third step. Combining all three steps and their decision function, we have the final classifier</p>
<p><span class="math display">\[\begin{align}
F_3(x) =&amp; \sum_{t=1}^3 \alpha_t f_t(x) \nonumber \\
=&amp; 0.4236 \cdot f_1(x) + 0.6496 \cdot f_2(x) + 0.9229 \cdot f_3(x)
\end{align}\]</span></p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="boosting.html#cb171-1" tabindex="-1"></a>  f3 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">ifelse</span>(x[, <span class="dv">1</span>] <span class="sc">&lt;</span> <span class="fl">0.85</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb171-2"><a href="boosting.html#cb171-2" tabindex="-1"></a>  e3 <span class="ot">=</span> <span class="fu">sum</span>(w3<span class="sc">*</span>(<span class="fu">f3</span>(X) <span class="sc">!=</span> y))</span>
<span id="cb171-3"><a href="boosting.html#cb171-3" tabindex="-1"></a>  a3 <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>((<span class="dv">1</span><span class="sc">-</span>e3)<span class="sc">/</span>e3)</span>
<span id="cb171-4"><a href="boosting.html#cb171-4" tabindex="-1"></a>  </span>
<span id="cb171-5"><a href="boosting.html#cb171-5" tabindex="-1"></a>  <span class="co"># the third tree</span></span>
<span id="cb171-6"><a href="boosting.html#cb171-6" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb171-7"><a href="boosting.html#cb171-7" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb171-8"><a href="boosting.html#cb171-8" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">30</span><span class="sc">*</span>w3)</span>
<span id="cb171-9"><a href="boosting.html#cb171-9" tabindex="-1"></a>  </span>
<span id="cb171-10"><a href="boosting.html#cb171-10" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">f3</span>(xgrid)</span>
<span id="cb171-11"><a href="boosting.html#cb171-11" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb171-12"><a href="boosting.html#cb171-12" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb171-13"><a href="boosting.html#cb171-13" tabindex="-1"></a>  </span>
<span id="cb171-14"><a href="boosting.html#cb171-14" tabindex="-1"></a>  <span class="co"># the final decision rule </span></span>
<span id="cb171-15"><a href="boosting.html#cb171-15" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>),</span>
<span id="cb171-16"><a href="boosting.html#cb171-16" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">ifelse</span>(y <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">1</span>), <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">1.1</span>), <span class="at">lwd =</span> <span class="dv">3</span>,</span>
<span id="cb171-17"><a href="boosting.html#cb171-17" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.9</span>), <span class="at">cex =</span> <span class="dv">3</span>)</span>
<span id="cb171-18"><a href="boosting.html#cb171-18" tabindex="-1"></a>  </span>
<span id="cb171-19"><a href="boosting.html#cb171-19" tabindex="-1"></a>  pred <span class="ot">=</span> a1<span class="sc">*</span><span class="fu">f1</span>(xgrid) <span class="sc">+</span> a2<span class="sc">*</span><span class="fu">f2</span>(xgrid) <span class="sc">+</span> a3<span class="sc">*</span><span class="fu">f3</span>(xgrid)</span>
<span id="cb171-20"><a href="boosting.html#cb171-20" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), </span>
<span id="cb171-21"><a href="boosting.html#cb171-21" tabindex="-1"></a>         <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb171-22"><a href="boosting.html#cb171-22" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.25</span>) <span class="co"># f1</span></span>
<span id="cb171-23"><a href="boosting.html#cb171-23" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">h =</span> <span class="fl">0.65</span>) <span class="co"># f2</span></span>
<span id="cb171-24"><a href="boosting.html#cb171-24" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.85</span>) <span class="co"># f3</span></span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-263-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="training-error-of-adaboost" class="section level2 hasAnchor" number="20.2">
<h2><span class="header-section-number">20.2</span> Training Error of AdaBoost<a href="boosting.html#training-error-of-adaboost" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There is an interesting property about the boosting algorithm that if we can always find a classifier that performs better than random guessing at each iteration <span class="math inline">\(t\)</span>, then the training error will eventually converge to zero. This works by analyzing the weight after the last iteration <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[\begin{align}
w_i^{(T+1)} =&amp; \frac{1}{Z_T} w_i^{(T)} \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=&amp; \frac{1}{Z_1\cdots Z_T} w_i^{(1)} \prod_{t = 1}^T \exp\big\{ - \alpha_t y_i f_t(x_i) \big\} \nonumber \\
=&amp; \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \exp\Big\{ - y_i \sum_{t = 1}^T \alpha_t f_t(x_i) \Big\}
\end{align}\]</span></p>
<p>Since <span class="math inline">\(\sum_{t = 1}^T \alpha_t f_t(x_i)\)</span> is just the model at the <span class="math inline">\(T\)</span>-th iteration, we can write it as <span class="math inline">\(F_T(x_i)\)</span>. Noticing that they sum up to 1, we have</p>
<p><span class="math display">\[1 = \sum_{i = 1}^n w_i^{(T+1)} = \frac{1}{Z_1\cdots Z_T} \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}\]</span>
and
<span class="math display">\[Z_1\cdots Z_T = \frac{1}{n} \sum_{i = 1}^n \exp\big\{ - y_i F_T(x_i) \big\}\]</span>
On the right-hand-side, this is the exponential loss after we fit the model. In fact, this quantity would bound above the 0/1 loss, since the exponential loss is <span class="math inline">\(\exp[ - y f(x) ]\)</span>,</p>
<ul>
<li>For correctly classified subjects, <span class="math inline">\(y f(x) &gt; 0\)</span>, and <span class="math inline">\(\exp[ - y f(x) ] &gt; 0\)</span></li>
<li>For incorrectly classified subjects, <span class="math inline">\(y f(x) &lt; 0\)</span> the exponential loss is larger than 1</li>
</ul>
<p>This means that</p>
<p><span class="math display">\[Z_1\cdots Z_T &gt; \frac{1}{n} \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\}\]</span>
Hence, if we want the final model to have low training error, we should bound above the <span class="math inline">\(Z_t\)</span>’s. Recall that <span class="math inline">\(Z_t\)</span> is used to normalize the weights, we have</p>
<p><span class="math display">\[Z_t = \sum_i^{n} w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i) ].\]</span>
We have two cases at this iteration, <span class="math inline">\(y_i f(x_i) = 1\)</span> for correct subjects, and <span class="math inline">\(y_i f(x_i) = -1\)</span> for the incorrect ones, hence,
By our definition, <span class="math inline">\(\epsilon_t = \sum_i w_i^{(t)} \mathbf{1} \big\{ y_i \neq f_t(x_i) \big\}\)</span> is the proportion of weights for mis-classified samples.
<span class="math display">\[\begin{align}
Z_t =&amp; \,\,\sum_{i=1}^n w_i^{(t)} \exp[ - \alpha_t y_i f_t(x_i)] \nonumber\\
=&amp;\,\,\sum_{y_i = f_t(x_i)} w_i^{(t)} \exp[ - \alpha_t ] +  \sum_{y_i \neq f_t(x_i)} w_i^{(t)} \exp[ \alpha_t ] \nonumber\\
=&amp; \,\, \exp[ - \alpha_t ] \sum_{y_i = f_t(x_i)} w_i^{(t)} + \exp[ \alpha_t ] \sum_{y_i \neq f_t(x_i)} w_i^{(t)}
\end{align}\]</span></p>
<p>So we have</p>
<p><span class="math display">\[ Z_t = (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ].\]</span></p>
<p>If we want to minimize the product of all <span class="math inline">\(Z_t\)</span>’s, we can consider minimizing each of them. Let’s consider this as a function of <span class="math inline">\(\alpha_t\)</span>, then by taking a derivative with respect to <span class="math inline">\(\alpha_t\)</span>, we have</p>
<p><span class="math display">\[ - (1 - \epsilon_t) \exp[ - \alpha_t ] + \epsilon_t \exp[ \alpha_t ] = 0\]</span>
and</p>
<p><span class="math display">\[\alpha_t = \frac{1}{2} \log \frac{1 - \epsilon_t}{\epsilon_t}.\]</span>
Plugging this back into <span class="math inline">\(Z_t\)</span>, we have</p>
<p><span class="math display">\[Z_t = 2 \sqrt{\epsilon_t(1-\epsilon_t)}\]</span>
Since <span class="math inline">\(\epsilon_t(1-\epsilon_t)\)</span> can only attain maximum of <span class="math inline">\(1/4\)</span>, <span class="math inline">\(Z_t\)</span> must be smaller than 1. This makes the product <span class="math inline">\(Z_1 \cdots Z_T\)</span> converging to 0. If we look at this more closely, by defining <span class="math inline">\(\gamma_t = \frac{1}{2} - \epsilon_t\)</span> as the improvement from a random model (with error <span class="math inline">\(1/2\)</span>), then</p>
<p><span class="math display">\[\begin{align}
Z_t =&amp; 2 \sqrt{\epsilon_t(1-\epsilon_t)} \nonumber \\
=&amp; \sqrt{1 - 4 \gamma_t^2} \nonumber \\
\leq&amp; \exp\big[ - 2 \gamma_t^2 \big]
\end{align}\]</span></p>
<p>The last equation is because by Taylor expansion, <span class="math inline">\(\exp\big[ - 4 \gamma_t^2 \big] \geq 1 - 4 \gamma_t^2\)</span>. Then, we can finally put all <span class="math inline">\(Z_t\)</span>’s together:</p>
<p><span class="math display">\[\begin{align}
\text{Training Error} =&amp; \sum_{i = 1}^n \mathbf{1} \big\{ y_i \neq \text{sign}(F_T(x_i)) \big\} \nonumber \\
=&amp; \sum_{i = 1}^n \exp \big[ - y_i \neq F_T(x_i) \big] \nonumber \\
=&amp; Z_1 \cdots Z_T \nonumber \\
\leq&amp; \exp \big[ - 2 \sum_{t=1}^T \gamma_t^2 \big],
\end{align}\]</span></p>
<p>which converges to 0 as long as <span class="math inline">\(\sum_{t=1}^T \gamma_t^2\)</span> accumulates up to infinite. But of course, in practice, it would increasing difficult find <span class="math inline">\(f_t(x)\)</span> that reduces the training error greatly.</p>
</div>
<div id="tuning-the-number-of-trees" class="section level2 hasAnchor" number="20.3">
<h2><span class="header-section-number">20.3</span> Tuning the Number of Trees<a href="boosting.html#tuning-the-number-of-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although we can get really low training classification error, this is subject to overfitting. The following code demonstrates what an overfitted looks like.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="boosting.html#cb172-1" tabindex="-1"></a>  <span class="co"># One-dimensional classification example</span></span>
<span id="cb172-2"><a href="boosting.html#cb172-2" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span>; <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb172-3"><a href="boosting.html#cb172-3" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">cbind</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> n), <span class="fu">runif</span>(n))</span>
<span id="cb172-4"><a href="boosting.html#cb172-4" tabindex="-1"></a>  py <span class="ot">=</span> (<span class="fu">sin</span>(<span class="dv">4</span><span class="sc">*</span>pi<span class="sc">*</span>x[, <span class="dv">1</span>]) <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb172-5"><a href="boosting.html#cb172-5" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="dv">1</span>, py)</span>
<span id="cb172-6"><a href="boosting.html#cb172-6" tabindex="-1"></a>  </span>
<span id="cb172-7"><a href="boosting.html#cb172-7" tabindex="-1"></a>  <span class="fu">plot</span>(x[, <span class="dv">1</span>], y <span class="sc">+</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="fl">0.05</span>, <span class="fl">0.05</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>), <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb172-8"><a href="boosting.html#cb172-8" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>,<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;P(Y=1 | X=x)&quot;</span>)</span>
<span id="cb172-9"><a href="boosting.html#cb172-9" tabindex="-1"></a>  <span class="fu">points</span>(x[, <span class="dv">1</span>], py, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-265-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="boosting.html#cb173-1" tabindex="-1"></a>  </span>
<span id="cb173-2"><a href="boosting.html#cb173-2" tabindex="-1"></a>  <span class="co"># fit AdaBoost with bootstrapping, I am using a large shrinkage factor</span></span>
<span id="cb173-3"><a href="boosting.html#cb173-3" tabindex="-1"></a>  gbm.fit <span class="ot">=</span> <span class="fu">gbm</span>(y<span class="sc">~</span>., <span class="fu">data.frame</span>(x, y), <span class="at">distribution=</span><span class="st">&quot;adaboost&quot;</span>, <span class="at">n.minobsinnode =</span> <span class="dv">2</span>, </span>
<span id="cb173-4"><a href="boosting.html#cb173-4" tabindex="-1"></a>                <span class="at">n.trees=</span><span class="dv">200</span>, <span class="at">shrinkage =</span> <span class="dv">1</span>, <span class="at">bag.fraction=</span><span class="fl">0.8</span>, <span class="at">cv.folds =</span> <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="boosting.html#cb174-1" tabindex="-1"></a>  <span class="co"># plot the decision function (Fx, not sign(Fx))</span></span>
<span id="cb174-2"><a href="boosting.html#cb174-2" tabindex="-1"></a>  size<span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">50</span>, <span class="dv">100</span>)</span>
<span id="cb174-3"><a href="boosting.html#cb174-3" tabindex="-1"></a></span>
<span id="cb174-4"><a href="boosting.html#cb174-4" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span>
<span id="cb174-5"><a href="boosting.html#cb174-5" tabindex="-1"></a>  {</span>
<span id="cb174-6"><a href="boosting.html#cb174-6" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb174-7"><a href="boosting.html#cb174-7" tabindex="-1"></a>    <span class="fu">plot</span>(x[, <span class="dv">1</span>], py, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">ylab =</span> <span class="st">&quot;P(Y=1 | X=x)&quot;</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb174-8"><a href="boosting.html#cb174-8" tabindex="-1"></a>    <span class="fu">points</span>(x[, <span class="dv">1</span>], y <span class="sc">+</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="fl">0.05</span>, <span class="fl">0.05</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>, <span class="at">ylim =</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.05</span>, <span class="fl">1.05</span>),</span>
<span id="cb174-9"><a href="boosting.html#cb174-9" tabindex="-1"></a>           <span class="at">col =</span> <span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>))</span>
<span id="cb174-10"><a href="boosting.html#cb174-10" tabindex="-1"></a>    Fx <span class="ot">=</span> <span class="fu">predict</span>(gbm.fit, <span class="at">n.trees=</span>size[i]) <span class="co"># this returns the fitted function, but not class</span></span>
<span id="cb174-11"><a href="boosting.html#cb174-11" tabindex="-1"></a>    <span class="fu">lines</span>(x[, <span class="dv">1</span>], <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span><span class="dv">2</span><span class="sc">*</span>Fx)), <span class="at">lwd =</span> <span class="dv">1</span>)</span>
<span id="cb174-12"><a href="boosting.html#cb174-12" tabindex="-1"></a>    <span class="fu">title</span>(<span class="fu">paste</span>(<span class="st">&quot;# of Iterations = &quot;</span>, size[i]))</span>
<span id="cb174-13"><a href="boosting.html#cb174-13" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-267-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Hence, selecting trees is necessary. For this purpose, we can use either the out-of-bag error to estimate the exponential upper bound, or simply do cross-validation.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="boosting.html#cb175-1" tabindex="-1"></a>  <span class="co"># get the best number of trees from cross-validation (or oob if no cv is used)</span></span>
<span id="cb175-2"><a href="boosting.html#cb175-2" tabindex="-1"></a>  <span class="fu">gbm.perf</span>(gbm.fit)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-269-1.png" width="45%" style="display: block; margin: auto;" /></p>
<pre><code>## [1] 39</code></pre>
</div>
<div id="gradient-boosting" class="section level2 hasAnchor" number="20.4">
<h2><span class="header-section-number">20.4</span> Gradient Boosting<a href="boosting.html#gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s take an alternative view of this problem, we use an additive structure to fit models</p>
<p><span class="math display">\[F_T(x) = \sum_{t = 1}^T \alpha_t f(x; \boldsymbol{\theta}_t)\]</span></p>
<p>by minimizing a loss function</p>
<p><span class="math display">\[\underset{\{\alpha_t, \boldsymbol{\theta}_t\}_{t=1}^T}{\min} \sum_{i=1}^n L\big(y_i, F_T(x_i)\big)\]</span>
In this framework, we may choose a loss function <span class="math inline">\(L\)</span> that is suitable for the problem, and also choose the base learner <span class="math inline">\(f(x; \boldsymbol{\theta})\)</span> with parameter <span class="math inline">\(\boldsymbol{\theta}\)</span>. Examples of this include linear function, spline, tree, etc.. While it maybe difficult to minimize over all parameters <span class="math inline">\(\{\alpha_t, \boldsymbol{\theta}_t\}_{t=1}^T\)</span>, we may consider doing this in a stage-wise fashion. The algorithm could work in the following way:</p>
<ul>
<li>Set <span class="math inline">\(F_0(x) = 0\)</span></li>
<li>For <span class="math inline">\(t = 1, \ldots, T\)</span>
<ul>
<li>Choose <span class="math inline">\((\alpha_t, \boldsymbol{\theta}_t)\)</span> to minimize the loss
<span class="math display">\[\underset{\alpha, \boldsymbol{\theta}}{\min} \,\, \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \boldsymbol{\theta})\big)\]</span></li>
<li>Update <span class="math inline">\(F_t(x) = F_{t-1}(x) + \alpha_t f(x; \boldsymbol{\theta}_t)\)</span></li>
</ul></li>
<li>Output <span class="math inline">\(F_T(x)\)</span> as the final model</li>
</ul>
<p>The previous AdaBoost example is using exponential loss function. Also, it doesn’t pick an optimal <span class="math inline">\(f(x; \boldsymbol{\theta})\)</span> at each step. We just need a model that is better than random. The step size <span class="math inline">\(\alpha_t\)</span> is optimized at each <span class="math inline">\(t\)</span> given the fitted <span class="math inline">\(f(x; \boldsymbol{\theta}_t)\)</span>.</p>
<p>Another example is the forward stage-wise linear regression. In this case, we fit a single variable linear model at each step <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[f(x, j) = \text{sign}\big(\text{Cor}(X_j, \mathbf{r})\big) X_j\]</span>
* <span class="math inline">\(\mathbf{r}\)</span> is the residual, as <span class="math inline">\(r_i = y_i - F_{t-1}(x_i)\)</span>
* <span class="math inline">\(j\)</span> is the index that has the largest absolute correlation with <span class="math inline">\(\mathbf{r}\)</span></p>
<p>Then we give a very small step size <span class="math inline">\(\alpha_t\)</span>, say, <span class="math inline">\(\alpha_t = 10^{-5}\)</span>, and with sign equal to the correlation between <span class="math inline">\(X_j\)</span>. In this case, <span class="math inline">\(F_t(x)\)</span> is almost equivalent to the Lasso solution path, as <span class="math inline">\(t\)</span> increases.</p>
<p>We may notice that <span class="math inline">\(r_i\)</span> is in fact the negative gradient of the squared-error loss, as a function of the fitted function:</p>
<p><span class="math display">\[r_{it} = - \left[ \frac{\partial \, \big(y_i - F(x_i)\big)^2 }{\partial \, F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}\]</span>
and we are essentially fitting a weak leaner <span class="math inline">\(f_t(x)\)</span> to the residuals and update the fitted model <span class="math inline">\(F_t(x)\)</span>. The following example shows the result of using a tree leaner as <span class="math inline">\(f_t(x)\)</span>:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="boosting.html#cb177-1" tabindex="-1"></a>  <span class="fu">library</span>(gbm)</span>
<span id="cb177-2"><a href="boosting.html#cb177-2" tabindex="-1"></a></span>
<span id="cb177-3"><a href="boosting.html#cb177-3" tabindex="-1"></a>  <span class="co"># a simple regression problem</span></span>
<span id="cb177-4"><a href="boosting.html#cb177-4" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb177-5"><a href="boosting.html#cb177-5" tabindex="-1"></a>  x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)</span>
<span id="cb177-6"><a href="boosting.html#cb177-6" tabindex="-1"></a>  fx <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(<span class="dv">3</span><span class="sc">*</span>pi<span class="sc">*</span>x)</span>
<span id="cb177-7"><a href="boosting.html#cb177-7" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">fx</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x))</span>
<span id="cb177-8"><a href="boosting.html#cb177-8" tabindex="-1"></a></span>
<span id="cb177-9"><a href="boosting.html#cb177-9" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb177-10"><a href="boosting.html#cb177-10" tabindex="-1"></a>  <span class="co"># plot the true regression line</span></span>
<span id="cb177-11"><a href="boosting.html#cb177-11" tabindex="-1"></a>  <span class="fu">lines</span>(x, <span class="fu">fx</span>(x), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-271-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can see that the fitted model progressively approaximates the true function.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="boosting.html#cb178-1" tabindex="-1"></a>  <span class="co"># fit regression boosting</span></span>
<span id="cb178-2"><a href="boosting.html#cb178-2" tabindex="-1"></a>  <span class="co"># I use a very large shrinkage value for demonstrating the functions</span></span>
<span id="cb178-3"><a href="boosting.html#cb178-3" tabindex="-1"></a>  <span class="co"># in practice you should use 0.1 or even smaller values for stability</span></span>
<span id="cb178-4"><a href="boosting.html#cb178-4" tabindex="-1"></a>  gbm.fit <span class="ot">=</span> <span class="fu">gbm</span>(y<span class="sc">~</span>x, <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>,</span>
<span id="cb178-5"><a href="boosting.html#cb178-5" tabindex="-1"></a>                <span class="at">n.trees=</span><span class="dv">300</span>, <span class="at">shrinkage=</span><span class="fl">0.5</span>, <span class="at">bag.fraction=</span><span class="fl">0.8</span>)</span>
<span id="cb178-6"><a href="boosting.html#cb178-6" tabindex="-1"></a></span>
<span id="cb178-7"><a href="boosting.html#cb178-7" tabindex="-1"></a>  <span class="co"># somehow, cross-validation for 1 dimensional problem creates error</span></span>
<span id="cb178-8"><a href="boosting.html#cb178-8" tabindex="-1"></a>  <span class="co"># gbm(y ~ ., data = data.frame(x, y), cv.folds = 3) # this produces an error  </span></span>
<span id="cb178-9"><a href="boosting.html#cb178-9" tabindex="-1"></a>  </span>
<span id="cb178-10"><a href="boosting.html#cb178-10" tabindex="-1"></a>  <span class="co"># plot the fitted regression function at several iterations</span></span>
<span id="cb178-11"><a href="boosting.html#cb178-11" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb178-12"><a href="boosting.html#cb178-12" tabindex="-1"></a>  size<span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">50</span>,<span class="dv">100</span>,<span class="dv">300</span>)</span>
<span id="cb178-13"><a href="boosting.html#cb178-13" tabindex="-1"></a>  </span>
<span id="cb178-14"><a href="boosting.html#cb178-14" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span>
<span id="cb178-15"><a href="boosting.html#cb178-15" tabindex="-1"></a>  {</span>
<span id="cb178-16"><a href="boosting.html#cb178-16" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb178-17"><a href="boosting.html#cb178-17" tabindex="-1"></a>    <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb178-18"><a href="boosting.html#cb178-18" tabindex="-1"></a>    <span class="fu">lines</span>(x, <span class="fu">fx</span>(x), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>)</span>
<span id="cb178-19"><a href="boosting.html#cb178-19" tabindex="-1"></a>    </span>
<span id="cb178-20"><a href="boosting.html#cb178-20" tabindex="-1"></a>    <span class="co"># this returns the fitted function, but not class</span></span>
<span id="cb178-21"><a href="boosting.html#cb178-21" tabindex="-1"></a>    Fx <span class="ot">=</span> <span class="fu">predict</span>(gbm.fit, <span class="at">n.trees=</span>size[i])</span>
<span id="cb178-22"><a href="boosting.html#cb178-22" tabindex="-1"></a>    <span class="fu">lines</span>(x, Fx, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span>
<span id="cb178-23"><a href="boosting.html#cb178-23" tabindex="-1"></a>    <span class="fu">title</span>(<span class="fu">paste</span>(<span class="st">&quot;# of Iterations = &quot;</span>, size[i]))</span>
<span id="cb178-24"><a href="boosting.html#cb178-24" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-272-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>This idea can be generalized to any loss function <span class="math inline">\(L\)</span>. This is the <strong>gradient boosting</strong> model:</p>
<ul>
<li>At each iteration <span class="math inline">\(t\)</span>, calculate ``pseudo-residuals’’, i.e., the negative gradient for each observation
<span class="math display">\[g_{it} = - \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F(x_i) = F_{t-1}(x_i)}\]</span></li>
<li>Fit <span class="math inline">\(f_t(x, \boldsymbol{\theta}_t)\)</span> to pseudo-residual <span class="math inline">\(g_{it}\)</span>’s</li>
<li>Search for the best
<span class="math display">\[\alpha_t = \underset{\alpha}{\arg\min} \sum_{i=1}^n L\big(y_i, F_{t-1}(x_i) + \alpha f(x_i; \boldsymbol{\theta}_t)\big)\]</span></li>
<li>Update <span class="math inline">\(F_t(x) = F_{t-1}(x) + \alpha_t f(x; \boldsymbol{\theta}_t)\)</span></li>
</ul>
<p>Hence, the only change when modeling different outcomes is to choose the loss function <span class="math inline">\(L\)</span>, and derive the pseudo-residuals</p>
<ul>
<li>For regression, the loss is <span class="math inline">\(\frac{1}{2} (y  - f(x))^2\)</span>, and the pseudo-residual is <span class="math inline">\(y_i - f(x_i)\)</span></li>
<li>For quantile regression to model median, the loss is <span class="math inline">\(|y  - f(x)|\)</span>, and the pseudo-residual is sign<span class="math inline">\((y_i - f(x_i))\)</span></li>
<li>For classification, we can use the negative log likelihood of a single observation <span class="math inline">\(- [ y\log(p) + (1-y)\log(1-p) ]\)</span>, and express <span class="math inline">\(p\)</span> as the log-odds of a scale predictor, i.e., <span class="math inline">\(f = \log(p/(1-p))\)</span>. Then the pseudo-residual is <span class="math inline">\(y_i - p(x_i)\)</span></li>
</ul>
</div>
<div id="gradient-boosting-with-logistic-link" class="section level2 hasAnchor" number="20.5">
<h2><span class="header-section-number">20.5</span> Gradient Boosting with Logistic Link<a href="boosting.html#gradient-boosting-with-logistic-link" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To see how the pseudo-residual of a classification model is derived, let’s use the logistic link function the predicted probability <span class="math inline">\(p\)</span> defined as:</p>
<p><span class="math display">\[
p =  \frac{e^{F(x)}}{1 + e^{F(x)}} = \frac{1}{1 + e^{-F(x)}}
\]</span></p>
<p>The negative log-likelihood for the Bernoulli distribution for a single instance is given by:</p>
<p><span class="math display">\[
L(y, p) = - [y \log(p) + (1 - y) \log(1 - p)]
\]</span></p>
<p>Let’s first find the partial derivative of this loss function with respect to <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
\frac{\partial L(y, p)}{\partial p} = -\left[ y \frac{1}{p} - (1-y) \frac{1}{1-p} \right]
\]</span></p>
<p>The derivative of <span class="math inline">\(p\)</span> with respect to <span class="math inline">\(F(x)\)</span> is:</p>
<p><span class="math display">\[
\frac{\partial p}{\partial F(x)} = \frac{e^{-F(x)}}{(1 + e^{-F(x)})^2} = p(1-p)
\]</span></p>
<p>Hence, the partial derivative of the loss function with respect to <span class="math inline">\(F\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial L(y, p)}{\partial F(x)} &amp;= \frac{\partial L(y, p)}{\partial p} \cdot \frac{\partial p}{\partial F(x)}\\
&amp;= -\left[ y \frac{1}{p} - (1-y) \frac{1}{1-p} \right] \cdot p(1-p)\\
&amp;= -(y - y p + p - y p)\\
&amp;= -(y - p)
\end{align*}\]</span></p>
<p>Note that we should move <span class="math inline">\(F(x)\)</span> to the negative gradient, then <span class="math inline">\(y_i - p_i\)</span> is the pseudo-residual that we use in the boosting algorithm to fit the next tree / linear booster. The sign mainly influences the direction of the adjustment but is accounted for in the optimization process.</p>

<div style="display:none;">
<!-- Conflict \def\bf{\mathbf{f}} -->
</div>
</div>
</div>



<h3> Reference<a href="reference.html#reference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-freund1997decision" class="csl-entry">
Freund, Yoav, and Robert E Schapire. 1997. <span>“A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.”</span> <em>Journal of Computer and System Sciences</em> 55 (1): 119–39.
</div>
<div id="ref-friedman2001greedy" class="csl-entry">
Friedman, Jerome H. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>Annals of Statistics</em>, 1189–1232.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-forests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="k-means.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "sepia",
    "family": "serif",
    "size": 1
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
