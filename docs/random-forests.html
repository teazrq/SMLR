<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 21 Random Forests | Statistical Machine Learning with R</title>
  <meta name="description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 21 Random Forests | Statistical Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 21 Random Forests | Statistical Machine Learning with R" />
  
  <meta name="twitter:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2025-10-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="classification-and-regression-trees.html"/>
<link rel="next" href="adaboost.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visual-studio-code.html"><a href="visual-studio-code.html"><i class="fa fa-check"></i><b>3</b> Visual Studio Code</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visual-studio-code.html"><a href="visual-studio-code.html#basics-and-resources-1"><i class="fa fa-check"></i><b>3.1</b> Basics and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linear-regression"><i class="fa fa-check"></i><b>4.2</b> Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#matrix-inversion"><i class="fa fa-check"></i><b>4.3</b> Matrix Inversion</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linearalgebra-SM"><i class="fa fa-check"></i><b>4.3.1</b> Rank-one Update</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#rank-k-update"><i class="fa fa-check"></i><b>4.3.2</b> Rank-<span class="math inline">\(k\)</span> Update</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#times-2-block-matrix-inversion"><i class="fa fa-check"></i><b>4.3.3</b> 2 <span class="math inline">\(\times\)</span> 2 Block Matrix Inversion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>5</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>5.1</b> Basic Concept</a></li>
<li class="chapter" data-level="5.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>5.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="5.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>5.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="5.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>5.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="5.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>5.5</b> Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>5.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>5.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="5.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>5.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>5.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>5.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>5.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>5.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>6</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>6.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>6.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>6.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>6.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>6.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>6.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>6.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>6.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>6.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>6.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>6.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>6.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>7</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>7.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="7.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>7.2</b> Ridge Penalty and the Reduced Variation</a></li>
<li class="chapter" data-level="7.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>7.3</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="7.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>7.4</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="7.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>7.5</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>7.5.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="7.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>7.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>7.6</b> Cross-validation</a></li>
<li class="chapter" data-level="7.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.7</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>7.7.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>7.8</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>7.8.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8</b> Lasso</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>8.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>8.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="8.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>8.3</b> The Solution Path</a></li>
<li class="chapter" data-level="8.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>8.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="8.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>8.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="8.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>8.6</b> Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>9</b> Spline</a>
<ul>
<li class="chapter" data-level="9.1" data-path="spline.html"><a href="spline.html#using-linear-models-for-nonlinear-trends"><i class="fa fa-check"></i><b>9.1</b> Using Linear models for Nonlinear Trends</a></li>
<li class="chapter" data-level="9.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>9.2</b> A Motivating Example and Polynomials</a></li>
<li class="chapter" data-level="9.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>9.3</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="9.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>9.4</b> Splines</a></li>
<li class="chapter" data-level="9.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>9.5</b> Spline Basis</a></li>
<li class="chapter" data-level="9.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>9.6</b> Natural Cubic Spline</a></li>
<li class="chapter" data-level="9.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>9.7</b> Smoothing Spline</a></li>
<li class="chapter" data-level="9.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>9.8</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="9.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>9.9</b> Extending Splines to Multiple Varibles</a></li>
</ul></li>
<li class="part"><span><b>III Linear Classification Models</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>10.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>10.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>10.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>10.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>11</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>11.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="11.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>11.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="11.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>11.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="11.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>11.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="11.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>11.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>11.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>IV Local Averaging and Kernel Smoothing</b></span></li>
<li class="chapter" data-level="12" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>12</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>12.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="12.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>12.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="12.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>12.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="12.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>12.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="12.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#degrees-of-freedom-1"><i class="fa fa-check"></i><b>12.6</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="12.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>12.7</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="12.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>12.8</b> Distance Measures</a></li>
<li class="chapter" data-level="12.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>12.9</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="12.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>12.10</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="12.11" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>12.11</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>13</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>13.1</b> KNN vs. Kernel</a></li>
<li class="chapter" data-level="13.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>13.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="13.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#expectation-of-the-parzen-estimator"><i class="fa fa-check"></i><b>13.3</b> Expectation of the Parzen estimator</a></li>
<li class="chapter" data-level="13.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>13.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>13.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>13.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="13.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>13.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="13.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>13.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="13.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>13.8</b> R Implementations</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>14</b> Nonparemetric Estimation Rates</a>
<ul>
<li class="chapter" data-level="14.1" data-path="nonpara.html"><a href="nonpara.html#kernel-density-estimation"><i class="fa fa-check"></i><b>14.1</b> Kernel Density Estimation</a></li>
<li class="chapter" data-level="14.2" data-path="nonpara.html"><a href="nonpara.html#the-effect-of-smoothness"><i class="fa fa-check"></i><b>14.2</b> The Effect of Smoothness</a></li>
<li class="chapter" data-level="14.3" data-path="nonpara.html"><a href="nonpara.html#the-effect-of-dimensionality"><i class="fa fa-check"></i><b>14.3</b> The Effect of Dimensionality</a></li>
<li class="chapter" data-level="14.4" data-path="nonpara.html"><a href="nonpara.html#nadaraya-watson-regression-estimator"><i class="fa fa-check"></i><b>14.4</b> Nadaraya-Watson Regression Estimator</a></li>
</ul></li>
<li class="part"><span><b>V Kernel Machines</b></span></li>
<li class="chapter" data-level="15" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>15</b> Reproducing Kernel Hilbert Space</a>
<ul>
<li class="chapter" data-level="15.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-motivation"><i class="fa fa-check"></i><b>15.1</b> The Motivation</a></li>
<li class="chapter" data-level="15.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#hilbert-space-preliminaries"><i class="fa fa-check"></i><b>15.2</b> Hilbert Space Preliminaries</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-space-of-square-integrable-functions"><i class="fa fa-check"></i><b>15.2.1</b> The Space of Square-Integrable Functions</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-kernel-function"><i class="fa fa-check"></i><b>15.3</b> A Kernel Function</a></li>
<li class="chapter" data-level="15.4" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-space-of-functions"><i class="fa fa-check"></i><b>15.4</b> A Space of Functions</a></li>
<li class="chapter" data-level="15.5" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-inner-product"><i class="fa fa-check"></i><b>15.5</b> The Inner Product</a></li>
<li class="chapter" data-level="15.6" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-rkhs"><i class="fa fa-check"></i><b>15.6</b> The RKHS</a></li>
<li class="chapter" data-level="15.7" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-reproducing-property"><i class="fa fa-check"></i><b>15.7</b> The Reproducing Property</a></li>
<li class="chapter" data-level="15.8" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#smoothness"><i class="fa fa-check"></i><b>15.8</b> Smoothness</a></li>
<li class="chapter" data-level="15.9" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-moorearonszajn-theorem"><i class="fa fa-check"></i><b>15.9</b> The Moore–Aronszajn Theorem</a></li>
<li class="chapter" data-level="15.10" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#examples"><i class="fa fa-check"></i><b>15.10</b> Examples</a>
<ul>
<li class="chapter" data-level="15.10.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#brownian-motion-kernel"><i class="fa fa-check"></i><b>15.10.1</b> Brownian Motion Kernel</a></li>
<li class="chapter" data-level="15.10.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#non-positive-definite-kernel"><i class="fa fa-check"></i><b>15.10.2</b> Non-positive Definite Kernel</a></li>
<li class="chapter" data-level="15.10.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#defining-new-kernels"><i class="fa fa-check"></i><b>15.10.3</b> Defining New Kernels</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>16</b> Kernel Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#linear-regression-as-a-constraint-optimization"><i class="fa fa-check"></i><b>16.1</b> Linear Regression as a Constraint Optimization</a></li>
<li class="chapter" data-level="16.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#the-kernel-ridge-regression"><i class="fa fa-check"></i><b>16.2</b> The Kernel Ridge Regression</a></li>
<li class="chapter" data-level="16.3" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#ridge-regression-as-a-linear-kernel-model"><i class="fa fa-check"></i><b>16.3</b> Ridge Regression as a Linear Kernel Model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>17</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="17.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>17.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="17.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>17.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>17.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>17.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="17.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>17.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="17.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>17.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="17.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>17.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="17.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>17.7</b> SVM as a Penalized Model</a></li>
<li class="chapter" data-level="17.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>17.8</b> Kernel and Feature Maps: Another Example</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html"><i class="fa fa-check"></i><b>18</b> The Representer Theorem</a>
<ul>
<li class="chapter" data-level="18.1" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#the-representer-theorem-1"><i class="fa fa-check"></i><b>18.1</b> The Representer Theorem</a></li>
<li class="chapter" data-level="18.2" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#notes-on-application"><i class="fa fa-check"></i><b>18.2</b> Notes on Application</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="support-vector-regression.html"><a href="support-vector-regression.html"><i class="fa fa-check"></i><b>19</b> Support Vector Regression</a>
<ul>
<li class="chapter" data-level="19.1" data-path="support-vector-regression.html"><a href="support-vector-regression.html#the-epsilon-insensitive-loss"><i class="fa fa-check"></i><b>19.1</b> The <span class="math inline">\(\epsilon\)</span>-insensitive Loss</a></li>
<li class="chapter" data-level="19.2" data-path="support-vector-regression.html"><a href="support-vector-regression.html#primal-and-dual-formulation-of-svr"><i class="fa fa-check"></i><b>19.2</b> Primal and Dual Formulation of SVR</a></li>
<li class="chapter" data-level="19.3" data-path="support-vector-regression.html"><a href="support-vector-regression.html#penalized-svr-with-rkhs"><i class="fa fa-check"></i><b>19.3</b> Penalized SVR with RKHS</a></li>
</ul></li>
<li class="part"><span><b>VI Trees and Ensembles</b></span></li>
<li class="chapter" data-level="20" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>20</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="20.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>20.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="20.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>20.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="20.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>20.3</b> Regression Trees</a></li>
<li class="chapter" data-level="20.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>20.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="20.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>20.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>21</b> Random Forests</a>
<ul>
<li class="chapter" data-level="21.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>21.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="21.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>21.2</b> Random Forests</a></li>
<li class="chapter" data-level="21.3" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forests"><i class="fa fa-check"></i><b>21.3</b> Kernel view of Random Forests</a></li>
<li class="chapter" data-level="21.4" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>21.4</b> Variable Importance</a></li>
<li class="chapter" data-level="21.5" data-path="random-forests.html"><a href="random-forests.html#adaptiveness-of-random-forest-kernel"><i class="fa fa-check"></i><b>21.5</b> Adaptiveness of Random Forest Kernel</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="adaboost.html"><a href="adaboost.html"><i class="fa fa-check"></i><b>22</b> AdaBoost</a>
<ul>
<li class="chapter" data-level="22.1" data-path="adaboost.html"><a href="adaboost.html#the-algorithm"><i class="fa fa-check"></i><b>22.1</b> The Algorithm</a></li>
<li class="chapter" data-level="22.2" data-path="adaboost.html"><a href="adaboost.html#training-error-bound"><i class="fa fa-check"></i><b>22.2</b> Training Error Bound</a></li>
<li class="chapter" data-level="22.3" data-path="adaboost.html"><a href="adaboost.html#the-stagewise-additive-model-and-probability-calibration"><i class="fa fa-check"></i><b>22.3</b> The Stagewise Additive Model and Probability Calibration</a></li>
<li class="chapter" data-level="22.4" data-path="adaboost.html"><a href="adaboost.html#tuning-the-number-of-trees"><i class="fa fa-check"></i><b>22.4</b> Tuning the Number of Trees</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html"><i class="fa fa-check"></i><b>23</b> Gradient Boosting Machines</a>
<ul>
<li class="chapter" data-level="23.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#motivation-lasso-as-boosting"><i class="fa fa-check"></i><b>23.1</b> Motivation: Lasso as Boosting</a></li>
<li class="chapter" data-level="23.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gradient-boosting"><i class="fa fa-check"></i><b>23.2</b> Gradient Boosting</a></li>
<li class="chapter" data-level="23.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gradient-boosting-with-general-loss"><i class="fa fa-check"></i><b>23.3</b> Gradient Boosting with General Loss</a></li>
<li class="chapter" data-level="23.4" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#logistic-link"><i class="fa fa-check"></i><b>23.4</b> Logistic Link</a></li>
<li class="chapter" data-level="23.5" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#xgboost"><i class="fa fa-check"></i><b>23.5</b> xgboost</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Learning</b></span></li>
<li class="chapter" data-level="24" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>24</b> K-Means</a>
<ul>
<li class="chapter" data-level="24.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>24.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="24.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>24.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="24.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>24.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>25</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="25.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>25.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="25.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>25.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="25.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>25.3</b> Example 2: RNA Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>26</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="26.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>26.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="26.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>26.1.1</b> Note: Scaling</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>26.2</b> Example 1: <code>iris</code> Data</a></li>
<li class="chapter" data-level="26.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>26.3</b> Example 2: Handwritten Digits</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>27</b> Self-Organizing Map</a>
<ul>
<li class="chapter" data-level="27.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>27.1</b> Basic Concepts</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>28</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="28.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#an-example"><i class="fa fa-check"></i><b>28.1</b> An Example</a></li>
<li class="chapter" data-level="28.2" data-path="spectral-clustering.html"><a href="spectral-clustering.html#adjacency-matrix"><i class="fa fa-check"></i><b>28.2</b> Adjacency Matrix</a></li>
<li class="chapter" data-level="28.3" data-path="spectral-clustering.html"><a href="spectral-clustering.html#laplacian-matrix"><i class="fa fa-check"></i><b>28.3</b> Laplacian Matrix</a></li>
<li class="chapter" data-level="28.4" data-path="spectral-clustering.html"><a href="spectral-clustering.html#derivation-of-the-feature-embedding"><i class="fa fa-check"></i><b>28.4</b> Derivation of the Feature Embedding</a></li>
<li class="chapter" data-level="28.5" data-path="spectral-clustering.html"><a href="spectral-clustering.html#feature-embedding"><i class="fa fa-check"></i><b>28.5</b> Feature Embedding</a></li>
<li class="chapter" data-level="28.6" data-path="spectral-clustering.html"><a href="spectral-clustering.html#clustering-with-embedded-features"><i class="fa fa-check"></i><b>28.6</b> Clustering with Embedded Features</a></li>
<li class="chapter" data-level="28.7" data-path="spectral-clustering.html"><a href="spectral-clustering.html#normalized-graph-laplacian"><i class="fa fa-check"></i><b>28.7</b> Normalized Graph Laplacian</a></li>
<li class="chapter" data-level="28.8" data-path="spectral-clustering.html"><a href="spectral-clustering.html#using-a-different-adjacency-matrix"><i class="fa fa-check"></i><b>28.8</b> Using a Different Adjacency Matrix</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html"><i class="fa fa-check"></i><b>29</b> Uniform Manifold Approximation and Projection</a>
<ul>
<li class="chapter" data-level="29.1" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#an-example-1"><i class="fa fa-check"></i><b>29.1</b> An Example</a></li>
<li class="chapter" data-level="29.2" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#tuning"><i class="fa fa-check"></i><b>29.2</b> Tuning</a></li>
<li class="chapter" data-level="29.3" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#another-example"><i class="fa fa-check"></i><b>29.3</b> Another Example</a></li>
</ul></li>
<li class="part"><span><b>VIII Reference</b></span></li>
<li class="chapter" data-level="30" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>30</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2023 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-forests" class="section level1 hasAnchor" number="21">
<h1><span class="header-section-number">Chapter 21</span> Random Forests<a href="random-forests.html#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Roughly speaking, random forests <span class="citation">(<a href="#ref-breiman2001random">Breiman 2001</a>)</span> are parallelly fitted CART models with some randomness. There are several main components:</p>
<ul>
<li>Bootstrapping of data for each tree using the Bagging idea <span class="citation">(<a href="#ref-breiman1996bagging">Breiman 1996</a>)</span>, and use the averaged result (for regression) or majority voting (for classification) of all trees as the prediction.</li>
<li>At each internal node, we may not consider all variables. Instead, we consider a randomly selected <code>mtry</code> variables to search for the best split. This idea was inspired by <span class="citation">Ho (<a href="#ref-ho1998random">1998</a>)</span>.</li>
<li>For each tree, we will not perform pruning. Instead, we simply stop when the internal node contains no more than <code>nodesize</code> number of observations.</li>
</ul>
<p>Later on, there were various version of random forests that attempts to improve the performance, from both computational and theoretical prospective. We will introduce them later.</p>
<div id="bagging-predictors" class="section level2 hasAnchor" number="21.1">
<h2><span class="header-section-number">21.1</span> Bagging Predictors<a href="random-forests.html#bagging-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>CART models may be difficult when dealing with non-axis-aligned decision boundaries. This can be seen from the example below, in a two-dimensional case. The idea of Bagging is that we can fit many CART models, each from a Bootstrap sample, i.e., sample with replacement from the original <span class="math inline">\(n\)</span> observations. The reason that Breiman considered bootstrap samples is because it can approximate the original distribution that generates the data. But the end result is that since each tree may be slightly different from each other, when we stack them, the decision bound can be more “smooth”.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="random-forests.html#cb159-1" tabindex="-1"></a>  <span class="co"># generate some data </span></span>
<span id="cb159-2"><a href="random-forests.html#cb159-2" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb159-3"><a href="random-forests.html#cb159-3" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb159-4"><a href="random-forests.html#cb159-4" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb159-5"><a href="random-forests.html#cb159-5" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb159-6"><a href="random-forests.html#cb159-6" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>((x1 <span class="sc">+</span> x2 <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> (x1 <span class="sc">+</span> x2 <span class="sc">&lt;</span> <span class="fl">0.5</span>) , <span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb159-7"><a href="random-forests.html#cb159-7" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span></code></pre></div>
<p>Let’s compare the decision rule of CART and Bagging. For CART, the decision line has to be aligned to axis. For Bagging, we use a total of 200 trees, specified by <code>nbagg</code> in the <code>ipred</code> package.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="random-forests.html#cb160-1" tabindex="-1"></a>  <span class="co"># fit CART</span></span>
<span id="cb160-2"><a href="random-forests.html#cb160-2" tabindex="-1"></a>  <span class="fu">library</span>(rpart)</span>
<span id="cb160-3"><a href="random-forests.html#cb160-3" tabindex="-1"></a>  rpart.fit <span class="ot">=</span> <span class="fu">rpart</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y))</span>
<span id="cb160-4"><a href="random-forests.html#cb160-4" tabindex="-1"></a></span>
<span id="cb160-5"><a href="random-forests.html#cb160-5" tabindex="-1"></a>  <span class="co"># we could fit a different tree using a bootstrap sample</span></span>
<span id="cb160-6"><a href="random-forests.html#cb160-6" tabindex="-1"></a>  <span class="co"># rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y)[sample(1:n, n, replace = TRUE), ])</span></span>
<span id="cb160-7"><a href="random-forests.html#cb160-7" tabindex="-1"></a></span>
<span id="cb160-8"><a href="random-forests.html#cb160-8" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">predict</span>(rpart.fit, xgrid, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>) <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">201</span>)</span>
<span id="cb160-9"><a href="random-forests.html#cb160-9" tabindex="-1"></a>  <span class="fu">contour</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), pred, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb160-10"><a href="random-forests.html#cb160-10" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb160-11"><a href="random-forests.html#cb160-11" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>))</span>
<span id="cb160-12"><a href="random-forests.html#cb160-12" tabindex="-1"></a>  <span class="fu">box</span>()    </span>
<span id="cb160-13"><a href="random-forests.html#cb160-13" tabindex="-1"></a>  <span class="fu">title</span>(<span class="st">&quot;CART&quot;</span>)</span>
<span id="cb160-14"><a href="random-forests.html#cb160-14" tabindex="-1"></a> </span>
<span id="cb160-15"><a href="random-forests.html#cb160-15" tabindex="-1"></a>  <span class="co"># fit Bagging</span></span>
<span id="cb160-16"><a href="random-forests.html#cb160-16" tabindex="-1"></a>  <span class="fu">library</span>(ipred)</span>
<span id="cb160-17"><a href="random-forests.html#cb160-17" tabindex="-1"></a>  bag.fit <span class="ot">=</span> <span class="fu">bagging</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y), <span class="at">nbagg =</span> <span class="dv">200</span>, <span class="at">ns =</span> <span class="dv">400</span>)</span>
<span id="cb160-18"><a href="random-forests.html#cb160-18" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">predict</span>(<span class="fu">prune</span>(bag.fit), xgrid) <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">201</span>, <span class="dv">201</span>)</span>
<span id="cb160-19"><a href="random-forests.html#cb160-19" tabindex="-1"></a>  <span class="fu">contour</span>(<span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), pred, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb160-20"><a href="random-forests.html#cb160-20" tabindex="-1"></a>  <span class="fu">points</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>)</span>
<span id="cb160-21"><a href="random-forests.html#cb160-21" tabindex="-1"></a>  <span class="fu">points</span>(xgrid, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>))</span>
<span id="cb160-22"><a href="random-forests.html#cb160-22" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb160-23"><a href="random-forests.html#cb160-23" tabindex="-1"></a>  <span class="fu">title</span>(<span class="st">&quot;Bagging&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-250-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="random-forests-1" class="section level2 hasAnchor" number="21.2">
<h2><span class="header-section-number">21.2</span> Random Forests<a href="random-forests.html#random-forests-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random forests are equipped with this bootstrapping strategy, but also with other randomness and mechanism. A Random forest can be controlled by several key parameters:</p>
<ul>
<li><code>ntree</code>: Number of trees, many controls the stability. We typically want to fit a large number of trees.</li>
<li><code>sampsize</code>: How many samples to use when fitting each tree. In practice, we often use the training sample size if sampled with replacement.</li>
<li><code>mtry</code>: Number of randomly sampled variable to consider at each internal node. This need to be tuned for adaptiveness to sparse signal (large <code>mtry</code>) or highly correlated features (small <code>mtry</code>).</li>
<li><code>nodesize</code>: Stop splitting when the node sample size is no larger than <code>nodesize</code>. This works similar to <span class="math inline">\(k\)</span> in a KNN model. However, its effect can be greatly affected by other tuning parameters.</li>
</ul>
<p>Using the <code>randomForest</code> package, we can fit the model. It is difficult to visualize this when <code>p &gt; 2</code>. But we can look at the testing error.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="random-forests.html#cb161-1" tabindex="-1"></a>  <span class="co"># generate some data with larger p</span></span>
<span id="cb161-2"><a href="random-forests.html#cb161-2" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb161-3"><a href="random-forests.html#cb161-3" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb161-4"><a href="random-forests.html#cb161-4" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb161-5"><a href="random-forests.html#cb161-5" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n<span class="sc">*</span>p, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), n, p)</span>
<span id="cb161-6"><a href="random-forests.html#cb161-6" tabindex="-1"></a>  x1 <span class="ot">=</span> X[, <span class="dv">1</span>]</span>
<span id="cb161-7"><a href="random-forests.html#cb161-7" tabindex="-1"></a>  x2 <span class="ot">=</span> X[, <span class="dv">2</span>]</span>
<span id="cb161-8"><a href="random-forests.html#cb161-8" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>((x1 <span class="sc">+</span> x2 <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> (x1 <span class="sc">+</span> x2 <span class="sc">&lt;</span> <span class="fl">0.5</span>), <span class="fl">0.8</span>, <span class="fl">0.2</span>))</span>
<span id="cb161-9"><a href="random-forests.html#cb161-9" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span>
<span id="cb161-10"><a href="random-forests.html#cb161-10" tabindex="-1"></a></span>
<span id="cb161-11"><a href="random-forests.html#cb161-11" tabindex="-1"></a>  <span class="co"># fit random forests with a selected tuning</span></span>
<span id="cb161-12"><a href="random-forests.html#cb161-12" tabindex="-1"></a>  <span class="fu">library</span>(randomForest)</span>
<span id="cb161-13"><a href="random-forests.html#cb161-13" tabindex="-1"></a><span class="do">## randomForest 4.7-1.2</span></span>
<span id="cb161-14"><a href="random-forests.html#cb161-14" tabindex="-1"></a><span class="do">## Type rfNews() to see new features/changes/bug fixes.</span></span>
<span id="cb161-15"><a href="random-forests.html#cb161-15" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb161-16"><a href="random-forests.html#cb161-16" tabindex="-1"></a><span class="do">## Attaching package: &#39;randomForest&#39;</span></span>
<span id="cb161-17"><a href="random-forests.html#cb161-17" tabindex="-1"></a><span class="do">## The following object is masked from &#39;package:ggplot2&#39;:</span></span>
<span id="cb161-18"><a href="random-forests.html#cb161-18" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb161-19"><a href="random-forests.html#cb161-19" tabindex="-1"></a><span class="do">##     margin</span></span>
<span id="cb161-20"><a href="random-forests.html#cb161-20" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(X, <span class="fu">as.factor</span>(y), <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb161-21"><a href="random-forests.html#cb161-21" tabindex="-1"></a>                        <span class="at">mtry =</span> <span class="dv">7</span>, <span class="at">nodesize =</span> <span class="dv">10</span>, <span class="at">sampsize =</span> <span class="dv">800</span>)</span></code></pre></div>
<p>Instead of generating a set of testing samples labels, let’s directly compare with the “true” decision rule, the Bayes rule.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="random-forests.html#cb162-1" tabindex="-1"></a>  <span class="co"># the testing data </span></span>
<span id="cb162-2"><a href="random-forests.html#cb162-2" tabindex="-1"></a>  Xtest <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n<span class="sc">*</span>p, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), n, p)</span>
<span id="cb162-3"><a href="random-forests.html#cb162-3" tabindex="-1"></a>  </span>
<span id="cb162-4"><a href="random-forests.html#cb162-4" tabindex="-1"></a>  <span class="co"># the Bayes rule</span></span>
<span id="cb162-5"><a href="random-forests.html#cb162-5" tabindex="-1"></a>  BayesRule <span class="ot">=</span> <span class="fu">ifelse</span>((Xtest[, <span class="dv">1</span>] <span class="sc">+</span> Xtest[, <span class="dv">2</span>] <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">&amp;</span> </span>
<span id="cb162-6"><a href="random-forests.html#cb162-6" tabindex="-1"></a>                     (Xtest[, <span class="dv">1</span>] <span class="sc">+</span> Xtest[, <span class="dv">2</span>] <span class="sc">&lt;</span> <span class="fl">0.5</span>), <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb162-7"><a href="random-forests.html#cb162-7" tabindex="-1"></a>  </span>
<span id="cb162-8"><a href="random-forests.html#cb162-8" tabindex="-1"></a>  <span class="fu">mean</span>( (<span class="fu">predict</span>(rf.fit, Xtest) <span class="sc">==</span> <span class="st">&quot;1&quot;</span>) <span class="sc">==</span> BayesRule )</span>
<span id="cb162-9"><a href="random-forests.html#cb162-9" tabindex="-1"></a><span class="do">## [1] 0.785</span></span></code></pre></div>
</div>
<div id="kernel-view-of-random-forests" class="section level2 hasAnchor" number="21.3">
<h2><span class="header-section-number">21.3</span> Kernel view of Random Forests<a href="random-forests.html#kernel-view-of-random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For this part, we need to install a new package <code>RLT</code>. This is a package in development. But you can install the current version from GitHub <a href="https://github.com/teazrq/RLT">(ver <span class="math inline">\(\geq\)</span> 4.2.6)</a>. Please note that the current CRAN version (ver. 3.2.5) does not work for this part. Use the following code to install the package. If you are using MacOS, then you need to follow <a href="https://teazrq.github.io/random-forests-tutorial/rlab/basics/packages.html">this guild</a> to install the package for OpenMP.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="random-forests.html#cb163-1" tabindex="-1"></a>  <span class="co"># install.packages(&quot;devtools&quot;)</span></span>
<span id="cb163-2"><a href="random-forests.html#cb163-2" tabindex="-1"></a>  devtools<span class="sc">::</span><span class="fu">install_github</span>(<span class="st">&quot;teazrq/RLT&quot;</span>)</span></code></pre></div>
<p>Similar to a tree model, random forest can also be viewed as kernel estimator. Essentially its a stacking of kernels induced from all trees. The idea has been illustrated in <span class="citation">Scornet (<a href="#ref-scornet2016random">2016</a>)</span>. However, since random forest is a random algorithm, each tree can be slightly different from each other. To incorporate this, denote the randomness of a tree estimator by <span class="math inline">\(\eta\)</span>, which can affect how the tree is constructed. Suppose we fit <span class="math inline">\(B\)</span> trees in a random forest, with each tree denoted as <span class="math inline">\({\cal T_b}\)</span>, then a random forest estimator can be expressed as</p>
<p><span class="math display">\[
\hat{f}(x) = \frac{1}{B} \sum_{b = 1}^B \frac{\sum_i K_{\cal T_b}(x, x_i; \eta_b) y_i}{ \sum_i K_{\cal T_b}(x, x_i; \eta_b) }.
\]</span></p>
<p>Note that in this expression, the denominators in each tree estimator are different since we may randomly end-up with some sample size smaller than the <code>nodesize</code> after a split. However, we can still think each terminal nodes as having roughly the same size. Hence, we could also consider an alternative kernel induced from random forest, which aligns with traditional kernel estimators.</p>
<p><span class="math display">\[
\hat{f}(x) = \frac{ \sum_i y_i \sum_{b = 1}^B K_{\cal T_b}(x, x_i) }{ \sum_i \sum_{b = 1}^B K_{\cal T_b}(x, x_i) }.
\]</span></p>
<p>In this case, the kernel function is</p>
<p><span class="math display">\[
K_\text{RF}(x, x_i) = \sum_{b = 1}^B K_{\cal T_b}(x, x_i),
\]</span></p>
<p>which counts how many times <span class="math inline">\(x\)</span> falls into the same terminal node as observation <span class="math inline">\(x_i\)</span>. Hence, this kernel representation can be incorporated into many machine learning algorithms. For example, if we are interested in a supervised clustering setting, we can first fit a random forest model and perform spectral clustering using the induced kernel matrix. We can also use the kernel ridge regression with the kernel induced. Let’s generate a new dataset with 5 continuous variables. The true model depends on just the first two variables.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="random-forests.html#cb164-1" tabindex="-1"></a>  <span class="co"># generate data</span></span>
<span id="cb164-2"><a href="random-forests.html#cb164-2" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">1000</span>; p <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb164-3"><a href="random-forests.html#cb164-3" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb164-4"><a href="random-forests.html#cb164-4" tabindex="-1"></a>  y <span class="ot">=</span> X[, <span class="dv">1</span>] <span class="sc">+</span> X[, <span class="dv">2</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span></code></pre></div>
<p>If we fit just one tree, there could be different variations based on the randomness. Please note that the name of these parameters can be different in the <code>RLT</code> package.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="random-forests.html#cb165-1" tabindex="-1"></a>  <span class="fu">library</span>(RLT)</span>
<span id="cb165-2"><a href="random-forests.html#cb165-2" tabindex="-1"></a><span class="do">## RLT and Random Forests v4.2.6</span></span>
<span id="cb165-3"><a href="random-forests.html#cb165-3" tabindex="-1"></a><span class="do">## pre-release at github.com/teazrq/RLT</span></span>
<span id="cb165-4"><a href="random-forests.html#cb165-4" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb165-5"><a href="random-forests.html#cb165-5" tabindex="-1"></a></span>
<span id="cb165-6"><a href="random-forests.html#cb165-6" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span>
<span id="cb165-7"><a href="random-forests.html#cb165-7" tabindex="-1"></a>  {</span>
<span id="cb165-8"><a href="random-forests.html#cb165-8" tabindex="-1"></a>    <span class="co"># fit a model with one tree</span></span>
<span id="cb165-9"><a href="random-forests.html#cb165-9" tabindex="-1"></a>    RLTfit <span class="ot">&lt;-</span> <span class="fu">RLT</span>(X, y, <span class="at">ntrees =</span> <span class="dv">1</span>, <span class="at">nmin =</span> <span class="dv">30</span>, <span class="at">mtry =</span> <span class="dv">5</span>,</span>
<span id="cb165-10"><a href="random-forests.html#cb165-10" tabindex="-1"></a>                  <span class="at">split.gen =</span> <span class="st">&quot;best&quot;</span>, <span class="at">resample.prob =</span> <span class="dv">1</span>,</span>
<span id="cb165-11"><a href="random-forests.html#cb165-11" tabindex="-1"></a>                  <span class="at">resample.replace =</span> <span class="cn">TRUE</span>,</span>
<span id="cb165-12"><a href="random-forests.html#cb165-12" tabindex="-1"></a>                  <span class="at">param.control =</span> <span class="fu">list</span>(<span class="st">&quot;resample.track&quot;</span> <span class="ot">=</span> <span class="cn">TRUE</span>))</span>
<span id="cb165-13"><a href="random-forests.html#cb165-13" tabindex="-1"></a>  </span>
<span id="cb165-14"><a href="random-forests.html#cb165-14" tabindex="-1"></a>    <span class="co"># target point 1</span></span>
<span id="cb165-15"><a href="random-forests.html#cb165-15" tabindex="-1"></a>    newX <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), </span>
<span id="cb165-16"><a href="random-forests.html#cb165-16" tabindex="-1"></a>                  <span class="dv">1</span>, <span class="dv">5</span>)</span>
<span id="cb165-17"><a href="random-forests.html#cb165-17" tabindex="-1"></a>    </span>
<span id="cb165-18"><a href="random-forests.html#cb165-18" tabindex="-1"></a>    KernelW <span class="ot">=</span> <span class="fu">forest.kernel</span>(RLTfit, <span class="at">X1 =</span> newX, <span class="at">X2 =</span> X, <span class="at">vs.train =</span> <span class="cn">TRUE</span>)<span class="sc">$</span>Kernel</span>
<span id="cb165-19"><a href="random-forests.html#cb165-19" tabindex="-1"></a>      </span>
<span id="cb165-20"><a href="random-forests.html#cb165-20" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb165-21"><a href="random-forests.html#cb165-21" tabindex="-1"></a>    <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb165-22"><a href="random-forests.html#cb165-22" tabindex="-1"></a>    <span class="fu">points</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> KernelW<span class="sc">&gt;</span><span class="dv">0</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb165-23"><a href="random-forests.html#cb165-23" tabindex="-1"></a>    <span class="fu">points</span>(newX[<span class="dv">1</span>], newX[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">5</span>)  </span>
<span id="cb165-24"><a href="random-forests.html#cb165-24" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-256-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>If we stack all of them, we obtain a random forest kernel.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="random-forests.html#cb166-1" tabindex="-1"></a>  RLTfit <span class="ot">&lt;-</span> <span class="fu">RLT</span>(X, y, <span class="at">ntrees =</span> <span class="dv">1000</span>, <span class="at">nmin =</span> <span class="dv">10</span>, <span class="at">mtry =</span> <span class="dv">5</span>,</span>
<span id="cb166-2"><a href="random-forests.html#cb166-2" tabindex="-1"></a>                <span class="at">split.gen =</span> <span class="st">&quot;best&quot;</span>, <span class="at">resample.prob =</span> <span class="dv">1</span>,</span>
<span id="cb166-3"><a href="random-forests.html#cb166-3" tabindex="-1"></a>                <span class="at">resample.replace =</span> <span class="cn">TRUE</span>, </span>
<span id="cb166-4"><a href="random-forests.html#cb166-4" tabindex="-1"></a>                <span class="at">param.control =</span> <span class="fu">list</span>(<span class="st">&quot;resample.track&quot;</span> <span class="ot">=</span> <span class="cn">TRUE</span>))</span>
<span id="cb166-5"><a href="random-forests.html#cb166-5" tabindex="-1"></a></span>
<span id="cb166-6"><a href="random-forests.html#cb166-6" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb166-7"><a href="random-forests.html#cb166-7" tabindex="-1"></a>  </span>
<span id="cb166-8"><a href="random-forests.html#cb166-8" tabindex="-1"></a>  <span class="co"># target point 1</span></span>
<span id="cb166-9"><a href="random-forests.html#cb166-9" tabindex="-1"></a>  newX <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), </span>
<span id="cb166-10"><a href="random-forests.html#cb166-10" tabindex="-1"></a>                <span class="dv">1</span>, <span class="dv">5</span>)</span>
<span id="cb166-11"><a href="random-forests.html#cb166-11" tabindex="-1"></a>  </span>
<span id="cb166-12"><a href="random-forests.html#cb166-12" tabindex="-1"></a>  KernelW <span class="ot">=</span> <span class="fu">forest.kernel</span>(RLTfit, <span class="at">X1 =</span> newX, <span class="at">X2 =</span> X, <span class="at">vs.train =</span> <span class="cn">TRUE</span>)<span class="sc">$</span>Kernel</span>
<span id="cb166-13"><a href="random-forests.html#cb166-13" tabindex="-1"></a>  </span>
<span id="cb166-14"><a href="random-forests.html#cb166-14" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb166-15"><a href="random-forests.html#cb166-15" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb166-16"><a href="random-forests.html#cb166-16" tabindex="-1"></a>  <span class="fu">points</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">cex =</span> <span class="dv">10</span><span class="sc">*</span>KernelW<span class="sc">/</span><span class="dv">1000</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb166-17"><a href="random-forests.html#cb166-17" tabindex="-1"></a>  <span class="fu">points</span>(newX[<span class="dv">1</span>], newX[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="dv">4</span>, <span class="at">lwd =</span> <span class="dv">5</span>)  </span>
<span id="cb166-18"><a href="random-forests.html#cb166-18" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="st">&quot;Target Point&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, </span>
<span id="cb166-19"><a href="random-forests.html#cb166-19" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="dv">5</span>, <span class="at">lty =</span> <span class="cn">NA</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb166-20"><a href="random-forests.html#cb166-20" tabindex="-1"></a>  </span>
<span id="cb166-21"><a href="random-forests.html#cb166-21" tabindex="-1"></a>  </span>
<span id="cb166-22"><a href="random-forests.html#cb166-22" tabindex="-1"></a>  <span class="co"># target point 2 </span></span>
<span id="cb166-23"><a href="random-forests.html#cb166-23" tabindex="-1"></a>  newX <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), </span>
<span id="cb166-24"><a href="random-forests.html#cb166-24" tabindex="-1"></a>                <span class="dv">1</span>, <span class="dv">5</span>)</span>
<span id="cb166-25"><a href="random-forests.html#cb166-25" tabindex="-1"></a>  </span>
<span id="cb166-26"><a href="random-forests.html#cb166-26" tabindex="-1"></a>  KernelW <span class="ot">=</span> <span class="fu">forest.kernel</span>(RLTfit, <span class="at">X1 =</span> newX, <span class="at">X2 =</span> X, <span class="at">vs.train =</span> <span class="cn">TRUE</span>)<span class="sc">$</span>Kernel</span>
<span id="cb166-27"><a href="random-forests.html#cb166-27" tabindex="-1"></a>  </span>
<span id="cb166-28"><a href="random-forests.html#cb166-28" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb166-29"><a href="random-forests.html#cb166-29" tabindex="-1"></a>  <span class="fu">plot</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb166-30"><a href="random-forests.html#cb166-30" tabindex="-1"></a>  <span class="fu">points</span>(X[, <span class="dv">1</span>], X[, <span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">cex =</span> <span class="dv">10</span><span class="sc">*</span>KernelW<span class="sc">/</span><span class="dv">1000</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb166-31"><a href="random-forests.html#cb166-31" tabindex="-1"></a>  <span class="fu">points</span>(newX[<span class="dv">1</span>], newX[<span class="dv">2</span>], <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="dv">4</span>, <span class="at">lwd =</span> <span class="dv">5</span>)</span>
<span id="cb166-32"><a href="random-forests.html#cb166-32" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="st">&quot;Target Point&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, </span>
<span id="cb166-33"><a href="random-forests.html#cb166-33" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="dv">5</span>, <span class="at">lty =</span> <span class="cn">NA</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-257-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="variable-importance" class="section level2 hasAnchor" number="21.4">
<h2><span class="header-section-number">21.4</span> Variable Importance<a href="random-forests.html#variable-importance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The idea of variable importance is to identify which features (or variables) are most influential in predicting the outcome based on the fitted model. Variable importance is typically assessed through techniques like mean decrease accuracy, which measures the decrease in model accuracy when a variable’s values are permuted across the out-of-bag samples, thereby disrupting the relationship between that variable and the target. Alternatively, it can also be measured using the mean decrease impurity, which calculates the total reduction in the criterion (Gini impurity, entropy, or mean squared error) that each variable provides when used in trees, averaged over all trees in the forest. The calculation can be summarized by the following steps:</p>
<ul>
<li>Train the Random Forest: Fit a random forest model to your data using all available variables.</li>
<li>Out-of-Bag Evaluation: For each tree in the forest, predict the outcome for the out-of-bag (OOB) samples—these are the samples not used in the construction of that particular tree. Compute the OOB accuracy (or another relevant metric like AUC for classification, MSE for regression) for these predictions.</li>
<li>Permute Variable &amp; Re-evaluate: For each variable of interest, randomly permute its values among the OOB samples. Then, use the same tree to make predictions on these “shuffled” data and compute the accuracy (or other metrics) again.</li>
<li>Calculate Decrease in Accuracy: Compare the accuracy obtained from the permuted data to the original OOB accuracy for each tree. The difference is a measure of the importance of the variable for that specific tree.</li>
<li>Average Over All Trees: Aggregate these importance measures across all trees in the forest to get a single importance score for each variable.</li>
</ul>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="random-forests.html#cb167-1" tabindex="-1"></a>    rf.fit <span class="ot">=</span> <span class="fu">randomForest</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb167-2"><a href="random-forests.html#cb167-2" tabindex="-1"></a>                      <span class="at">nodesize =</span> <span class="dv">10</span>, <span class="at">mtry =</span> <span class="dv">20</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb167-3"><a href="random-forests.html#cb167-3" tabindex="-1"></a></span>
<span id="cb167-4"><a href="random-forests.html#cb167-4" tabindex="-1"></a>    <span class="co"># variable importance for %IncMSE (1st column)</span></span>
<span id="cb167-5"><a href="random-forests.html#cb167-5" tabindex="-1"></a>    rf.fit<span class="sc">$</span>importance</span></code></pre></div>
</div>
<div id="adaptiveness-of-random-forest-kernel" class="section level2 hasAnchor" number="21.5">
<h2><span class="header-section-number">21.5</span> Adaptiveness of Random Forest Kernel<a href="random-forests.html#adaptiveness-of-random-forest-kernel" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>However, random forest can adapt pretty well in a high-dimensional, especially sparse setting. This is because of the greedy splitting rule selection. The adaptiveness works in a way that, it tends to ignore covariates that are not effective on explaining the variation of <span class="math inline">\(Y\)</span>. Hence, making the model similar to a kernel method on a low-dimensional space. The following example illustrate this effect in a two-dimensional case. We can see that the outcome is only related to the first dimension. Hence, when setting <code>mtry = 2</code>, we will almost always prefer to split on the first variable, making its neighbors very close to the target prediction point <span class="math inline">\((0, 0)^T\)</span>.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r samplecode"><code class="sourceCode r"><span id="cb168-1"><a href="random-forests.html#cb168-1" tabindex="-1"></a>  <span class="co"># generate some data </span></span>
<span id="cb168-2"><a href="random-forests.html#cb168-2" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb168-3"><a href="random-forests.html#cb168-3" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">400</span></span>
<span id="cb168-4"><a href="random-forests.html#cb168-4" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb168-5"><a href="random-forests.html#cb168-5" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb168-6"><a href="random-forests.html#cb168-6" tabindex="-1"></a>  y <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>x1 <span class="sc">+</span> <span class="fl">0.2</span><span class="sc">*</span><span class="fu">rnorm</span>(n)</span>
<span id="cb168-7"><a href="random-forests.html#cb168-7" tabindex="-1"></a>  xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), <span class="at">x2 =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="fl">0.01</span>))</span>
<span id="cb168-8"><a href="random-forests.html#cb168-8" tabindex="-1"></a>  </span>
<span id="cb168-9"><a href="random-forests.html#cb168-9" tabindex="-1"></a>  <span class="co"># fit forest</span></span>
<span id="cb168-10"><a href="random-forests.html#cb168-10" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">RLT</span>(<span class="at">x =</span> <span class="fu">data.frame</span>(x1, x2), <span class="at">y =</span> y, <span class="at">model =</span> <span class="st">&quot;regression&quot;</span>,</span>
<span id="cb168-11"><a href="random-forests.html#cb168-11" tabindex="-1"></a>               <span class="at">mtry =</span> <span class="dv">2</span>, <span class="at">nmin =</span> <span class="dv">40</span>, <span class="at">param.control =</span> <span class="fu">list</span>(<span class="at">resample.track =</span> <span class="cn">TRUE</span>))</span>
<span id="cb168-12"><a href="random-forests.html#cb168-12" tabindex="-1"></a>  </span>
<span id="cb168-13"><a href="random-forests.html#cb168-13" tabindex="-1"></a>  <span class="co"># calculate kernel</span></span>
<span id="cb168-14"><a href="random-forests.html#cb168-14" tabindex="-1"></a>  rf.kernel <span class="ot">=</span> <span class="fu">forest.kernel</span>(rf.fit, <span class="at">X1 =</span> <span class="fu">data.frame</span>(<span class="st">&quot;x1&quot;</span> <span class="ot">=</span> <span class="dv">0</span>, <span class="st">&quot;x2&quot;</span> <span class="ot">=</span> <span class="dv">0</span>), </span>
<span id="cb168-15"><a href="random-forests.html#cb168-15" tabindex="-1"></a>                            <span class="at">X2 =</span> <span class="fu">data.frame</span>(x1, x2), <span class="at">vs.train =</span> <span class="cn">TRUE</span>)<span class="sc">$</span>Kernel</span>
<span id="cb168-16"><a href="random-forests.html#cb168-16" tabindex="-1"></a>  </span>
<span id="cb168-17"><a href="random-forests.html#cb168-17" tabindex="-1"></a>  <span class="co"># kernel weights</span></span>
<span id="cb168-18"><a href="random-forests.html#cb168-18" tabindex="-1"></a>  <span class="fu">plot</span>(x1, x2, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">cex =</span> (rf.kernel <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">^</span><span class="fl">0.15</span> <span class="sc">-</span> <span class="fl">0.7</span>)</span>
<span id="cb168-19"><a href="random-forests.html#cb168-19" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">18</span>, <span class="at">cex =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-260-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The tuning parameter <code>mtry</code> has a very strong effect on controlling this greediness. When <code>mtry</code> is large, we will be very greedy on selecting the true signal variable to split. In a high-dimensional setting, we may only use a few variables before reaching a terminal node, making the model only rely a few dimensions. When we use a very small <code>mtry</code>, the model behaves similarly to a regular kernel estimator with good smoothing (small variance) property. However, since it is effectively randomly selecting a dimension to split, the bandwidth on each dimension would also be similar but large since we can only afford a few splits before the node size becomes too small. This can be seen from the following example, with <code>mtry = 1</code>.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r samplecode"><code class="sourceCode r"><span id="cb169-1"><a href="random-forests.html#cb169-1" tabindex="-1"></a>  <span class="co"># fit forest</span></span>
<span id="cb169-2"><a href="random-forests.html#cb169-2" tabindex="-1"></a>  rf.fit <span class="ot">=</span> <span class="fu">RLT</span>(<span class="at">x =</span> <span class="fu">data.frame</span>(x1, x2), <span class="at">y =</span> y, <span class="at">model =</span> <span class="st">&quot;regression&quot;</span>,</span>
<span id="cb169-3"><a href="random-forests.html#cb169-3" tabindex="-1"></a>               <span class="at">mtry =</span> <span class="dv">1</span>, <span class="at">nmin =</span> <span class="dv">40</span>, <span class="at">param.control =</span> <span class="fu">list</span>(<span class="at">resample.track =</span> <span class="cn">TRUE</span>))</span>
<span id="cb169-4"><a href="random-forests.html#cb169-4" tabindex="-1"></a>  </span>
<span id="cb169-5"><a href="random-forests.html#cb169-5" tabindex="-1"></a>  <span class="co"># calculate kernel</span></span>
<span id="cb169-6"><a href="random-forests.html#cb169-6" tabindex="-1"></a>  rf.kernel <span class="ot">=</span> <span class="fu">forest.kernel</span>(rf.fit, <span class="at">X1 =</span> <span class="fu">data.frame</span>(<span class="st">&quot;x1&quot;</span> <span class="ot">=</span> <span class="dv">0</span>, <span class="st">&quot;x2&quot;</span> <span class="ot">=</span> <span class="dv">0</span>), </span>
<span id="cb169-7"><a href="random-forests.html#cb169-7" tabindex="-1"></a>                            <span class="at">X2 =</span> <span class="fu">data.frame</span>(x1, x2), <span class="at">vs.train =</span> <span class="cn">TRUE</span>)<span class="sc">$</span>Kernel</span>
<span id="cb169-8"><a href="random-forests.html#cb169-8" tabindex="-1"></a>  </span>
<span id="cb169-9"><a href="random-forests.html#cb169-9" tabindex="-1"></a>  <span class="co"># kernel weights</span></span>
<span id="cb169-10"><a href="random-forests.html#cb169-10" tabindex="-1"></a>  <span class="fu">plot</span>(x1, x2, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>, <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">cex =</span> (rf.kernel <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">^</span><span class="fl">0.15</span> <span class="sc">-</span> <span class="fl">0.7</span>)</span>
<span id="cb169-11"><a href="random-forests.html#cb169-11" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">18</span>, <span class="at">cex =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-262-1.png" width="45%" style="display: block; margin: auto;" /></p>

<div style="display:none;">
<!-- Conflict \def\bf{\mathbf{f}} -->
</div>
</div>
</div>
<h3> Reference<a href="reference.html#reference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-breiman1996bagging" class="csl-entry">
Breiman, Leo. 1996. <span>“Bagging Predictors.”</span> <em>Machine Learning</em> 24 (2): 123–40.
</div>
<div id="ref-breiman2001random" class="csl-entry">
———. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-ho1998random" class="csl-entry">
Ho, Tin Kam. 1998. <span>“The Random Subspace Method for Constructing Decision Forests.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 20 (8): 832–44.
</div>
<div id="ref-scornet2016random" class="csl-entry">
Scornet, Erwan. 2016. <span>“Random Forests and Kernel Methods.”</span> <em>IEEE Transactions on Information Theory</em> 62 (3): 1485–1500.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification-and-regression-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="adaboost.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "sepia",
    "family": "serif",
    "size": 1
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
