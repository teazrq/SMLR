<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Ridge Regression | Statistical Learning and Machine Learning with R</title>
  <meta name="description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Ridge Regression | Statistical Learning and Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Ridge Regression | Statistical Learning and Machine Learning with R" />
  
  <meta name="twitter:description" content="A textbook for STAT 542 and 432 at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2022-04-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="linear-regression-and-model-selection.html"/>
<link rel="next" href="lasso.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.20/datatables.js"></script>
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.10.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning and Machine Learning with R</a></li>

<li class="divider"></li>
<li><a href="index.html#preface">Preface<span></span></a>
<ul>
<li><a href="index.html#target-audience">Target Audience<span></span></a></li>
<li><a href="index.html#whats-covered">What’s Covered?<span></span></a></li>
<li><a href="index.html#acknowledgements">Acknowledgements<span></span></a></li>
<li><a href="index.html#license">License<span></span></a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge<span></span></b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides<span></span></a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations<span></span></a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data<span></span></a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions<span></span></a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers<span></span></a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources<span></span></a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code<span></span></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks<span></span></a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data<span></span></a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory<span></span></a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting<span></span></a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options<span></span></a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX<span></span></a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX<span></span></a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options<span></span></a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>3</b> Linear Algebra Basics<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>3.1</b> Definition<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>4</b> Optimization Basics<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>4.1</b> Basic Concept<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>4.2</b> Global vs. Local Optima<span></span></a></li>
<li class="chapter" data-level="4.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>4.3</b> Example: Linear Regression using <code>optim()</code><span></span></a></li>
<li class="chapter" data-level="4.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>4.4</b> First and Second Order Properties<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>4.5</b> Algorithm<span></span></a></li>
<li class="chapter" data-level="4.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>4.6</b> Second-order Methods<span></span></a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>4.6.1</b> Newton’s Method<span></span></a></li>
<li class="chapter" data-level="4.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>4.6.2</b> Quasi-Newton Methods<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>4.7</b> First-order Methods<span></span></a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>4.7.1</b> Gradient Descent<span></span></a></li>
<li class="chapter" data-level="4.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.7.2</b> Gradient Descent Example: Linear Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>4.8</b> Coordinate Descent<span></span></a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.8.1</b> Coordinate Descent Example: Linear Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9</b> Stocastic Gradient Descent<span></span></a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9.1</b> Mini-batch Stocastic Gradient Descent<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>4.10</b> Lagrangian Multiplier for Constrained Problems<span></span></a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models<span></span></b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>5</b> Linear Regression and Model Selection<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>5.1</b> Example: real estate data<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>5.2</b> Notation and Basic Properties<span></span></a></li>
<li class="chapter" data-level="5.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>5.3</b> Using the <code>lm()</code> Function<span></span></a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Adding Covariates<span></span></a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>5.3.2</b> Categorical Variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>5.4</b> Model Selection Criteria<span></span></a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>5.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span><span></span></a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>5.4.2</b> Using AIC and BIC<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>5.5</b> Model Selection Algorithms<span></span></a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>5.5.1</b> Best Subset Selection with <code>leaps</code><span></span></a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>5.5.2</b> Step-wise regression using <code>step()</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>5.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span><span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>6</b> Ridge Regression<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>6.1</b> Motivation: Correlated Variables and Convexity<span></span></a></li>
<li class="chapter" data-level="6.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>6.2</b> Ridge Penalty and the Reduced Variation<span></span></a></li>
<li class="chapter" data-level="6.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>6.3</b> Bias and Variance of Ridge Regression<span></span></a></li>
<li class="chapter" data-level="6.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>6.4</b> Degrees of Freedom<span></span></a></li>
<li class="chapter" data-level="6.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>6.5</b> Using the <code>lm.ridge()</code> function<span></span></a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>6.5.1</b> Scaling Issue<span></span></a></li>
<li class="chapter" data-level="6.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>6.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>6.6</b> Cross-validation<span></span></a></li>
<li class="chapter" data-level="6.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.7</b> Leave-one-out cross-validation<span></span></a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>6.7.1</b> Generalized cross-validation<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>6.8</b> The <code>glmnet</code> package<span></span></a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>6.8.1</b> Scaling Issue<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>7</b> Lasso<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>7.1</b> One-Variable Lasso and Shrinkage<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>7.2</b> Constrained Optimization View<span></span></a></li>
<li class="chapter" data-level="7.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>7.3</b> The Solution Path<span></span></a></li>
<li class="chapter" data-level="7.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>7.4</b> Path-wise Coordinate Descent<span></span></a></li>
<li class="chapter" data-level="7.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>7.5</b> Using the <code>glmnet</code> package<span></span></a></li>
<li class="chapter" data-level="7.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>7.6</b> Elastic-Net<span></span></a></li>
</ul></li>
<li class="part"><span><b>III Nonparametric Models<span></span></b></span></li>
<li class="chapter" data-level="8" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>8</b> Spline<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="spline.html"><a href="spline.html#from-linear-to-nonlinear"><i class="fa fa-check"></i><b>8.1</b> From Linear to Nonlinear<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>8.2</b> A Motivating Example and Polynomials<span></span></a></li>
<li class="chapter" data-level="8.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>8.3</b> Piecewise Polynomials<span></span></a></li>
<li class="chapter" data-level="8.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>8.4</b> Splines<span></span></a></li>
<li class="chapter" data-level="8.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>8.5</b> Spline Basis<span></span></a></li>
<li class="chapter" data-level="8.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>8.6</b> Natural Cubic Spline<span></span></a></li>
<li class="chapter" data-level="8.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>8.7</b> Smoothing Spline<span></span></a></li>
<li class="chapter" data-level="8.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>8.8</b> Fitting Smoothing Splines<span></span></a></li>
<li class="chapter" data-level="8.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>8.9</b> Extending Splines to Multiple Varibles<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>9</b> K-Neariest Neighber<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>9.1</b> Definition<span></span></a></li>
<li class="chapter" data-level="9.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>9.2</b> Tuning <span class="math inline">\(k\)</span><span></span></a></li>
<li class="chapter" data-level="9.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>9.3</b> The Bias-variance Trade-off<span></span></a></li>
<li class="chapter" data-level="9.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>9.4</b> KNN for Classification<span></span></a></li>
<li class="chapter" data-level="9.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>9.5</b> Example 1: An artificial data<span></span></a></li>
<li class="chapter" data-level="9.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>9.6</b> Tuning with the <code>caret</code> Package<span></span></a></li>
<li class="chapter" data-level="9.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>9.7</b> Distance Measures<span></span></a></li>
<li class="chapter" data-level="9.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>9.8</b> 1NN Error Bound<span></span></a></li>
<li class="chapter" data-level="9.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>9.9</b> Example 2: Handwritten Digit Data<span></span></a></li>
<li class="chapter" data-level="9.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>9.10</b> Curse of Dimensionality<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>10</b> Kernel Smoothing<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>10.1</b> KNN vs. Kernel<span></span></a></li>
<li class="chapter" data-level="10.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>10.2</b> Kernel Density Estimations<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>10.3</b> Bias-variance trade-off<span></span></a></li>
<li class="chapter" data-level="10.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>10.4</b> Gaussian Kernel Regression<span></span></a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off-1"><i class="fa fa-check"></i><b>10.4.1</b> Bias-variance Trade-off<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>10.5</b> Choice of Kernel Functions<span></span></a></li>
<li class="chapter" data-level="10.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>10.6</b> Local Linear Regression<span></span></a></li>
<li class="chapter" data-level="10.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>10.7</b> Local Polynomial Regression<span></span></a></li>
<li class="chapter" data-level="10.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>10.8</b> R Implementations<span></span></a></li>
</ul></li>
<li class="part"><span><b>IV Classification Models<span></span></b></span></li>
<li class="chapter" data-level="11" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Logistic Regression<span></span></a>
<ul>
<li class="chapter" data-level="11.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>11.1</b> Modeling Binary Outcomes<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>11.2</b> Example: Cleveland Clinic Heart Disease Data<span></span></a></li>
<li class="chapter" data-level="11.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>11.3</b> Interpretation of the Parameters<span></span></a></li>
<li class="chapter" data-level="11.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Solving a Logistic Regression<span></span></a></li>
<li class="chapter" data-level="11.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>11.5</b> Example: South Africa Heart Data<span></span></a></li>
<li class="chapter" data-level="11.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Penalized Logistic Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>12</b> Discriminant Analysis<span></span></a>
<ul>
<li class="chapter" data-level="12.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>12.1</b> Bayes Rule<span></span></a></li>
<li class="chapter" data-level="12.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>12.2</b> Example: Linear Discriminant Analysis (LDA)<span></span></a></li>
<li class="chapter" data-level="12.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>12.3</b> Linear Discriminant Analysis<span></span></a></li>
<li class="chapter" data-level="12.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>12.4</b> Example: Quadratic Discriminant Analysis (QDA)<span></span></a></li>
<li class="chapter" data-level="12.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>12.5</b> Quadratic Discriminant Analysis<span></span></a></li>
<li class="chapter" data-level="12.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>12.6</b> Example: the Hand Written Digit Data<span></span></a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning Algorithms<span></span></b></span></li>
<li class="chapter" data-level="13" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>13</b> Support Vector Machines<span></span></a>
<ul>
<li class="chapter" data-level="13.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>13.1</b> Maximum-margin Classifier<span></span></a></li>
<li class="chapter" data-level="13.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>13.2</b> Linearly Separable SVM<span></span></a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>13.2.1</b> From Primal to Dual<span></span></a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>13.3</b> Linearly Non-separable SVM with Slack Variables<span></span></a></li>
<li class="chapter" data-level="13.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>13.4</b> Example: <code>SAheart</code> Data<span></span></a></li>
<li class="chapter" data-level="13.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>13.5</b> Nonlinear SVM via Kernel Trick<span></span></a></li>
<li class="chapter" data-level="13.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>13.6</b> Example: <code>mixture.example</code> Data<span></span></a></li>
<li class="chapter" data-level="13.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>13.7</b> SVM as a Penalized Model<span></span></a></li>
<li class="chapter" data-level="13.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>13.8</b> Kernel and Feature Maps: Another Example<span></span></a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>14</b> Reproducing Kernel Hilbert Space<span></span></a>
<ul>
<li class="chapter" data-level="14.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#constructing-the-rkhs"><i class="fa fa-check"></i><b>14.1</b> Constructing the RKHS<span></span></a></li>
<li class="chapter" data-level="14.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#properties-of-rkhs"><i class="fa fa-check"></i><b>14.2</b> Properties of RKHS<span></span></a></li>
<li class="chapter" data-level="14.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-representer-theorem"><i class="fa fa-check"></i><b>14.3</b> The Representer Theorem<span></span></a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>15</b> Kernel Ridge Regression<span></span></a>
<ul>
<li class="chapter" data-level="15.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#example-linear-kernel-and-ridge-regression"><i class="fa fa-check"></i><b>15.1</b> Example: Linear Kernel and Ridge Regression<span></span></a></li>
<li class="chapter" data-level="15.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#example-alternative-view"><i class="fa fa-check"></i><b>15.2</b> Example: Alternative View<span></span></a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>16</b> Classification and Regression Trees<span></span></a>
<ul>
<li class="chapter" data-level="16.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>16.1</b> Example: Classification Tree<span></span></a></li>
<li class="chapter" data-level="16.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>16.2</b> Splitting a Node<span></span></a></li>
<li class="chapter" data-level="16.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>16.3</b> Regression Trees<span></span></a></li>
<li class="chapter" data-level="16.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>16.4</b> Predicting a Target Point<span></span></a></li>
<li class="chapter" data-level="16.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>16.5</b> Tuning a Tree Model<span></span></a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>17</b> Random Forests<span></span></a>
<ul>
<li class="chapter" data-level="17.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>17.1</b> Bagging Predictors<span></span></a></li>
<li class="chapter" data-level="17.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>17.2</b> Random Forests<span></span></a></li>
<li class="chapter" data-level="17.3" data-path="random-forests.html"><a href="random-forests.html#effect-of-mtry"><i class="fa fa-check"></i><b>17.3</b> Effect of <code>mtry</code><span></span></a></li>
<li class="chapter" data-level="17.4" data-path="random-forests.html"><a href="random-forests.html#effect-of-nodesize"><i class="fa fa-check"></i><b>17.4</b> Effect of <code>nodesize</code><span></span></a></li>
<li class="chapter" data-level="17.5" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>17.5</b> Variable Importance<span></span></a></li>
<li class="chapter" data-level="17.6" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forets"><i class="fa fa-check"></i><b>17.6</b> Kernel view of Random Forets<span></span></a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>18</b> Boosting<span></span></a>
<ul>
<li class="chapter" data-level="18.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>18.1</b> AdaBoost<span></span></a></li>
<li class="chapter" data-level="18.2" data-path="boosting.html"><a href="boosting.html#training-error-of-adaboost"><i class="fa fa-check"></i><b>18.2</b> Training Error of AdaBoost<span></span></a></li>
</ul></li>
<li class="part"><span><b>VI Unsupervised Learning<span></span></b></span></li>
<li class="chapter" data-level="19" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>19</b> K-Means<span></span></a>
<ul>
<li class="chapter" data-level="19.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>19.1</b> Basic Concepts<span></span></a></li>
<li class="chapter" data-level="19.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>19.2</b> Example 1: <code>iris</code> data<span></span></a></li>
<li class="chapter" data-level="19.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>19.3</b> Example 2: clustering of image pixels<span></span></a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>20</b> Hierarchical Clustering<span></span></a>
<ul>
<li class="chapter" data-level="20.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>20.1</b> Basic Concepts<span></span></a></li>
<li class="chapter" data-level="20.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>20.2</b> Example 1: <code>iris</code> data<span></span></a></li>
<li class="chapter" data-level="20.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>20.3</b> Example 2: RNA Expression Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>21</b> Principle Component Analysis<span></span></a>
<ul>
<li class="chapter" data-level="21.1" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>21.1</b> Basic Concepts<span></span></a>
<ul>
<li class="chapter" data-level="21.1.1" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>21.1.1</b> Note: Scaling<span></span></a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>21.2</b> Example 1: <code>iris</code> Data<span></span></a></li>
<li class="chapter" data-level="21.3" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>21.3</b> Example 2: Handwritten Digits<span></span></a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>22</b> Self-Organizing Map<span></span></a>
<ul>
<li class="chapter" data-level="22.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>22.1</b> Basic Concepts<span></span></a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>23</b> Spectral Clustering<span></span></a>
<ul>
<li class="chapter" data-level="23.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#basic-concepts-4"><i class="fa fa-check"></i><b>23.1</b> Basic Concepts<span></span></a></li>
</ul></li>
<li class="part"><span><b>VII Reference<span></span></b></span></li>
<li class="chapter" data-level="24" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>24</b> Reference<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2022 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning and Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ridge-regression" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Ridge Regression<a href="ridge-regression.html#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Ridge regression was proposed by <span class="citation"><a href="#ref-hoerl1970ridge" role="doc-biblioref">Hoerl and Kennard</a> (<a href="#ref-hoerl1970ridge" role="doc-biblioref">1970</a>)</span>, but is also a special case of Tikhonov regularization. The essential idea is very simple: Knowing that the ordinary least squares (OLS) solution is not unique in an ill-posed problem, i.e., <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is not invertible, a ridge regression adds a ridge (diagonal matrix) on <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span>:</p>
<p><span class="math display">\[\widehat{\boldsymbol \beta}^\text{ridge} = (\mathbf{X}^\text{T}\mathbf{X}+ n \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y},\]</span>
It provides a solution of linear regression when multicollinearity happens, especially when the number of variables is larger than the sample size. Alternatively, this is also the solution of a regularized least square estimator. We add an <span class="math inline">\(\ell_2\)</span> penalty to the residual sum of squares, i.e.,</p>
<p><span class="math display">\[
\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; \mathop{\mathrm{arg\,min}}_{\boldsymbol \beta} (\mathbf{y}- \mathbf{X}\boldsymbol \beta)^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol \beta) + n \lambda \lVert\boldsymbol \beta\rVert_2^2\\
=&amp; \mathop{\mathrm{arg\,min}}_{\boldsymbol \beta} \frac{1}{n} \sum_{i=1}^n (y_i - x_i^\text{T}\boldsymbol \beta)^2 + \lambda \sum_{j=1}^p \beta_j^2,
\end{align}
\]</span></p>
<p>for some penalty <span class="math inline">\(\lambda &gt; 0\)</span>. Another approach that leads to the ridge regression is a constraint on the <span class="math inline">\(\ell_2\)</span> norm of the parameters, which will be introduced in the next Chapter. Ridge regression is used extensively in genetic analyses to address “small-<span class="math inline">\(n\)</span>-large-<span class="math inline">\(p\)</span>” problems. We will start with a motivation example and then discuss the bias-variance trade-off issue.</p>
<div id="motivation-correlated-variables-and-convexity" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Motivation: Correlated Variables and Convexity<a href="ridge-regression.html#motivation-correlated-variables-and-convexity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ridge regression has many advantages. Most notably, it can address highly correlated variables. From an optimization point of view, having highly correlated variables means that the objective function (<span class="math inline">\(\ell_2\)</span> loss) becomes “flat” along certain directions in the parameter domain. This can be seen from the following example, where the true parameters are both <span class="math inline">\(1\)</span> while the estimated parameters concludes almost all effects to the first variable. You can change different seed to observe the variability of these parameter estimates and notice that they are quite large. Instead, if we fit a ridge regression, the parameter estimates are relatively stable.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="ridge-regression.html#cb67-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb67-2"><a href="ridge-regression.html#cb67-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb67-3"><a href="ridge-regression.html#cb67-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">30</span></span>
<span id="cb67-4"><a href="ridge-regression.html#cb67-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb67-5"><a href="ridge-regression.html#cb67-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create highly correlated variables and a linear model</span></span>
<span id="cb67-6"><a href="ridge-regression.html#cb67-6" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb67-7"><a href="ridge-regression.html#cb67-7" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb67-8"><a href="ridge-regression.html#cb67-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb67-9"><a href="ridge-regression.html#cb67-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># compare parameter estimates</span></span>
<span id="cb67-10"><a href="ridge-regression.html#cb67-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="fu">lm</span>(y<span class="sc">~</span>X<span class="dv">-1</span>))<span class="sc">$</span>coef</span>
<span id="cb67-11"><a href="ridge-regression.html#cb67-11" aria-hidden="true" tabindex="-1"></a><span class="do">##     Estimate Std. Error    t value  Pr(&gt;|t|)</span></span>
<span id="cb67-12"><a href="ridge-regression.html#cb67-12" aria-hidden="true" tabindex="-1"></a><span class="do">## X1 1.8461255   1.294541 1.42608527 0.1648987</span></span>
<span id="cb67-13"><a href="ridge-regression.html#cb67-13" aria-hidden="true" tabindex="-1"></a><span class="do">## X2 0.0990278   1.321283 0.07494822 0.9407888</span></span>
<span id="cb67-14"><a href="ridge-regression.html#cb67-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb67-15"><a href="ridge-regression.html#cb67-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note that the true parameters are all 1&#39;s</span></span>
<span id="cb67-16"><a href="ridge-regression.html#cb67-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Be careful that the `lambda` parameter in lm.ridge is our (n*lambda)</span></span>
<span id="cb67-17"><a href="ridge-regression.html#cb67-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm.ridge</span>(y<span class="sc">~</span>X<span class="dv">-1</span>, <span class="at">lambda=</span><span class="dv">5</span>)</span>
<span id="cb67-18"><a href="ridge-regression.html#cb67-18" aria-hidden="true" tabindex="-1"></a><span class="do">##        X1        X2 </span></span>
<span id="cb67-19"><a href="ridge-regression.html#cb67-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 0.9413221 0.8693253</span></span></code></pre></div>
<p>The variance of both <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are quite large. This is expected because we know from linear regression that the variance of <span class="math inline">\(\widehat{\boldsymbol \beta}\)</span> is <span class="math inline">\(\sigma^2 (\mathbf{X}^\text{T}\mathbf{X})^{-1}\)</span>. However, since the columns of <span class="math inline">\(\mathbf{X}\)</span> are highly correlated, the smallest eigenvalue of <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is close to 0, making the largest eigenvalue of <span class="math inline">\((\mathbf{X}^\text{T}\mathbf{X})^{-1}\)</span> very large. This can also be interpreted through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="ridge-regression.html#cb68-1" aria-hidden="true" tabindex="-1"></a>  beta1 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="fl">0.005</span>)</span>
<span id="cb68-2"><a href="ridge-regression.html#cb68-2" aria-hidden="true" tabindex="-1"></a>  beta2 <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="fl">0.005</span>)</span>
<span id="cb68-3"><a href="ridge-regression.html#cb68-3" aria-hidden="true" tabindex="-1"></a>  allbeta <span class="ot">&lt;-</span> <span class="fu">data.matrix</span>(<span class="fu">expand.grid</span>(beta1, beta2))</span>
<span id="cb68-4"><a href="ridge-regression.html#cb68-4" aria-hidden="true" tabindex="-1"></a>  rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>), X, y), </span>
<span id="cb68-5"><a href="ridge-regression.html#cb68-5" aria-hidden="true" tabindex="-1"></a>                <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb68-6"><a href="ridge-regression.html#cb68-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb68-7"><a href="ridge-regression.html#cb68-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># quantile levels for drawing contour</span></span>
<span id="cb68-8"><a href="ridge-regression.html#cb68-8" aria-hidden="true" tabindex="-1"></a>  quanlvl <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>)</span>
<span id="cb68-9"><a href="ridge-regression.html#cb68-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb68-10"><a href="ridge-regression.html#cb68-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># plot the contour</span></span>
<span id="cb68-11"><a href="ridge-regression.html#cb68-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb68-12"><a href="ridge-regression.html#cb68-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb68-13"><a href="ridge-regression.html#cb68-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb68-14"><a href="ridge-regression.html#cb68-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the truth</span></span>
<span id="cb68-15"><a href="ridge-regression.html#cb68-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb68-16"><a href="ridge-regression.html#cb68-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb68-17"><a href="ridge-regression.html#cb68-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the data </span></span>
<span id="cb68-18"><a href="ridge-regression.html#cb68-18" aria-hidden="true" tabindex="-1"></a>  betahat <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(y<span class="sc">~</span>X<span class="dv">-1</span>))</span>
<span id="cb68-19"><a href="ridge-regression.html#cb68-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-86-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Over many simulation runs, the solution lies around the line of <span class="math inline">\(\beta_1 + \beta_2 = 2\)</span>.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="ridge-regression.html#cb69-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the truth</span></span>
<span id="cb69-2"><a href="ridge-regression.html#cb69-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb69-3"><a href="ridge-regression.html#cb69-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb69-4"><a href="ridge-regression.html#cb69-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb69-5"><a href="ridge-regression.html#cb69-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate many datasets in a simulation </span></span>
<span id="cb69-6"><a href="ridge-regression.html#cb69-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb69-7"><a href="ridge-regression.html#cb69-7" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb69-8"><a href="ridge-regression.html#cb69-8" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb69-9"><a href="ridge-regression.html#cb69-9" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb69-10"><a href="ridge-regression.html#cb69-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb69-11"><a href="ridge-regression.html#cb69-11" aria-hidden="true" tabindex="-1"></a>    betahat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb69-12"><a href="ridge-regression.html#cb69-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb69-13"><a href="ridge-regression.html#cb69-13" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-87-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="ridge-penalty-and-the-reduced-variation" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Ridge Penalty and the Reduced Variation<a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues on <span class="math inline">\(\mathbf{X}^\mathbf{X}\)</span>, making the eignvalues of <span class="math inline">\((\mathbf{X}^\mathbf{X})^{-1}\)</span> smaller. Here is a plot of the Ridge <span class="math inline">\(\ell_2\)</span> penalty.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-88-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is a balance of the bias-variance trade-off, which will be discussed in the following.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="ridge-regression.html#cb70-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb70-2"><a href="ridge-regression.html#cb70-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-3"><a href="ridge-regression.html#cb70-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adding a L2 penalty to the objective function</span></span>
<span id="cb70-4"><a href="ridge-regression.html#cb70-4" aria-hidden="true" tabindex="-1"></a>    rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> b <span class="sc">%*%</span> b, X, y),</span>
<span id="cb70-5"><a href="ridge-regression.html#cb70-5" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb70-6"><a href="ridge-regression.html#cb70-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb70-7"><a href="ridge-regression.html#cb70-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the ridge solution</span></span>
<span id="cb70-8"><a href="ridge-regression.html#cb70-8" aria-hidden="true" tabindex="-1"></a>    bh <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="fu">diag</span>(<span class="dv">2</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb70-9"><a href="ridge-regression.html#cb70-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb70-10"><a href="ridge-regression.html#cb70-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb70-11"><a href="ridge-regression.html#cb70-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb70-12"><a href="ridge-regression.html#cb70-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(bh[<span class="dv">1</span>], bh[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb70-13"><a href="ridge-regression.html#cb70-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">box</span>()</span>
<span id="cb70-14"><a href="ridge-regression.html#cb70-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb70-15"><a href="ridge-regression.html#cb70-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># adding a larger penalty</span></span>
<span id="cb70-16"><a href="ridge-regression.html#cb70-16" aria-hidden="true" tabindex="-1"></a>    rss <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">apply</span>(allbeta, <span class="dv">1</span>, <span class="cf">function</span>(b, X, y) <span class="fu">sum</span>((y <span class="sc">-</span> X <span class="sc">%*%</span> b)<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="dv">10</span><span class="sc">*</span>b <span class="sc">%*%</span> b, X, y),</span>
<span id="cb70-17"><a href="ridge-regression.html#cb70-17" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">length</span>(beta1), <span class="fu">length</span>(beta2))</span>
<span id="cb70-18"><a href="ridge-regression.html#cb70-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb70-19"><a href="ridge-regression.html#cb70-19" aria-hidden="true" tabindex="-1"></a>    bh <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb70-20"><a href="ridge-regression.html#cb70-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb70-21"><a href="ridge-regression.html#cb70-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the ridge solution</span></span>
<span id="cb70-22"><a href="ridge-regression.html#cb70-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">contour</span>(beta1, beta2, rss, <span class="at">levels =</span> <span class="fu">quantile</span>(rss, quanlvl))</span>
<span id="cb70-23"><a href="ridge-regression.html#cb70-23" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb70-24"><a href="ridge-regression.html#cb70-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(bh[<span class="dv">1</span>], bh[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb70-25"><a href="ridge-regression.html#cb70-25" aria-hidden="true" tabindex="-1"></a>    <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-89-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>We can check the ridge solution over many simulation runs</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="ridge-regression.html#cb71-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb71-2"><a href="ridge-regression.html#cb71-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-3"><a href="ridge-regression.html#cb71-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the truth</span></span>
<span id="cb71-4"><a href="ridge-regression.html#cb71-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb71-5"><a href="ridge-regression.html#cb71-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb71-6"><a href="ridge-regression.html#cb71-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb71-7"><a href="ridge-regression.html#cb71-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate many datasets in a simulation </span></span>
<span id="cb71-8"><a href="ridge-regression.html#cb71-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb71-9"><a href="ridge-regression.html#cb71-9" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb71-10"><a href="ridge-regression.html#cb71-10" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb71-11"><a href="ridge-regression.html#cb71-11" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb71-12"><a href="ridge-regression.html#cb71-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb71-13"><a href="ridge-regression.html#cb71-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># betahat &lt;- solve(t(X) %*% X + 2*diag(2)) %*% t(X) %*% y</span></span>
<span id="cb71-14"><a href="ridge-regression.html#cb71-14" aria-hidden="true" tabindex="-1"></a>    betahat <span class="ot">&lt;-</span> <span class="fu">lm.ridge</span>(y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="dv">2</span>)<span class="sc">$</span>coef</span>
<span id="cb71-15"><a href="ridge-regression.html#cb71-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb71-16"><a href="ridge-regression.html#cb71-16" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-90-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>This effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of <span class="math inline">\(\boldsymbol \beta\)</span> changes. We show this with two penalty values, and see how the estimated parameters are away from the truth.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="ridge-regression.html#cb72-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb72-2"><a href="ridge-regression.html#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="ridge-regression.html#cb72-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># small penalty</span></span>
<span id="cb72-4"><a href="ridge-regression.html#cb72-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb72-5"><a href="ridge-regression.html#cb72-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb72-6"><a href="ridge-regression.html#cb72-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb72-7"><a href="ridge-regression.html#cb72-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate many datasets in a simulation </span></span>
<span id="cb72-8"><a href="ridge-regression.html#cb72-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb72-9"><a href="ridge-regression.html#cb72-9" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb72-10"><a href="ridge-regression.html#cb72-10" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb72-11"><a href="ridge-regression.html#cb72-11" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb72-12"><a href="ridge-regression.html#cb72-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-13"><a href="ridge-regression.html#cb72-13" aria-hidden="true" tabindex="-1"></a>    betahat <span class="ot">&lt;-</span> <span class="fu">lm.ridge</span>(y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="dv">2</span>)<span class="sc">$</span>coef</span>
<span id="cb72-14"><a href="ridge-regression.html#cb72-14" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb72-15"><a href="ridge-regression.html#cb72-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb72-16"><a href="ridge-regression.html#cb72-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb72-17"><a href="ridge-regression.html#cb72-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># large penalty</span></span>
<span id="cb72-18"><a href="ridge-regression.html#cb72-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb72-19"><a href="ridge-regression.html#cb72-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>) </span>
<span id="cb72-20"><a href="ridge-regression.html#cb72-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb72-21"><a href="ridge-regression.html#cb72-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># generate many datasets in a simulation </span></span>
<span id="cb72-22"><a href="ridge-regression.html#cb72-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>)</span>
<span id="cb72-23"><a href="ridge-regression.html#cb72-23" aria-hidden="true" tabindex="-1"></a>  {</span>
<span id="cb72-24"><a href="ridge-regression.html#cb72-24" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">=</span> <span class="fu">mvrnorm</span>(n, <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">0.99</span>, <span class="fl">0.99</span>, <span class="dv">1</span>), <span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb72-25"><a href="ridge-regression.html#cb72-25" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> X[,<span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>])</span>
<span id="cb72-26"><a href="ridge-regression.html#cb72-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-27"><a href="ridge-regression.html#cb72-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># betahat &lt;- solve(t(X) %*% X + 30*diag(2)) %*% t(X) %*% y</span></span>
<span id="cb72-28"><a href="ridge-regression.html#cb72-28" aria-hidden="true" tabindex="-1"></a>    betahat <span class="ot">&lt;-</span> <span class="fu">lm.ridge</span>(y <span class="sc">~</span> X <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="dv">30</span>)<span class="sc">$</span>coef</span>
<span id="cb72-29"><a href="ridge-regression.html#cb72-29" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(betahat[<span class="dv">1</span>], betahat[<span class="dv">2</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span>
<span id="cb72-30"><a href="ridge-regression.html#cb72-30" aria-hidden="true" tabindex="-1"></a>  }</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-91-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="bias-and-variance-of-ridge-regression" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Bias and Variance of Ridge Regression<a href="ridge-regression.html#bias-and-variance-of-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can set a relationship between Ridge and OLS, assuming that the OLS estimator exist.</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}\\
=&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \color{OrangeRed}{(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}}\\
=&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \color{OrangeRed}{\widehat{\boldsymbol \beta}^\text{ols}}
\end{align}\]</span></p>
<p>This leads to a biased estimator (since the OLS estimator is unbiased) if we use any nonzero <span class="math inline">\(\lambda\)</span>.</p>
<ul>
<li>As <span class="math inline">\(\lambda \rightarrow 0\)</span>, the ridge solution is eventually the same as OLS</li>
<li>As <span class="math inline">\(\lambda \rightarrow \infty\)</span>, <span class="math inline">\(\widehat{\boldsymbol \beta}^\text{ridge} \rightarrow 0\)</span></li>
</ul>
<p>It can be easier to analyze a case with <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}= n \mathbf{I}\)</span>, i.e, with standardized and orthogonal columns in <span class="math inline">\(\mathbf{X}\)</span>. Note that in this case, each <span class="math inline">\(\beta_j^{\text{ols}}\)</span> is just the projection of <span class="math inline">\(\mathbf{y}\)</span> onto <span class="math inline">\(\mathbf{x}_j\)</span>, the <span class="math inline">\(j\)</span>th column of the design matrix. We also have</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}^\text{ridge} =&amp; (\mathbf{X}^\text{T}\mathbf{X}+ n\lambda \mathbf{I})^{-1} (\mathbf{X}^\text{T}\mathbf{X}) \widehat{\boldsymbol \beta}^\text{ols}\\
=&amp; (\mathbf{I}+ \lambda \mathbf{I})^{-1}\widehat{\boldsymbol \beta}^\text{ols}\\
=&amp; (1 + \lambda)^{-1} \widehat{\boldsymbol \beta}^\text{ols}\\

\Longrightarrow \beta_j^{\text{ridge}} =&amp; \frac{1}{1 + \lambda} \beta_j^\text{ols}
\end{align}\]</span></p>
<p>Then in this case, the bias and variance of the ridge estimator can be explicitly expressed:</p>
<ul>
<li><span class="math inline">\(\text{Bias}(\beta_j^{\text{ridge}}) = \frac{-\lambda}{1 + \lambda} \beta_j^\text{ols}\)</span> (not zero)</li>
<li><span class="math inline">\(\text{Var}(\beta_j^{\text{ridge}}) = \frac{1}{(1 + \lambda)^2} \text{Var}(\beta_j^\text{ols})\)</span> (reduced from OLS)</li>
</ul>
<p>Of course, we can ask the question: is it worth it? We could proceed with a simple analysis of the MSE of <span class="math inline">\(\beta\)</span> (dropping <span class="math inline">\(j\)</span>):</p>
<p><span class="math display">\[\begin{align}
\text{MSE}(\beta) &amp;= \text{E}(\widehat{\beta} - \beta)^2 \\
&amp;= \text{E}[\widehat{\beta} - \text{E}(\widehat{\beta})]^2 + \text{E}[\widehat{\beta} - \beta]^2 \\
&amp;= \text{E}[\widehat{\beta} - \text{E}(\widehat{\beta})]^2 + 0 + [\text{E}(\widehat{\beta}) - \beta]^2 \\
&amp;= \text{Var}(\widehat{\beta}) + \text{Bias}^2.
\end{align}\]</span></p>
<p>This bias-variance breakdown formula will appear multiple times. Now, plug-in the results developed earlier based on the orthogonal design matrix, and investigate the derivative of the MSE of the Ridge estimator, we have</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \text{MSE}(\widehat{\beta}^\text{ridge})}{ \partial \lambda} =&amp; \frac{\partial}{\partial \lambda} \left[ \frac{1}{(1+\lambda)^2} \text{Var}(\widehat{\beta}^\text{ols}) + \frac{\lambda^2}{(1 + \lambda)^2} \beta^2 \right] \\
=&amp; \frac{2}{(1+\lambda)^3} \left[ \lambda \beta^2 - \text{Var}(\widehat{\beta}^\text{ols}) \right]
\end{align}\]</span></p>
<p>Note that when the derivative is negative, increasing <span class="math inline">\(\lambda\)</span> would decrease the MSE. This implies that we can reduce the MSE by choosing a small <span class="math inline">\(\lambda\)</span>. Of course the situation is much more involving when the columns in <span class="math inline">\(\mathbf{X}\)</span> are not orthogonal. However, the following analysis helps to understand a non-orthogonal case. It is essentially re-organizing the columns of <span class="math inline">\(\mathbf{X}\)</span> into its principle components so that they are still orthogonal.</p>
<p>Let’s first take a singular value decomposition (SVD) of <span class="math inline">\(\mathbf{X}\)</span>, with <span class="math inline">\(\mathbf{X}= \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}\)</span>, then the columns in <span class="math inline">\(\mathbf{U}\)</span> form an orthonormal basis and columns in <span class="math inline">\(\mathbf{U}\mathbf{D}\)</span> are the <strong>principal components</strong> and <span class="math inline">\(\mathbf{V}\)</span> defines the principle directions. In addition, we have <span class="math inline">\(n \widehat{\boldsymbol \Sigma} = \mathbf{X}^\text{T}\mathbf{X}= \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}\)</span>. Assuming that <span class="math inline">\(p &lt; n\)</span>, and <span class="math inline">\(\mathbf{X}\)</span> has full column ranks, then the Ridge estimator fitted <span class="math inline">\(\mathbf{y}\)</span> value can be decomposed as</p>
<p><span class="math display">\[\begin{align}
\widehat{\mathbf{y}}^\text{ridge} =&amp; \mathbf{X}\widehat{\beta}^\text{ridge} \\ 
=&amp; \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n \lambda)^{-1} \mathbf{X}^\text{T}\mathbf{y}\\
=&amp; \mathbf{U}\mathbf{D}\mathbf{V}^\text{T}( \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}+ n \lambda \mathbf{V}\mathbf{V}^\text{T})^{-1} \mathbf{V}\mathbf{D}\mathbf{U}^\text{T}\mathbf{y}\\
=&amp; \mathbf{U}\mathbf{D}^2 (n \lambda + \mathbf{D}^2)^{-1} \mathbf{U}^\text{T}\mathbf{y}\\
=&amp; \sum_{j = 1}^p \mathbf{u}_j \left( \frac{d_j^2}{n \lambda + d_j^2} \mathbf{u}_j^\text{T}\mathbf{y}\right),
\end{align}\]</span></p>
<p>where <span class="math inline">\(d_j\)</span> is the <span class="math inline">\(j\)</span>th eigenvalue of the PCA. Hence, the Ridge regression fitted value can be understood as</p>
<ul>
<li>Perform PCA of <span class="math inline">\(\mathbf{X}\)</span></li>
<li>Project <span class="math inline">\(\mathbf{y}\)</span> onto the PCs</li>
<li>Shrink the projection <span class="math inline">\(\mathbf{u}_j^\text{T}\mathbf{y}\)</span> by the factor <span class="math inline">\(d_j^2 / (n \lambda + d_j^2)\)</span></li>
<li>Reassemble the PCs using all the shrunken length</li>
</ul>
<p>Hence, the bias-variance notion can be understood as the trade-off on these derived directions <span class="math inline">\(\mathbf{u}_j\)</span> and their corresponding parameters <span class="math inline">\(\mathbf{u}_j^\text{T}\mathbf{y}\)</span>.</p>
</div>
<div id="degrees-of-freedom" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Degrees of Freedom<a href="ridge-regression.html#degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We know that for a linear model, the degrees of freedom (DF) is simply the number of parameters used. There is a formal definition, using</p>
<p><span class="math display">\[\begin{align}
\text{DF}(\widehat{f}) =&amp; \frac{1}{\sigma^2} \text{Cov}(\widehat{y}_i, y_i)\\
=&amp; \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}, \mathbf{y})]
\end{align}\]</span></p>
<p>We can check that for a linear regression (assuming the intercept is already included in <span class="math inline">\(\mathbf{X}\)</span>), the DF is</p>
<p><span class="math display">\[\begin{align}
\frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}^\text{ols}, \mathbf{y})] =&amp; \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}, \mathbf{y})] \\
=&amp; \text{Trace}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}) \\
=&amp; \text{Trace}(\mathbf{I}_{p\times p})\\
=&amp; p
\end{align}\]</span></p>
<p>For the Ridge regression, we can perform the same analysis on ridge regression.</p>
<p><span class="math display">\[\begin{align}
\frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\widehat{\mathbf{y}}^\text{ridge}, \mathbf{y})] =&amp; \frac{1}{\sigma^2} \text{Trace}[\text{Cov}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ n \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\mathbf{y}, \mathbf{y})] \\
=&amp; \text{Trace}(\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}) \\
=&amp; \text{Trace}(\mathbf{U}\mathbf{D}\mathbf{V}^\text{T}( \mathbf{V}\mathbf{D}^2 \mathbf{V}^\text{T}+ n \lambda \mathbf{V}\mathbf{V}^\text{T})^{-1} \mathbf{V}\mathbf{D}\mathbf{U}^\text{T})\\
=&amp; \sum_{j = 1}^p \frac{d_j^2}{d_j^2 + n\lambda}
\end{align}\]</span></p>
<p>Note that this is smaller than <span class="math inline">\(p\)</span> as long as <span class="math inline">\(\lambda \neq 0\)</span>. This implies that the Ridge regression does not use the full potential of all <span class="math inline">\(p\)</span> variables, since there is a risk of over-fitting.</p>
</div>
<div id="using-the-lm.ridge-function" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Using the <code>lm.ridge()</code> function<a href="ridge-regression.html#using-the-lm.ridge-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have seen how the <code>lm.ridge()</code> can be used to fit a Ridge regression. However, keep in mind that the <code>lambda</code> parameter used in the function actually specifies the <span class="math inline">\(n \lambda\)</span> entirely we used in our notation. However, regardless, our goal is mainly to tune this parameter to achieve a good balance of bias-variance trade off. However, the difficulty here is to evaluate the performance without knowing the truth. Let’s first use a simulated example, in which we do know the truth and then introduce the cross-validation approach for real data where we do not know the truth.</p>
<p>We use the prostate cancer data <code>prostate</code> from the <code>ElemStatLearn</code> package. The dataset contains 8 explanatory variables and one outcome <code>lpsa</code>, the log prostate-specific antigen value.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="ridge-regression.html#cb73-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># ElemStatLearn is currently archived, install a previous version</span></span>
<span id="cb73-2"><a href="ridge-regression.html#cb73-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># library(devtools)</span></span>
<span id="cb73-3"><a href="ridge-regression.html#cb73-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># install_version(&quot;ElemStatLearn&quot;, version = &quot;2015.6.26&quot;, repos = &quot;http://cran.r-project.org&quot;)</span></span>
<span id="cb73-4"><a href="ridge-regression.html#cb73-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb73-5"><a href="ridge-regression.html#cb73-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(prostate)</span>
<span id="cb73-6"><a href="ridge-regression.html#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="do">##       lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa train</span></span>
<span id="cb73-7"><a href="ridge-regression.html#cb73-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829  TRUE</span></span>
<span id="cb73-8"><a href="ridge-regression.html#cb73-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189  TRUE</span></span>
<span id="cb73-9"><a href="ridge-regression.html#cb73-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189  TRUE</span></span>
<span id="cb73-10"><a href="ridge-regression.html#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189  TRUE</span></span>
<span id="cb73-11"><a href="ridge-regression.html#cb73-11" aria-hidden="true" tabindex="-1"></a><span class="do">## 5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636  TRUE</span></span>
<span id="cb73-12"><a href="ridge-regression.html#cb73-12" aria-hidden="true" tabindex="-1"></a><span class="do">## 6 -1.0498221 3.228826  50 -1.386294   0 -1.386294       6     0  0.7654678  TRUE</span></span></code></pre></div>
<div id="scaling-issue" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Scaling Issue<a href="ridge-regression.html#scaling-issue" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use <code>lm.ridge()</code> with a fixed <span class="math inline">\(\lambda\)</span> value, as we have shown in the previous example. Its syntax is again similar to the <code>lm()</code> function, with an additional argument <code>lambda</code>. We can also compare that with our own code.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="ridge-regression.html#cb74-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># lm.ridge function from the MASS package</span></span>
<span id="cb74-2"><a href="ridge-regression.html#cb74-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm.ridge</span>(lpsa <span class="sc">~</span>., <span class="at">data =</span> prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>], <span class="at">lambda =</span> <span class="dv">1</span>)</span>
<span id="cb74-3"><a href="ridge-regression.html#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                  lcavol     lweight         age        lbph         svi         lcp     gleason </span></span>
<span id="cb74-4"><a href="ridge-regression.html#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  0.14716982  0.55209405  0.61998311 -0.02049376  0.09488234  0.74846397 -0.09399009  0.05227074 </span></span>
<span id="cb74-5"><a href="ridge-regression.html#cb74-5" aria-hidden="true" tabindex="-1"></a><span class="do">##       pgg45 </span></span>
<span id="cb74-6"><a href="ridge-regression.html#cb74-6" aria-hidden="true" tabindex="-1"></a><span class="do">##  0.00424397</span></span>
<span id="cb74-7"><a href="ridge-regression.html#cb74-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-8"><a href="ridge-regression.html#cb74-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># using our own code</span></span>
<span id="cb74-9"><a href="ridge-regression.html#cb74-9" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">cbind</span>(<span class="dv">1</span>, <span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]))</span>
<span id="cb74-10"><a href="ridge-regression.html#cb74-10" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> prostate[, <span class="dv">9</span>]</span>
<span id="cb74-11"><a href="ridge-regression.html#cb74-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="fu">diag</span>(<span class="dv">9</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb74-12"><a href="ridge-regression.html#cb74-12" aria-hidden="true" tabindex="-1"></a><span class="do">##                [,1]</span></span>
<span id="cb74-13"><a href="ridge-regression.html#cb74-13" aria-hidden="true" tabindex="-1"></a><span class="do">##          0.07941225</span></span>
<span id="cb74-14"><a href="ridge-regression.html#cb74-14" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol   0.55985143</span></span>
<span id="cb74-15"><a href="ridge-regression.html#cb74-15" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight  0.60398302</span></span>
<span id="cb74-16"><a href="ridge-regression.html#cb74-16" aria-hidden="true" tabindex="-1"></a><span class="do">## age     -0.01957258</span></span>
<span id="cb74-17"><a href="ridge-regression.html#cb74-17" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph     0.09395770</span></span>
<span id="cb74-18"><a href="ridge-regression.html#cb74-18" aria-hidden="true" tabindex="-1"></a><span class="do">## svi      0.68809341</span></span>
<span id="cb74-19"><a href="ridge-regression.html#cb74-19" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp     -0.08863685</span></span>
<span id="cb74-20"><a href="ridge-regression.html#cb74-20" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason  0.06288206</span></span>
<span id="cb74-21"><a href="ridge-regression.html#cb74-21" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45    0.00416878</span></span></code></pre></div>
<p>However, they look different. This is because ridge regression has a scaling issue: it would shrink parameters differently if the corresponding covariates have different scales. This can be seen from our previous development of the SVD analysis. Since the shrinkage is the same for all <span class="math inline">\(d_j\)</span>s, it would apply a larger shrinkage for small <span class="math inline">\(d_j\)</span>. A commonly used approach to deal with the scaling issue is to <strong>standardize all covariates</strong> such that they are treated the same way. In addition, we will also <strong>center both <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span></strong> before performing the ridge regression. An interesting consequence of centering is that we do not need the intercept anymore, since <span class="math inline">\(\mathbf{X}\boldsymbol \beta= \mathbf{0}\)</span> for all <span class="math inline">\(\boldsymbol \beta\)</span>. One last point is that when performing scaling, <code>lm.ridge()</code> use the <span class="math inline">\(n\)</span> factor instead of <span class="math inline">\(n-1\)</span> when calculating the standard deviation. Hence, incorporating all these, we have</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="ridge-regression.html#cb75-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># perform centering and scaling</span></span>
<span id="cb75-2"><a href="ridge-regression.html#cb75-2" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">scale</span>(<span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]), <span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb75-3"><a href="ridge-regression.html#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="ridge-regression.html#cb75-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># use n instead of (n-1) for standardization</span></span>
<span id="cb75-5"><a href="ridge-regression.html#cb75-5" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">nrow</span>(X)</span>
<span id="cb75-6"><a href="ridge-regression.html#cb75-6" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">=</span> X <span class="sc">*</span> <span class="fu">sqrt</span>(n <span class="sc">/</span> (n<span class="dv">-1</span>))</span>
<span id="cb75-7"><a href="ridge-regression.html#cb75-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb75-8"><a href="ridge-regression.html#cb75-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># center y but not scaling</span></span>
<span id="cb75-9"><a href="ridge-regression.html#cb75-9" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">scale</span>(prostate[, <span class="dv">9</span>], <span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb75-10"><a href="ridge-regression.html#cb75-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb75-11"><a href="ridge-regression.html#cb75-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># getting the estimated parameter</span></span>
<span id="cb75-12"><a href="ridge-regression.html#cb75-12" aria-hidden="true" tabindex="-1"></a>  mybeta <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X <span class="sc">+</span> <span class="fu">diag</span>(<span class="dv">8</span>)) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb75-13"><a href="ridge-regression.html#cb75-13" aria-hidden="true" tabindex="-1"></a>  ridge.fit <span class="ot">=</span> <span class="fu">lm.ridge</span>(lpsa <span class="sc">~</span>., <span class="at">data =</span> prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>], <span class="at">lambda =</span> <span class="dv">1</span>)</span>
<span id="cb75-14"><a href="ridge-regression.html#cb75-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb75-15"><a href="ridge-regression.html#cb75-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note that $coef obtains the coefficients internally from lm.ridge</span></span>
<span id="cb75-16"><a href="ridge-regression.html#cb75-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># however coef() would transform these back to the original scale version</span></span>
<span id="cb75-17"><a href="ridge-regression.html#cb75-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(mybeta, ridge.fit<span class="sc">$</span>coef)</span>
<span id="cb75-18"><a href="ridge-regression.html#cb75-18" aria-hidden="true" tabindex="-1"></a><span class="do">##                [,1]        [,2]</span></span>
<span id="cb75-19"><a href="ridge-regression.html#cb75-19" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol   0.64734891  0.64734891</span></span>
<span id="cb75-20"><a href="ridge-regression.html#cb75-20" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight  0.26423507  0.26423507</span></span>
<span id="cb75-21"><a href="ridge-regression.html#cb75-21" aria-hidden="true" tabindex="-1"></a><span class="do">## age     -0.15178989 -0.15178989</span></span>
<span id="cb75-22"><a href="ridge-regression.html#cb75-22" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph     0.13694453  0.13694453</span></span>
<span id="cb75-23"><a href="ridge-regression.html#cb75-23" aria-hidden="true" tabindex="-1"></a><span class="do">## svi      0.30825889  0.30825889</span></span>
<span id="cb75-24"><a href="ridge-regression.html#cb75-24" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp     -0.13074243 -0.13074243</span></span>
<span id="cb75-25"><a href="ridge-regression.html#cb75-25" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason  0.03755141  0.03755141</span></span>
<span id="cb75-26"><a href="ridge-regression.html#cb75-26" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45    0.11907848  0.11907848</span></span></code></pre></div>
</div>
<div id="multiple-lambda-values" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Multiple <span class="math inline">\(\lambda\)</span> values<a href="ridge-regression.html#multiple-lambda-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since we now face the problem of bias-variance trade-off, we can fit the model with multiple <span class="math inline">\(\lambda\)</span> values and select the best. This can be done using the following code.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="ridge-regression.html#cb76-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb76-2"><a href="ridge-regression.html#cb76-2" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">=</span> <span class="fu">lm.ridge</span>(lpsa<span class="sc">~</span>.,  <span class="at">data =</span> prostate[, <span class="sc">-</span><span class="dv">10</span>], <span class="at">lambda =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="at">by=</span><span class="fl">0.2</span>))</span></code></pre></div>
<p>For each <span class="math inline">\(\lambda\)</span>, the coefficients of all variables are recorded. The plot shows how these coefficients change as a function of <span class="math inline">\(\lambda\)</span>. We can easily see that as <span class="math inline">\(\lambda\)</span> becomes larger, the coefficients are shrunken towards 0. This is consistent with our understanding of the bias. On the very left hand size of the plot, the value of each parameter corresponds to the OLS result since no penalty is applied. Be careful that the coefficients of the fitted objects <code>fit$coef</code> are scaled by the standard deviation of the covariates. If you need the original scale, make sure to use <code>coef(fit)</code>.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="ridge-regression.html#cb77-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">matplot</span>(<span class="fu">coef</span>(fit)[, <span class="sc">-</span><span class="dv">1</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Lambda&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficients&quot;</span>)</span>
<span id="cb77-2"><a href="ridge-regression.html#cb77-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">text</span>(<span class="fu">rep</span>(<span class="dv">50</span>, <span class="dv">8</span>), <span class="fu">coef</span>(fit)[<span class="dv">1</span>,<span class="sc">-</span><span class="dv">1</span>], <span class="fu">colnames</span>(prostate)[<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>])</span>
<span id="cb77-3"><a href="ridge-regression.html#cb77-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Prostate Cancer Data: Ridge Coefficients&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-97-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>To select the best <span class="math inline">\(\lambda\)</span> value, there can be several different methods. We will discuss two approaches among them: <span class="math inline">\(k\)</span>-fold cross-validation and generalized cross-validation (GCV).</p>
</div>
</div>
<div id="cross-validation" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Cross-validation<a href="ridge-regression.html#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Cross-validation (CV) is a technique to evaluate the performance of a model on an independent set of data. The essential idea is to separate out a subset of the data and do not use that part during the training, while using it for testing. We can then rotate to or sample a different subset as the testing data. Different cross-validation methods differs on the mechanisms of generating such testing data. <strong><span class="math inline">\(K\)</span>-fold cross-validation</strong> is probably the the most popular among them. The method works in the following steps:</p>
<ol style="list-style-type: decimal">
<li>Randomly split the data into <span class="math inline">\(K\)</span> equal portions</li>
<li>For each <span class="math inline">\(k\)</span> in <span class="math inline">\(1, \ldots, K\)</span>: use the <span class="math inline">\(k\)</span>th portion as the testing data and the rest as training data, obtain the testing error</li>
<li>Average all <span class="math inline">\(K\)</span> testing errors</li>
</ol>
<p>Here is a graphical demonstration of a <span class="math inline">\(10\)</span>-fold CV:</p>
<center>
<img src="images/kfoldcv.png" style="width:80.0%" />
</center>
<p>There are also many other cross-validation procedures, for example, the <strong>Monte Carlo cross-validation</strong> randomly splits the data into training and testing (instead of fix <span class="math inline">\(K\)</span> portions) each time and repeat the process as many times as we like. The benefit of such procedure is that if this is repeated enough times, the estimated testing error becomes fairly stable, and not affected much by the random mechanism. On the other hand, we can also repeat the entire <span class="math inline">\(K\)</span>-fold CV process many times, then average the errors. This is also trying to reduced the influence of randomness.</p>
</div>
<div id="leave-one-out-cross-validation" class="section level2 hasAnchor" number="6.7">
<h2><span class="header-section-number">6.7</span> Leave-one-out cross-validation<a href="ridge-regression.html#leave-one-out-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Regarding the randomness, the leave-one-out cross-validation is completely nonrandom. It is essentially the <span class="math inline">\(k\)</span>-fold CV approach, but with <span class="math inline">\(k\)</span> equal to <span class="math inline">\(n\)</span>, the sample size. A standard approach would require to re-fit the model <span class="math inline">\(n\)</span> times, however, some linear algebra can show that there is an equivalent form using the “Hat” matrix when fitting a linear regression:</p>
<p><span class="math display">\[\begin{align}
\text{CV}(n) =&amp; \frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_{[i]})^2\\
=&amp; \frac{1}{n} \sum_{i=1}^n \left( \frac{y_i - \widehat{y}_i}{1 - \mathbf{H}_{ii}} \right)^2,
\end{align}\]</span>
where <span class="math inline">\(\widehat{y}_{i}\)</span> is the fitted value using the whole dataset, but <span class="math inline">\(\widehat{y}_{[i]}\)</span> is the prediction of <span class="math inline">\(i\)</span>th observation using the data without it when fitting the model. And <span class="math inline">\(\mathbf{H}_{ii}\)</span> is the <span class="math inline">\(i\)</span>th diagonal element of the hat matrix <span class="math inline">\(\mathbf{H}= \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}\)</span>. The proof is essentially an application of the <a href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula">Sherman–Morrison–Woodbury (SMW)</a> formula, which is also used when deriving the rank-one update of a quasi-Newton optimization approach.</p>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>Denote <span class="math inline">\(\mathbf{X}_{[i]}\)</span> and <span class="math inline">\(\mathbf{y}_{[i]}\)</span> the data derived from <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>, but with the <span class="math inline">\(i\)</span> observation (<span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>) removed. We then have the properties that</p>
<p><span class="math display">\[\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]} = \mathbf{X}^\text{T}\mathbf{X}- x_i x_i^\text{T}, \]</span></p>
<p>and</p>
<p><span class="math display">\[\mathbf{H}_{ii} = x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i.\]</span></p>
<p>By the SMW formula, we have</p>
<p><span class="math display">\[(\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]})^{-1} = (\mathbf{X}^\text{T}\mathbf{X})^{-1} + \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}}, \]</span></p>
<p>Further notice that</p>
<p><span class="math display">\[\mathbf{X}_{[i]}^\text{T}\mathbf{y}_{[i]} = \mathbf{X}^\text{T}\mathbf{y}- x_i y_i, \]</span></p>
<p>we can then reconstruct the fitted parameter when observation <span class="math inline">\(i\)</span> is removed:</p>
<p><span class="math display">\[\begin{align}
\widehat{\boldsymbol \beta}_{[i]} =&amp; (\mathbf{X}_{[i]}^\text{T}\mathbf{X}_{[i]})^{-1} \mathbf{X}_{[i]}^\text{T}\mathbf{y}_{[i]} \\
=&amp; \left[ (\mathbf{X}^\text{T}\mathbf{X})^{-1} + \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}} \right] (\mathbf{X}^\text{T}\mathbf{y}- x_i y_i)\\
=&amp; (\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{X}^\text{T}\mathbf{y}+ \left[ - (\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i y_i +  \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1}x_i x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1}}{ 1 - \mathbf{H}_{ii}} (\mathbf{X}^\text{T}\mathbf{y}- x_i y_i) \right] \\
=&amp; \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left[ y_i (1 - \mathbf{H}_{ii}) - x_i^\text{T}\widehat{\boldsymbol \beta} + \mathbf{H}_{ii} y_i \right]\\
=&amp; \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)
\end{align}\]</span></p>
<p>Then the error of the <span class="math inline">\(i\)</span>th obervation from the leave-one-out model is</p>
<p><span class="math display">\[\begin{align}
y _i - \widehat{y}_{[i]} =&amp; y _i - x_i^\text{T}\widehat{\boldsymbol \beta}_{[i]} \\
=&amp; y _i - x_i^\text{T}\left[ \widehat{\boldsymbol \beta} - \frac{(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)  \right]\\
=&amp; y _i - x_i^\text{T}\widehat{\boldsymbol \beta} + \frac{x_i^\text{T}(\mathbf{X}^\text{T}\mathbf{X})^{-1} x_i}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)\\
=&amp; y _i - x_i^\text{T}\widehat{\boldsymbol \beta} + \frac{\mathbf{H}_{ii}}{1 - \mathbf{H}_{ii}} \left( y_i - x_i^\text{T}\widehat{\boldsymbol \beta} \right)\\
=&amp; \frac{y_i - x_i^\text{T}\widehat{\boldsymbol \beta}}{1 - \mathbf{H}_{ii}}
\end{align}\]</span></p>
<p>This completes the proof.</p>
</div>
<div id="generalized-cross-validation" class="section level3 hasAnchor" number="6.7.1">
<h3><span class="header-section-number">6.7.1</span> Generalized cross-validation<a href="ridge-regression.html#generalized-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The generalized cross-validation (GCV, <span class="citation"><a href="#ref-golub1979generalized" role="doc-biblioref">Golub, Heath, and Wahba</a> (<a href="#ref-golub1979generalized" role="doc-biblioref">1979</a>)</span>) is a modified version of the leave-one-out CV:</p>
<p><span class="math display">\[\text{GCV}(\lambda) = \frac{\sum_{i=1}^n (y_i - x_i^\text{T}\widehat{\boldsymbol \beta}^\text{ridge}_\lambda)}{(n - \text{Trace}(\mathbf{S}_\lambda))}\]</span>
where <span class="math inline">\(\mathbf{S}_\lambda\)</span> is the hat matrix corresponding to the ridge regression:</p>
<p><span class="math display">\[\mathbf{S}_\lambda = \mathbf{X}(\mathbf{X}^\text{T}\mathbf{X}+ \lambda \mathbf{I})^{-1} \mathbf{X}^\text{T}\]</span></p>
<p>The following plot shows how GCV value changes as a function of <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="ridge-regression.html#cb78-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># use GCV to select the best lambda</span></span>
<span id="cb78-2"><a href="ridge-regression.html#cb78-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(fit<span class="sc">$</span>lambda[<span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>], fit<span class="sc">$</span>GCV[<span class="dv">1</span><span class="sc">:</span><span class="dv">500</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, </span>
<span id="cb78-3"><a href="ridge-regression.html#cb78-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">ylab =</span> <span class="st">&quot;GCV&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Lambda&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb78-4"><a href="ridge-regression.html#cb78-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">title</span>(<span class="st">&quot;Prostate Cancer Data: GCV&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-99-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can select the best <span class="math inline">\(\lambda\)</span> that produces the smallest GCV.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="ridge-regression.html#cb79-1" aria-hidden="true" tabindex="-1"></a>    fit<span class="sc">$</span>lambda[<span class="fu">which.min</span>(fit<span class="sc">$</span>GCV)]</span>
<span id="cb79-2"><a href="ridge-regression.html#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 6.8</span></span>
<span id="cb79-3"><a href="ridge-regression.html#cb79-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">coef</span>(fit)[<span class="fu">which.min</span>(fit<span class="sc">$</span>GCV), ], <span class="dv">4</span>)</span>
<span id="cb79-4"><a href="ridge-regression.html#cb79-4" aria-hidden="true" tabindex="-1"></a><span class="do">##          lcavol lweight     age    lbph     svi     lcp gleason   pgg45 </span></span>
<span id="cb79-5"><a href="ridge-regression.html#cb79-5" aria-hidden="true" tabindex="-1"></a><span class="do">##  0.0170  0.4949  0.6050 -0.0169  0.0863  0.6885 -0.0420  0.0634  0.0034</span></span></code></pre></div>
</div>
</div>
<div id="the-glmnet-package" class="section level2 hasAnchor" number="6.8">
<h2><span class="header-section-number">6.8</span> The <code>glmnet</code> package<a href="ridge-regression.html#the-glmnet-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <code>glmnet</code> package implements the <span class="math inline">\(k\)</span>-fold cross-validation. To perform a ridge regression with cross-validation, we need to use the <code>cv.glmnet()</code> function with <span class="math inline">\(alpha = 0\)</span>. Here, the <span class="math inline">\(\alpha\)</span> is a parameter that controls the <span class="math inline">\(\ell_2\)</span> and <span class="math inline">\(\ell_1\)</span> (Lasso) penalties. In addition, the lambda values are also automatically selected, on the log-scale.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="ridge-regression.html#cb80-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(glmnet)</span>
<span id="cb80-2"><a href="ridge-regression.html#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Loading required package: Matrix</span></span>
<span id="cb80-3"><a href="ridge-regression.html#cb80-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Loaded glmnet 4.1-3</span></span>
<span id="cb80-4"><a href="ridge-regression.html#cb80-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb80-5"><a href="ridge-regression.html#cb80-5" aria-hidden="true" tabindex="-1"></a>  fit2 <span class="ot">=</span> <span class="fu">cv.glmnet</span>(<span class="fu">data.matrix</span>(prostate[, <span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>]), prostate<span class="sc">$</span>lpsa, <span class="at">nfolds =</span> <span class="dv">10</span>, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb80-6"><a href="ridge-regression.html#cb80-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(fit2<span class="sc">$</span>glmnet.fit, <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-101-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>It is useful to plot the cross-validation error against the <span class="math inline">\(\lambda\)</span> values , then select the corresponding <span class="math inline">\(\lambda\)</span> with the smallest error. The corresponding coefficient values can be obtained using the <code>s = "lambda.min"</code> option in the <code>coef()</code> function. However, this can still be subject to over-fitting, and sometimes practitioners use <code>s = "lambda.1se"</code> to select a slightly heavier penalized version based on the variations observed from different folds.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="ridge-regression.html#cb81-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(fit2)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-102-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="ridge-regression.html#cb82-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(fit2, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</span>
<span id="cb82-2"><a href="ridge-regression.html#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span></span>
<span id="cb82-3"><a href="ridge-regression.html#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="do">##                       s1</span></span>
<span id="cb82-4"><a href="ridge-regression.html#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  0.011566731</span></span>
<span id="cb82-5"><a href="ridge-regression.html#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol       0.492211875</span></span>
<span id="cb82-6"><a href="ridge-regression.html#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight      0.604155671</span></span>
<span id="cb82-7"><a href="ridge-regression.html#cb82-7" aria-hidden="true" tabindex="-1"></a><span class="do">## age         -0.016727236</span></span>
<span id="cb82-8"><a href="ridge-regression.html#cb82-8" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph         0.085820464</span></span>
<span id="cb82-9"><a href="ridge-regression.html#cb82-9" aria-hidden="true" tabindex="-1"></a><span class="do">## svi          0.685477646</span></span>
<span id="cb82-10"><a href="ridge-regression.html#cb82-10" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp         -0.039717080</span></span>
<span id="cb82-11"><a href="ridge-regression.html#cb82-11" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason      0.063806235</span></span>
<span id="cb82-12"><a href="ridge-regression.html#cb82-12" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45        0.003411982</span></span>
<span id="cb82-13"><a href="ridge-regression.html#cb82-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coef</span>(fit2, <span class="at">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</span>
<span id="cb82-14"><a href="ridge-regression.html#cb82-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span></span>
<span id="cb82-15"><a href="ridge-regression.html#cb82-15" aria-hidden="true" tabindex="-1"></a><span class="do">##                       s1</span></span>
<span id="cb82-16"><a href="ridge-regression.html#cb82-16" aria-hidden="true" tabindex="-1"></a><span class="do">## (Intercept)  0.035381749</span></span>
<span id="cb82-17"><a href="ridge-regression.html#cb82-17" aria-hidden="true" tabindex="-1"></a><span class="do">## lcavol       0.264613825</span></span>
<span id="cb82-18"><a href="ridge-regression.html#cb82-18" aria-hidden="true" tabindex="-1"></a><span class="do">## lweight      0.421408730</span></span>
<span id="cb82-19"><a href="ridge-regression.html#cb82-19" aria-hidden="true" tabindex="-1"></a><span class="do">## age         -0.002555681</span></span>
<span id="cb82-20"><a href="ridge-regression.html#cb82-20" aria-hidden="true" tabindex="-1"></a><span class="do">## lbph         0.049916919</span></span>
<span id="cb82-21"><a href="ridge-regression.html#cb82-21" aria-hidden="true" tabindex="-1"></a><span class="do">## svi          0.452500472</span></span>
<span id="cb82-22"><a href="ridge-regression.html#cb82-22" aria-hidden="true" tabindex="-1"></a><span class="do">## lcp          0.075346975</span></span>
<span id="cb82-23"><a href="ridge-regression.html#cb82-23" aria-hidden="true" tabindex="-1"></a><span class="do">## gleason      0.083894617</span></span>
<span id="cb82-24"><a href="ridge-regression.html#cb82-24" aria-hidden="true" tabindex="-1"></a><span class="do">## pgg45        0.002615235</span></span></code></pre></div>
<div id="scaling-issue-1" class="section level3 hasAnchor" number="6.8.1">
<h3><span class="header-section-number">6.8.1</span> Scaling Issue<a href="ridge-regression.html#scaling-issue-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>glmnet</code> package would using the same strategies for scaling: center and standardize <span class="math inline">\(\mathbf{X}\)</span> and center <span class="math inline">\(\mathbf{y}\)</span>. A slight difference is that it considers using <span class="math inline">\(1/(2n)\)</span> as the normalizing factor of the residual sum of squares, but also uses <span class="math inline">\(\lambda/2 \lVert \boldsymbol \beta\rVert_2^2\)</span> as the penalty. This does not affect our formulation since the <span class="math inline">\(1/2\)</span> cancels out. However, it would slightly affect the Lasso formulation introduced in the next Chapter since the <span class="math inline">\(\ell_1\)</span> penalty does not apply this <span class="math inline">\(1/2\)</span> factor. Nonetheless, we can check the (nearly) equivalence between <code>lm.ridge</code> and <code>glmnet()</code>:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="ridge-regression.html#cb83-1" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb83-2"><a href="ridge-regression.html#cb83-2" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb83-3"><a href="ridge-regression.html#cb83-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-4"><a href="ridge-regression.html#cb83-4" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">scale</span>(<span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)))</span>
<span id="cb83-5"><a href="ridge-regression.html#cb83-5" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">scale</span>(X[, <span class="dv">1</span>] <span class="sc">+</span> X[,<span class="dv">2</span>]<span class="sc">*</span><span class="fl">0.5</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)))</span>
<span id="cb83-6"><a href="ridge-regression.html#cb83-6" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb83-7"><a href="ridge-regression.html#cb83-7" aria-hidden="true" tabindex="-1"></a>  lam <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">3</span>, <span class="fl">0.1</span>)</span>
<span id="cb83-8"><a href="ridge-regression.html#cb83-8" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb83-9"><a href="ridge-regression.html#cb83-9" aria-hidden="true" tabindex="-1"></a>  fit1 <span class="ot">&lt;-</span> <span class="fu">lm.ridge</span>(y <span class="sc">~</span> X, <span class="at">lambda =</span> lam)</span>
<span id="cb83-10"><a href="ridge-regression.html#cb83-10" aria-hidden="true" tabindex="-1"></a>  fit2 <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> lam <span class="sc">/</span> <span class="fu">nrow</span>(X))</span>
<span id="cb83-11"><a href="ridge-regression.html#cb83-11" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb83-12"><a href="ridge-regression.html#cb83-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the estimated parameters</span></span>
<span id="cb83-13"><a href="ridge-regression.html#cb83-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb83-14"><a href="ridge-regression.html#cb83-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matplot</span>(<span class="fu">apply</span>(<span class="fu">coef</span>(fit1), <span class="dv">2</span>, rev), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;lm.ridge&quot;</span>)</span>
<span id="cb83-15"><a href="ridge-regression.html#cb83-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matplot</span>(<span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">coef</span>(fit2))), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;glmnet&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-103-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="ridge-regression.html#cb84-1" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb84-2"><a href="ridge-regression.html#cb84-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check differences</span></span>
<span id="cb84-3"><a href="ridge-regression.html#cb84-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">max</span>(<span class="fu">abs</span>(<span class="fu">apply</span>(<span class="fu">coef</span>(fit1), <span class="dv">2</span>, rev) <span class="sc">-</span> <span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">coef</span>(fit2)))))</span>
<span id="cb84-4"><a href="ridge-regression.html#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.0009968625</span></span></code></pre></div>

</div>
</div>
</div>
<h3> Reference<a href="reference.html#reference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-golub1979generalized" class="csl-entry">
Golub, Gene H, Michael Heath, and Grace Wahba. 1979. <span>“Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter.”</span> <em>Technometrics</em> 21 (2): 215–23.
</div>
<div id="ref-hoerl1970ridge" class="csl-entry">
Hoerl, Arthur E, and Robert W Kennard. 1970. <span>“Ridge Regression: Biased Estimation for Nonorthogonal Problems.”</span> <em>Technometrics</em> 12 (1): 55–67.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression-and-model-selection.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lasso.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "serif",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
