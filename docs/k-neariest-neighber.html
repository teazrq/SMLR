<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 K-Neariest Neighber | Statistical Machine Learning with R</title>
  <meta name="description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 K-Neariest Neighber | Statistical Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 K-Neariest Neighber | Statistical Machine Learning with R" />
  
  <meta name="twitter:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2025-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="discriminant-analysis.html"/>
<link rel="next" href="kernel-smoothing.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visual-studio-code.html"><a href="visual-studio-code.html"><i class="fa fa-check"></i><b>3</b> Visual Studio Code</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visual-studio-code.html"><a href="visual-studio-code.html#basics-and-resources-1"><i class="fa fa-check"></i><b>3.1</b> Basics and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linear-regression"><i class="fa fa-check"></i><b>4.2</b> Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#matrix-inversion"><i class="fa fa-check"></i><b>4.3</b> Matrix Inversion</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linearalgebra-SM"><i class="fa fa-check"></i><b>4.3.1</b> Rank-one Update</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#rank-k-update"><i class="fa fa-check"></i><b>4.3.2</b> Rank-<span class="math inline">\(k\)</span> Update</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#times-2-block-matrix-inversion"><i class="fa fa-check"></i><b>4.3.3</b> 2 <span class="math inline">\(\times\)</span> 2 Block Matrix Inversion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>5</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>5.1</b> Basic Concept</a></li>
<li class="chapter" data-level="5.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>5.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="5.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>5.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="5.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>5.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="5.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>5.5</b> Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>5.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>5.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="5.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>5.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>5.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>5.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>5.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>5.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>6</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>6.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>6.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>6.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>6.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>6.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>6.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>6.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>6.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>6.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>6.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>6.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>6.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>7</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>7.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="7.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>7.2</b> Ridge Penalty and the Reduced Variation</a></li>
<li class="chapter" data-level="7.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>7.3</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="7.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>7.4</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="7.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>7.5</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>7.5.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="7.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>7.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>7.6</b> Cross-validation</a></li>
<li class="chapter" data-level="7.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.7</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>7.7.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>7.8</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>7.8.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8</b> Lasso</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>8.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>8.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="8.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>8.3</b> The Solution Path</a></li>
<li class="chapter" data-level="8.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>8.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="8.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>8.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="8.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>8.6</b> Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>9</b> Spline</a>
<ul>
<li class="chapter" data-level="9.1" data-path="spline.html"><a href="spline.html#using-linear-models-for-nonlinear-trends"><i class="fa fa-check"></i><b>9.1</b> Using Linear models for Nonlinear Trends</a></li>
<li class="chapter" data-level="9.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>9.2</b> A Motivating Example and Polynomials</a></li>
<li class="chapter" data-level="9.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>9.3</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="9.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>9.4</b> Splines</a></li>
<li class="chapter" data-level="9.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>9.5</b> Spline Basis</a></li>
<li class="chapter" data-level="9.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>9.6</b> Natural Cubic Spline</a></li>
<li class="chapter" data-level="9.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>9.7</b> Smoothing Spline</a></li>
<li class="chapter" data-level="9.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>9.8</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="9.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>9.9</b> Extending Splines to Multiple Varibles</a></li>
</ul></li>
<li class="part"><span><b>III Linear Classification Models</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>10.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>10.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>10.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>10.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>11</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>11.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="11.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>11.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="11.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>11.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="11.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>11.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="11.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>11.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>11.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Models</b></span></li>
<li class="chapter" data-level="12" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>12</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>12.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="12.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>12.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="12.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>12.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="12.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>12.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="12.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#degrees-of-freedom-1"><i class="fa fa-check"></i><b>12.6</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="12.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>12.7</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="12.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>12.8</b> Distance Measures</a></li>
<li class="chapter" data-level="12.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>12.9</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="12.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>12.10</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="12.11" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>12.11</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>13</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>13.1</b> KNN vs. Kernel</a></li>
<li class="chapter" data-level="13.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>13.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="13.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>13.3</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="13.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>13.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off-1"><i class="fa fa-check"></i><b>13.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>13.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="13.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>13.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="13.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>13.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="13.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>13.8</b> R Implementations</a></li>
</ul></li>
<li class="part"><span><b>V Kernel Machines</b></span></li>
<li class="chapter" data-level="14" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>14</b> Reproducing Kernel Hilbert Space</a>
<ul>
<li class="chapter" data-level="14.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-motivation"><i class="fa fa-check"></i><b>14.1</b> The Motivation</a></li>
<li class="chapter" data-level="14.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#hilbert-space-preliminaries"><i class="fa fa-check"></i><b>14.2</b> Hilbert Space Preliminaries</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-space-of-square-integrable-functions"><i class="fa fa-check"></i><b>14.2.1</b> The Space of Square-Integrable Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-kernel-function"><i class="fa fa-check"></i><b>14.3</b> A Kernel Function</a></li>
<li class="chapter" data-level="14.4" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-space-of-functions"><i class="fa fa-check"></i><b>14.4</b> A Space of Functions</a></li>
<li class="chapter" data-level="14.5" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-inner-product"><i class="fa fa-check"></i><b>14.5</b> The Inner Product</a></li>
<li class="chapter" data-level="14.6" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-rkhs"><i class="fa fa-check"></i><b>14.6</b> The RKHS</a></li>
<li class="chapter" data-level="14.7" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-reproducing-property"><i class="fa fa-check"></i><b>14.7</b> The Reproducing Property</a></li>
<li class="chapter" data-level="14.8" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#smoothness"><i class="fa fa-check"></i><b>14.8</b> Smoothness</a></li>
<li class="chapter" data-level="14.9" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-moorearonszajn-theorem"><i class="fa fa-check"></i><b>14.9</b> The Moore–Aronszajn Theorem</a></li>
<li class="chapter" data-level="14.10" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#examples"><i class="fa fa-check"></i><b>14.10</b> Examples</a>
<ul>
<li class="chapter" data-level="14.10.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#brownian-motion-kernel"><i class="fa fa-check"></i><b>14.10.1</b> Brownian Motion Kernel</a></li>
<li class="chapter" data-level="14.10.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#non-positive-definite-kernel"><i class="fa fa-check"></i><b>14.10.2</b> Non-positive Definite Kernel</a></li>
<li class="chapter" data-level="14.10.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#defining-new-kernels"><i class="fa fa-check"></i><b>14.10.3</b> Defining New Kernels</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>15</b> Kernel Ridge Regression</a>
<ul>
<li class="chapter" data-level="15.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#linear-regression-as-a-constraint-optimization"><i class="fa fa-check"></i><b>15.1</b> Linear Regression as a Constraint Optimization</a></li>
<li class="chapter" data-level="15.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#the-kernel-ridge-regression"><i class="fa fa-check"></i><b>15.2</b> The Kernel Ridge Regression</a></li>
<li class="chapter" data-level="15.3" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#ridge-regression-as-a-linear-kernel-model"><i class="fa fa-check"></i><b>15.3</b> Ridge Regression as a Linear Kernel Model</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>16</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="16.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>16.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="16.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>16.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>16.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>16.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="16.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>16.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="16.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>16.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="16.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>16.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="16.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>16.7</b> SVM as a Penalized Model</a></li>
<li class="chapter" data-level="16.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>16.8</b> Kernel and Feature Maps: Another Example</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html"><i class="fa fa-check"></i><b>17</b> The Representer Theorem</a>
<ul>
<li class="chapter" data-level="17.1" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#the-representer-theorem-1"><i class="fa fa-check"></i><b>17.1</b> The Representer Theorem</a></li>
<li class="chapter" data-level="17.2" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#notes-on-application"><i class="fa fa-check"></i><b>17.2</b> Notes on Application</a></li>
</ul></li>
<li class="part"><span><b>VI Trees and Ensembles</b></span></li>
<li class="chapter" data-level="18" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>18</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="18.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>18.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="18.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>18.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="18.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>18.3</b> Regression Trees</a></li>
<li class="chapter" data-level="18.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>18.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="18.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>18.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>19</b> Random Forests</a>
<ul>
<li class="chapter" data-level="19.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>19.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="19.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>19.2</b> Random Forests</a></li>
<li class="chapter" data-level="19.3" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forests"><i class="fa fa-check"></i><b>19.3</b> Kernel view of Random Forests</a></li>
<li class="chapter" data-level="19.4" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>19.4</b> Variable Importance</a></li>
<li class="chapter" data-level="19.5" data-path="random-forests.html"><a href="random-forests.html#adaptiveness-of-random-forest-kernel"><i class="fa fa-check"></i><b>19.5</b> Adaptiveness of Random Forest Kernel</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>20</b> Boosting</a>
<ul>
<li class="chapter" data-level="20.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>20.1</b> AdaBoost</a></li>
<li class="chapter" data-level="20.2" data-path="boosting.html"><a href="boosting.html#training-error-of-adaboost"><i class="fa fa-check"></i><b>20.2</b> Training Error of AdaBoost</a></li>
<li class="chapter" data-level="20.3" data-path="boosting.html"><a href="boosting.html#tuning-the-number-of-trees"><i class="fa fa-check"></i><b>20.3</b> Tuning the Number of Trees</a></li>
<li class="chapter" data-level="20.4" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>20.4</b> Gradient Boosting</a></li>
<li class="chapter" data-level="20.5" data-path="boosting.html"><a href="boosting.html#gradient-boosting-with-logistic-link"><i class="fa fa-check"></i><b>20.5</b> Gradient Boosting with Logistic Link</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Learning</b></span></li>
<li class="chapter" data-level="21" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>21</b> K-Means</a>
<ul>
<li class="chapter" data-level="21.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>21.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="21.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>21.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="21.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>21.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>22</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="22.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>22.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="22.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>22.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="22.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>22.3</b> Example 2: RNA Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>23</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="23.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>23.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>23.1.1</b> Note: Scaling</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>23.2</b> Example 1: <code>iris</code> Data</a></li>
<li class="chapter" data-level="23.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>23.3</b> Example 2: Handwritten Digits</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>24</b> Self-Organizing Map</a>
<ul>
<li class="chapter" data-level="24.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>24.1</b> Basic Concepts</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>25</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="25.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#an-example"><i class="fa fa-check"></i><b>25.1</b> An Example</a></li>
<li class="chapter" data-level="25.2" data-path="spectral-clustering.html"><a href="spectral-clustering.html#adjacency-matrix"><i class="fa fa-check"></i><b>25.2</b> Adjacency Matrix</a></li>
<li class="chapter" data-level="25.3" data-path="spectral-clustering.html"><a href="spectral-clustering.html#laplacian-matrix"><i class="fa fa-check"></i><b>25.3</b> Laplacian Matrix</a></li>
<li class="chapter" data-level="25.4" data-path="spectral-clustering.html"><a href="spectral-clustering.html#derivation-of-the-feature-embedding"><i class="fa fa-check"></i><b>25.4</b> Derivation of the Feature Embedding</a></li>
<li class="chapter" data-level="25.5" data-path="spectral-clustering.html"><a href="spectral-clustering.html#feature-embedding"><i class="fa fa-check"></i><b>25.5</b> Feature Embedding</a></li>
<li class="chapter" data-level="25.6" data-path="spectral-clustering.html"><a href="spectral-clustering.html#clustering-with-embedded-features"><i class="fa fa-check"></i><b>25.6</b> Clustering with Embedded Features</a></li>
<li class="chapter" data-level="25.7" data-path="spectral-clustering.html"><a href="spectral-clustering.html#normalized-graph-laplacian"><i class="fa fa-check"></i><b>25.7</b> Normalized Graph Laplacian</a></li>
<li class="chapter" data-level="25.8" data-path="spectral-clustering.html"><a href="spectral-clustering.html#using-a-different-adjacency-matrix"><i class="fa fa-check"></i><b>25.8</b> Using a Different Adjacency Matrix</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html"><i class="fa fa-check"></i><b>26</b> Uniform Manifold Approximation and Projection</a>
<ul>
<li class="chapter" data-level="26.1" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#an-example-1"><i class="fa fa-check"></i><b>26.1</b> An Example</a></li>
<li class="chapter" data-level="26.2" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#tuning"><i class="fa fa-check"></i><b>26.2</b> Tuning</a></li>
<li class="chapter" data-level="26.3" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#another-example"><i class="fa fa-check"></i><b>26.3</b> Another Example</a></li>
</ul></li>
<li class="part"><span><b>VIII Reference</b></span></li>
<li class="chapter" data-level="27" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>27</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2023 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="k-neariest-neighber" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Chapter 12</span> K-Neariest Neighber<a href="k-neariest-neighber.html#k-neariest-neighber" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Method we introduced before usually have very clear definition of what the parameters are. In a linear model, we have a set of parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> and our estimated function value, for any target point <span class="math inline">\(x_0\)</span> is <span class="math inline">\(x_0^\text{T}\boldsymbol{\beta}\)</span>. As the model gets more complicated, we may need models that can be even more flexible. An example we saw previously is the smoothing spline, in which the features (knots) are adaptively defined using the observed samples, and there are <span class="math inline">\(n\)</span> parameters. Many of the nonparametric models shares this property. Probably the simplest one of them is the <span class="math inline">\(K\)</span> nearest neighbor (KNN) method, which is based on the idea of local averaging, i.e., we estimate <span class="math inline">\(f(x_0)\)</span> using observations close to <span class="math inline">\(x_0\)</span>. KNN can be used for both regression and classification problems. This is traditionally called <strong>nonparametric models</strong> in statistics.</p>
<div id="definition-1" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Definition<a href="k-neariest-neighber.html#definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we collect a set of observations <span class="math inline">\(\{x_i, y_i\}_{i=1}^n\)</span>, the prediction at a new target point <span class="math inline">\(x_0\)</span> is</p>
<p><span class="math display">\[\widehat y = \frac{1}{k} \sum_{x_i \in N_k(x_0)} y_i,\]</span>
where <span class="math inline">\(N_k(x_0)\)</span> defines the <span class="math inline">\(k\)</span> samples from the training data that are closest to <span class="math inline">\(x_0\)</span>. As default, closeness is defined using a distance measure, such as the Euclidean distance. Here is a demonstration of the fitted regression function.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="k-neariest-neighber.html#cb122-1" tabindex="-1"></a>    <span class="co"># generate training data with 2*sin(x) and random Gaussian errors</span></span>
<span id="cb122-2"><a href="k-neariest-neighber.html#cb122-2" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)    </span>
<span id="cb122-3"><a href="k-neariest-neighber.html#cb122-3" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">15</span>, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb122-4"><a href="k-neariest-neighber.html#cb122-4" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x))</span>
<span id="cb122-5"><a href="k-neariest-neighber.html#cb122-5" tabindex="-1"></a>    </span>
<span id="cb122-6"><a href="k-neariest-neighber.html#cb122-6" tabindex="-1"></a>    <span class="co"># generate testing data points where we evaluate the prediction function</span></span>
<span id="cb122-7"><a href="k-neariest-neighber.html#cb122-7" tabindex="-1"></a>    test.x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.001</span>)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi</span>
<span id="cb122-8"><a href="k-neariest-neighber.html#cb122-8" tabindex="-1"></a></span>
<span id="cb122-9"><a href="k-neariest-neighber.html#cb122-9" tabindex="-1"></a>    <span class="co"># &quot;1-nearest neighbor&quot; regression using kknn package</span></span>
<span id="cb122-10"><a href="k-neariest-neighber.html#cb122-10" tabindex="-1"></a>    <span class="fu">library</span>(kknn)</span>
<span id="cb122-11"><a href="k-neariest-neighber.html#cb122-11" tabindex="-1"></a>    knn.fit <span class="ot">=</span> <span class="fu">kknn</span>(y <span class="sc">~</span> x, <span class="at">train =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), </span>
<span id="cb122-12"><a href="k-neariest-neighber.html#cb122-12" tabindex="-1"></a>                   <span class="at">test =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> test.x),</span>
<span id="cb122-13"><a href="k-neariest-neighber.html#cb122-13" tabindex="-1"></a>                   <span class="at">k =</span> <span class="dv">1</span>, <span class="at">kernel =</span> <span class="st">&quot;rectangular&quot;</span>)</span>
<span id="cb122-14"><a href="k-neariest-neighber.html#cb122-14" tabindex="-1"></a>    test.pred <span class="ot">=</span> knn.fit<span class="sc">$</span>fitted.values</span>
<span id="cb122-15"><a href="k-neariest-neighber.html#cb122-15" tabindex="-1"></a>    </span>
<span id="cb122-16"><a href="k-neariest-neighber.html#cb122-16" tabindex="-1"></a>    <span class="co"># plot the data</span></span>
<span id="cb122-17"><a href="k-neariest-neighber.html#cb122-17" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">4</span>))</span>
<span id="cb122-18"><a href="k-neariest-neighber.html#cb122-18" tabindex="-1"></a>    <span class="fu">plot</span>(x, y, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi), <span class="at">pch =</span> <span class="st">&quot;o&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, </span>
<span id="cb122-19"><a href="k-neariest-neighber.html#cb122-19" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb122-20"><a href="k-neariest-neighber.html#cb122-20" tabindex="-1"></a>    <span class="fu">title</span>(<span class="at">main=</span><span class="st">&quot;1-Nearest Neighbor Regression&quot;</span>, <span class="at">cex.main =</span> <span class="fl">1.5</span>)</span>
<span id="cb122-21"><a href="k-neariest-neighber.html#cb122-21" tabindex="-1"></a>    </span>
<span id="cb122-22"><a href="k-neariest-neighber.html#cb122-22" tabindex="-1"></a>    <span class="co"># plot the true regression line</span></span>
<span id="cb122-23"><a href="k-neariest-neighber.html#cb122-23" tabindex="-1"></a>    <span class="fu">lines</span>(test.x, <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(test.x), <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb122-24"><a href="k-neariest-neighber.html#cb122-24" tabindex="-1"></a>    </span>
<span id="cb122-25"><a href="k-neariest-neighber.html#cb122-25" tabindex="-1"></a>    <span class="co"># plot the fitted line</span></span>
<span id="cb122-26"><a href="k-neariest-neighber.html#cb122-26" tabindex="-1"></a>    <span class="fu">lines</span>(test.x, test.pred, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb122-27"><a href="k-neariest-neighber.html#cb122-27" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Fitted line&quot;</span>, <span class="st">&quot;True function&quot;</span>), </span>
<span id="cb122-28"><a href="k-neariest-neighber.html#cb122-28" tabindex="-1"></a>           <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-166-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="tuning-k" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Tuning <span class="math inline">\(k\)</span><a href="k-neariest-neighber.html#tuning-k" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Tuning <span class="math inline">\(k\)</span> is crucial for <span class="math inline">\(k\)</span>NN. Let’s observe its effect. This time, I generate 200 observations. For 1NN, the fitted regression line is very jumpy.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="k-neariest-neighber.html#cb123-1" tabindex="-1"></a>  <span class="co"># generate more data</span></span>
<span id="cb123-2"><a href="k-neariest-neighber.html#cb123-2" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb123-3"><a href="k-neariest-neighber.html#cb123-3" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">200</span></span>
<span id="cb123-4"><a href="k-neariest-neighber.html#cb123-4" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb123-5"><a href="k-neariest-neighber.html#cb123-5" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x))</span>
<span id="cb123-6"><a href="k-neariest-neighber.html#cb123-6" tabindex="-1"></a></span>
<span id="cb123-7"><a href="k-neariest-neighber.html#cb123-7" tabindex="-1"></a>  test.y <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sin</span>(test.x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(test.x))</span>
<span id="cb123-8"><a href="k-neariest-neighber.html#cb123-8" tabindex="-1"></a>  </span>
<span id="cb123-9"><a href="k-neariest-neighber.html#cb123-9" tabindex="-1"></a>  <span class="co"># 1-nearest neighbor</span></span>
<span id="cb123-10"><a href="k-neariest-neighber.html#cb123-10" tabindex="-1"></a>  knn.fit <span class="ot">=</span> <span class="fu">kknn</span>(y <span class="sc">~</span> x, <span class="at">train =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), </span>
<span id="cb123-11"><a href="k-neariest-neighber.html#cb123-11" tabindex="-1"></a>                 <span class="at">test =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> test.x),</span>
<span id="cb123-12"><a href="k-neariest-neighber.html#cb123-12" tabindex="-1"></a>                 <span class="at">k =</span> <span class="dv">1</span>, <span class="at">kernel =</span> <span class="st">&quot;rectangular&quot;</span>)</span>
<span id="cb123-13"><a href="k-neariest-neighber.html#cb123-13" tabindex="-1"></a>  test.pred <span class="ot">=</span> knn.fit<span class="sc">$</span>fitted.values</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-168-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can evaluate the prediction error of this model:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="k-neariest-neighber.html#cb124-1" tabindex="-1"></a>  <span class="co"># prediction error</span></span>
<span id="cb124-2"><a href="k-neariest-neighber.html#cb124-2" tabindex="-1"></a>  <span class="fu">mean</span>((test.pred <span class="sc">-</span> test.y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb124-3"><a href="k-neariest-neighber.html#cb124-3" tabindex="-1"></a><span class="do">## [1] 2.097488</span></span></code></pre></div>
<p>If we consider different values of <span class="math inline">\(k\)</span>, we can observe the trade-off between bias and variance.</p>
<pre><code>## Prediction Error for K = 1 : 2.09748780881932
## Prediction Error for K = 5 : 1.39071867992277
## Prediction Error for K = 10 : 1.24696415340282
## Prediction Error for K = 33 : 1.21589627474692
## Prediction Error for K = 66 : 1.37604707375972
## Prediction Error for K = 100 : 1.42868908518756</code></pre>
<p><img src="SMLR_files/figure-html/unnamed-chunk-170-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>As <span class="math inline">\(k\)</span> increases, we have a more stable model, i.e., smaller variance. However, the bias is also increased. As <span class="math inline">\(k\)</span> decreases, the bias decreases, but the model is less stable.</p>
</div>
<div id="the-bias-variance-trade-off" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> The Bias-variance Trade-off<a href="k-neariest-neighber.html#the-bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Formally, the prediction error (at a given target point <span class="math inline">\(x_0\)</span>) can be broke down into three parts: the <strong>irreducible error</strong>, the <strong>bias squared</strong>, and the <strong>variance</strong>.</p>
<span class="math display">\[\begin{aligned}
\text{E}\Big[ \big( Y - \widehat f(x_0) \big)^2 \Big] &amp;= \text{E}\Big[ \big( Y - f(x_0) + f(x_0) -  \text{E}[\widehat f(x_0)] + \text{E}[\widehat f(x_0)] - \widehat f(x_0) \big)^2 \Big] \\
&amp;= \text{E}\Big[ \big( Y - f(x_0) \big)^2 \Big] + \text{E}\Big[ \big(f(x_0) - \text{E}[\widehat f(x_0)] \big)^2 \Big] + \text{E}\Big[ \big(E[\widehat f(x_0)] - \widehat f(x_0) \big)^2 \Big] + \text{Cross Terms}\\
&amp;= \underbrace{\text{E}\Big[ ( Y - f(x_0))^2 \big]}_{\text{Irreducible Error}} +
\underbrace{\Big(f(x_0) - \text{E}[\widehat f(x_0)]\Big)^2}_{\text{Bias}^2} +
\underbrace{\text{E}\Big[ \big(\widehat f(x_0) - \text{E}[\widehat f(x_0)] \big)^2 \Big]}_{\text{Variance}}
\end{aligned}\]</span>
<p>As we can see from the previous example, when <span class="math inline">\(k=1\)</span>, the prediction error is about 2. This is because for all the testing points, the theoretical irreducible error is 1 (variance of the error term), the bias is almost 0 since the function is smooth, and the variance is the variance of 1 nearest neighbor, which is again 1. On the other extreme side, when <span class="math inline">\(k = n\)</span>, the variance should be in the level of <span class="math inline">\(1/n\)</span>, the bias is the difference between the sin function and the overall average. Overall, we can expect the trend:</p>
<ul>
<li>As <span class="math inline">\(k\)</span> increases, bias increases and variance decreases</li>
<li>As <span class="math inline">\(k\)</span> decreases, bias decreases and variance increases</li>
</ul>
</div>
<div id="knn-for-classification" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> KNN for Classification<a href="k-neariest-neighber.html#knn-for-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For classification, KNN is different from the regression model in term of finding neighbers. The only difference is to majority voting instead of averaging. Majority voting means that we look for the most popular class label among its neighbors. For 1NN, it is simply the class of the closest neighbor. The visualization of 1NN is a Voronoi tessellation. The plot on the left is some randomly observed data in <span class="math inline">\([0, 1]^2\)</span>, and the plot on the right is the corresponding 1NN classification model.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-171-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-1-an-artificial-data" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> Example 1: An artificial data<a href="k-neariest-neighber.html#example-1-an-artificial-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use artificial data from the <code>ElemStatLearn</code> package.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="k-neariest-neighber.html#cb126-1" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb126-2"><a href="k-neariest-neighber.html#cb126-2" tabindex="-1"></a>  </span>
<span id="cb126-3"><a href="k-neariest-neighber.html#cb126-3" tabindex="-1"></a>  x <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>x</span>
<span id="cb126-4"><a href="k-neariest-neighber.html#cb126-4" tabindex="-1"></a>  y <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>y</span>
<span id="cb126-5"><a href="k-neariest-neighber.html#cb126-5" tabindex="-1"></a>  xnew <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>xnew</span>
<span id="cb126-6"><a href="k-neariest-neighber.html#cb126-6" tabindex="-1"></a>  </span>
<span id="cb126-7"><a href="k-neariest-neighber.html#cb126-7" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">4</span>))</span>
<span id="cb126-8"><a href="k-neariest-neighber.html#cb126-8" tabindex="-1"></a>  <span class="fu">plot</span>(x, <span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), </span>
<span id="cb126-9"><a href="k-neariest-neighber.html#cb126-9" tabindex="-1"></a>       <span class="at">axes =</span> <span class="cn">FALSE</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb126-10"><a href="k-neariest-neighber.html#cb126-10" tabindex="-1"></a>  <span class="fu">box</span>()</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-173-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The decision boundary is highly nonlinear. We can utilize the <code>contour()</code> function to demonstrate the result.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="k-neariest-neighber.html#cb127-1" tabindex="-1"></a>  <span class="co"># knn classification </span></span>
<span id="cb127-2"><a href="k-neariest-neighber.html#cb127-2" tabindex="-1"></a>  k <span class="ot">=</span> <span class="dv">15</span></span>
<span id="cb127-3"><a href="k-neariest-neighber.html#cb127-3" tabindex="-1"></a>  knn.fit <span class="ot">&lt;-</span> <span class="fu">knn</span>(x, xnew, y, <span class="at">k=</span>k)</span>
<span id="cb127-4"><a href="k-neariest-neighber.html#cb127-4" tabindex="-1"></a>  </span>
<span id="cb127-5"><a href="k-neariest-neighber.html#cb127-5" tabindex="-1"></a>  px1 <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>px1</span>
<span id="cb127-6"><a href="k-neariest-neighber.html#cb127-6" tabindex="-1"></a>  px2 <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>px2</span>
<span id="cb127-7"><a href="k-neariest-neighber.html#cb127-7" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">matrix</span>(knn.fit <span class="sc">==</span> <span class="st">&quot;1&quot;</span>, <span class="fu">length</span>(px1), <span class="fu">length</span>(px2))</span>
<span id="cb127-8"><a href="k-neariest-neighber.html#cb127-8" tabindex="-1"></a>  </span>
<span id="cb127-9"><a href="k-neariest-neighber.html#cb127-9" tabindex="-1"></a>  <span class="fu">contour</span>(px1, px2, pred, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">labels=</span><span class="st">&quot;&quot;</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span>
<span id="cb127-10"><a href="k-neariest-neighber.html#cb127-10" tabindex="-1"></a>  <span class="fu">box</span>()</span>
<span id="cb127-11"><a href="k-neariest-neighber.html#cb127-11" tabindex="-1"></a>  <span class="fu">title</span>(<span class="fu">paste</span>(k, <span class="st">&quot;-Nearest Neighbor&quot;</span>, <span class="at">sep=</span> <span class="st">&quot;&quot;</span>))</span>
<span id="cb127-12"><a href="k-neariest-neighber.html#cb127-12" tabindex="-1"></a>  <span class="fu">points</span>(x, <span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb127-13"><a href="k-neariest-neighber.html#cb127-13" tabindex="-1"></a>  mesh <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(px1, px2)</span>
<span id="cb127-14"><a href="k-neariest-neighber.html#cb127-14" tabindex="-1"></a>  <span class="fu">points</span>(mesh, <span class="at">pch=</span><span class="st">&quot;.&quot;</span>, <span class="at">cex=</span><span class="fl">1.2</span>, <span class="at">col=</span><span class="fu">ifelse</span>(pred, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-174-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>We can evaluate the in-sample prediction result of this model using a confusion matrix:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="k-neariest-neighber.html#cb128-1" tabindex="-1"></a>  <span class="co"># the confusion matrix</span></span>
<span id="cb128-2"><a href="k-neariest-neighber.html#cb128-2" tabindex="-1"></a>  knn.fit <span class="ot">&lt;-</span> <span class="fu">knn</span>(x, x, y, <span class="at">k =</span> <span class="dv">15</span>)</span>
<span id="cb128-3"><a href="k-neariest-neighber.html#cb128-3" tabindex="-1"></a>  xtab <span class="ot">=</span> <span class="fu">table</span>(knn.fit, y)</span>
<span id="cb128-4"><a href="k-neariest-neighber.html#cb128-4" tabindex="-1"></a>  </span>
<span id="cb128-5"><a href="k-neariest-neighber.html#cb128-5" tabindex="-1"></a>  <span class="fu">library</span>(caret)</span>
<span id="cb128-6"><a href="k-neariest-neighber.html#cb128-6" tabindex="-1"></a>  <span class="fu">confusionMatrix</span>(xtab)</span>
<span id="cb128-7"><a href="k-neariest-neighber.html#cb128-7" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb128-8"><a href="k-neariest-neighber.html#cb128-8" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb128-9"><a href="k-neariest-neighber.html#cb128-9" tabindex="-1"></a><span class="do">##        y</span></span>
<span id="cb128-10"><a href="k-neariest-neighber.html#cb128-10" tabindex="-1"></a><span class="do">## knn.fit  0  1</span></span>
<span id="cb128-11"><a href="k-neariest-neighber.html#cb128-11" tabindex="-1"></a><span class="do">##       0 82 13</span></span>
<span id="cb128-12"><a href="k-neariest-neighber.html#cb128-12" tabindex="-1"></a><span class="do">##       1 18 87</span></span>
<span id="cb128-13"><a href="k-neariest-neighber.html#cb128-13" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb128-14"><a href="k-neariest-neighber.html#cb128-14" tabindex="-1"></a><span class="do">##                Accuracy : 0.845           </span></span>
<span id="cb128-15"><a href="k-neariest-neighber.html#cb128-15" tabindex="-1"></a><span class="do">##                  95% CI : (0.7873, 0.8922)</span></span>
<span id="cb128-16"><a href="k-neariest-neighber.html#cb128-16" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5             </span></span>
<span id="cb128-17"><a href="k-neariest-neighber.html#cb128-17" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : &lt;2e-16          </span></span>
<span id="cb128-18"><a href="k-neariest-neighber.html#cb128-18" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb128-19"><a href="k-neariest-neighber.html#cb128-19" tabindex="-1"></a><span class="do">##                   Kappa : 0.69            </span></span>
<span id="cb128-20"><a href="k-neariest-neighber.html#cb128-20" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb128-21"><a href="k-neariest-neighber.html#cb128-21" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.4725          </span></span>
<span id="cb128-22"><a href="k-neariest-neighber.html#cb128-22" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb128-23"><a href="k-neariest-neighber.html#cb128-23" tabindex="-1"></a><span class="do">##             Sensitivity : 0.8200          </span></span>
<span id="cb128-24"><a href="k-neariest-neighber.html#cb128-24" tabindex="-1"></a><span class="do">##             Specificity : 0.8700          </span></span>
<span id="cb128-25"><a href="k-neariest-neighber.html#cb128-25" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.8632          </span></span>
<span id="cb128-26"><a href="k-neariest-neighber.html#cb128-26" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.8286          </span></span>
<span id="cb128-27"><a href="k-neariest-neighber.html#cb128-27" tabindex="-1"></a><span class="do">##              Prevalence : 0.5000          </span></span>
<span id="cb128-28"><a href="k-neariest-neighber.html#cb128-28" tabindex="-1"></a><span class="do">##          Detection Rate : 0.4100          </span></span>
<span id="cb128-29"><a href="k-neariest-neighber.html#cb128-29" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.4750          </span></span>
<span id="cb128-30"><a href="k-neariest-neighber.html#cb128-30" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.8450          </span></span>
<span id="cb128-31"><a href="k-neariest-neighber.html#cb128-31" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb128-32"><a href="k-neariest-neighber.html#cb128-32" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : 0               </span></span>
<span id="cb128-33"><a href="k-neariest-neighber.html#cb128-33" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
</div>
<div id="degrees-of-freedom-1" class="section level2 hasAnchor" number="12.6">
<h2><span class="header-section-number">12.6</span> Degrees of Freedom<a href="k-neariest-neighber.html#degrees-of-freedom-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>From our ridge lecture, we have a definition of the degrees of freedom. For a linear model, the degrees of freedom is <span class="math inline">\(p\)</span>. For KNN, we can simply verify this by</p>
<p><span class="math display">\[\begin{align}
\text{DF}(\widehat{f}) =&amp; \frac{1}{\sigma^2} \sum_{i = 1}^n \text{Cov}(\widehat{y}_i, y_i)\\
=&amp; \frac{1}{\sigma^2} \sum_{i = 1}^n \frac{\sigma^2}{k}\\
=&amp; \frac{n}{k}
\end{align}\]</span></p>
</div>
<div id="tuning-with-the-caret-package" class="section level2 hasAnchor" number="12.7">
<h2><span class="header-section-number">12.7</span> Tuning with the <code>caret</code> Package<a href="k-neariest-neighber.html#tuning-with-the-caret-package" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <code>caret</code> package has some built-in feature that can tune some popular machine learning models using cross-validation. The cross-validation settings need to be specified using the <span class="math inline">\(trainControl()\)</span> function.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="k-neariest-neighber.html#cb129-1" tabindex="-1"></a>  <span class="fu">library</span>(caret)</span>
<span id="cb129-2"><a href="k-neariest-neighber.html#cb129-2" tabindex="-1"></a>  control <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>There are other cross-validation methods, such as <code>repeatedcv</code> the repeats the CV several times, and leave-one-out CV <code>LOOCV</code>. For more details, you can read the <a href="https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/trainControl">documentation</a>. We can then setup the training by specifying a grid of <span class="math inline">\(k\)</span> values, and also the CV setup. Make sure that you specify <code>method = "knn"</code> and also construct the outcome as a factor in a data frame.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="k-neariest-neighber.html#cb130-1" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb130-2"><a href="k-neariest-neighber.html#cb130-2" tabindex="-1"></a>  knn.cvfit <span class="ot">&lt;-</span> <span class="fu">train</span>(y <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>, </span>
<span id="cb130-3"><a href="k-neariest-neighber.html#cb130-3" tabindex="-1"></a>                     <span class="at">data =</span> <span class="fu">data.frame</span>(<span class="st">&quot;x&quot;</span> <span class="ot">=</span> x, <span class="st">&quot;y&quot;</span> <span class="ot">=</span> <span class="fu">as.factor</span>(y)),</span>
<span id="cb130-4"><a href="k-neariest-neighber.html#cb130-4" tabindex="-1"></a>                     <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">40</span>, <span class="dv">1</span>)),</span>
<span id="cb130-5"><a href="k-neariest-neighber.html#cb130-5" tabindex="-1"></a>                     <span class="at">trControl =</span> control)</span>
<span id="cb130-6"><a href="k-neariest-neighber.html#cb130-6" tabindex="-1"></a>  </span>
<span id="cb130-7"><a href="k-neariest-neighber.html#cb130-7" tabindex="-1"></a>  <span class="fu">plot</span>(knn.cvfit<span class="sc">$</span>results<span class="sc">$</span>k, <span class="dv">1</span><span class="sc">-</span>knn.cvfit<span class="sc">$</span>results<span class="sc">$</span>Accuracy,</span>
<span id="cb130-8"><a href="k-neariest-neighber.html#cb130-8" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;K&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Classification Error&quot;</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>,</span>
<span id="cb130-9"><a href="k-neariest-neighber.html#cb130-9" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-178-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Print out the fitted object, we can see that the best <span class="math inline">\(k\)</span> is 6. And there is a clear “U” shaped pattern that shows the potential bias-variance trade-off.</p>
</div>
<div id="distance-measures" class="section level2 hasAnchor" number="12.8">
<h2><span class="header-section-number">12.8</span> Distance Measures<a href="k-neariest-neighber.html#distance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Closeness between two points needs to be defined based on some distance measures. By default, we use the squared Euclidean distance (<span class="math inline">\(\ell_2\)</span> norm) for continuous variables:</p>
<p><span class="math display">\[d^2(\mathbf{u}, \mathbf{v}) = \lVert \mathbf{u}- \mathbf{v}\rVert_2^2 = \sum_{j=1}^p (u_j, v_j)^2.\]</span>
However, this measure is not scale invariant. A variable with large scale can dominate this measure. Hence, we often consider a normalized version:</p>
<p><span class="math display">\[d^2(\mathbf{u}, \mathbf{v}) = \sum_{j=1}^p \frac{(u_j, v_j)^2}{\sigma_j^2},\]</span>
where <span class="math inline">\(\sigma_j^2\)</span> can be estimated using the sample variance of variable <span class="math inline">\(j\)</span>. Another choice that further taking the covariance structure into consideration is the <strong>Mahalanobis distance</strong>:</p>
<p><span class="math display">\[d^2(\mathbf{u}, \mathbf{v}) = (\mathbf{u}- \mathbf{v})^\text{T}\Sigma^{-1} (\mathbf{u}- \mathbf{v}),\]</span>
where <span class="math inline">\(\Sigma\)</span> is the covariance matrix, and can be estimated using the sample covariance. In the following plot, the red cross and orange cross have the same Euclidean distance to the center. However, the red cross is more of a “outlier” based on the joint distribution. The Mahalanobis distance would reflect this.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="k-neariest-neighber.html#cb131-1" tabindex="-1"></a>  x<span class="ot">=</span><span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb131-2"><a href="k-neariest-neighber.html#cb131-2" tabindex="-1"></a>  y<span class="ot">=</span><span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.3</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fl">0.3</span><span class="sc">*</span><span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb131-3"><a href="k-neariest-neighber.html#cb131-3" tabindex="-1"></a></span>
<span id="cb131-4"><a href="k-neariest-neighber.html#cb131-4" tabindex="-1"></a>  <span class="fu">library</span>(car)</span>
<span id="cb131-5"><a href="k-neariest-neighber.html#cb131-5" tabindex="-1"></a>  <span class="fu">dataEllipse</span>(x, y, <span class="at">levels=</span><span class="fu">c</span>(<span class="fl">0.6</span>, <span class="fl">0.9</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb131-6"><a href="k-neariest-neighber.html#cb131-6" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb131-7"><a href="k-neariest-neighber.html#cb131-7" tabindex="-1"></a>  <span class="fu">points</span>(<span class="dv">1</span>, <span class="fl">1.5</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">pch =</span> <span class="dv">4</span>, <span class="at">cex =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-179-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>For categorical variables, the Hamming distance is commonly used:</p>
<p><span class="math display">\[d(\mathbf{u}, \mathbf{v}) = \sum_{j=1}^p I(u_j \neq v_j).\]</span>
It simply counts how many entries are not the same.</p>
</div>
<div id="nn-error-bound" class="section level2 hasAnchor" number="12.9">
<h2><span class="header-section-number">12.9</span> 1NN Error Bound<a href="k-neariest-neighber.html#nn-error-bound" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can show that 1NN can achieve reasonable performance for fixed <span class="math inline">\(p\)</span>, as <span class="math inline">\(n \rightarrow \infty\)</span> by showing that the 1NN error is no more than twice of the Bayes error, which is the smaller one of <span class="math inline">\(P(Y = 1 | X = x_0)\)</span> and <span class="math inline">\(1 - P(Y = 1 | X = x_0)\)</span>.</p>
<p>Let’s denote <span class="math inline">\(x_{1nn}\)</span> the closest neighbor of <span class="math inline">\(x_0\)</span>, we have <span class="math inline">\(d(x_0, x_{1nn}) \rightarrow 0\)</span> by the law of large numbers. Assuming smoothness, we have <span class="math inline">\(P(Y = 1 | x_{1nn}) \rightarrow P(Y = 1 | x_0)\)</span>. Hence, the 1NN error is the chance we make a wrong prediction, which can happen in two situations. WLOG, let’s assume that <span class="math inline">\(P(Y = 1 | X = x_0)\)</span> is larger than <span class="math inline">\(1-P(Y = 1 | X = x_0)\)</span>, then</p>
<p><span class="math display">\[\begin{align}
&amp; \,P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})] + P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})]\\
\leq &amp; \,[ 1 - P(Y=1|x_{1nn})] + [ 1 - P(Y=1|x_{1nn})]\\
\rightarrow &amp; \, 2[ 1 - P(Y=1|x_0)]\\
= &amp; \,2 \times \text{Bayes Error}\\
\end{align}\]</span></p>
<p>This is a crude bound, but shows that 1NN can still be a reasonable estimator when the noise is small (Bayes error small).</p>
</div>
<div id="example-2-handwritten-digit-data" class="section level2 hasAnchor" number="12.10">
<h2><span class="header-section-number">12.10</span> Example 2: Handwritten Digit Data<a href="k-neariest-neighber.html#example-2-handwritten-digit-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s consider another example using the handwritten digit data. Each observation in this data is a <span class="math inline">\(16 \times 16\)</span> pixel image. Hence, the total number of variables is <span class="math inline">\(256\)</span>. At each pixel, we have the gray scale as the numerical value.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="k-neariest-neighber.html#cb132-1" tabindex="-1"></a>  <span class="co"># Handwritten Digit Recognition Data</span></span>
<span id="cb132-2"><a href="k-neariest-neighber.html#cb132-2" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb132-3"><a href="k-neariest-neighber.html#cb132-3" tabindex="-1"></a>  <span class="co"># the first column is the true digit</span></span>
<span id="cb132-4"><a href="k-neariest-neighber.html#cb132-4" tabindex="-1"></a>  <span class="fu">dim</span>(zip.train)</span>
<span id="cb132-5"><a href="k-neariest-neighber.html#cb132-5" tabindex="-1"></a><span class="do">## [1] 7291  257</span></span>
<span id="cb132-6"><a href="k-neariest-neighber.html#cb132-6" tabindex="-1"></a>  <span class="fu">dim</span>(zip.test)</span>
<span id="cb132-7"><a href="k-neariest-neighber.html#cb132-7" tabindex="-1"></a><span class="do">## [1] 2007  257</span></span>
<span id="cb132-8"><a href="k-neariest-neighber.html#cb132-8" tabindex="-1"></a>  </span>
<span id="cb132-9"><a href="k-neariest-neighber.html#cb132-9" tabindex="-1"></a>  <span class="co"># look at one sample</span></span>
<span id="cb132-10"><a href="k-neariest-neighber.html#cb132-10" tabindex="-1"></a>  <span class="fu">image</span>(<span class="fu">zip2image</span>(zip.train, <span class="dv">1</span>), <span class="at">col=</span><span class="fu">gray</span>(<span class="dv">256</span><span class="sc">:</span><span class="dv">0</span><span class="sc">/</span><span class="dv">256</span>), <span class="at">zlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), </span>
<span id="cb132-11"><a href="k-neariest-neighber.html#cb132-11" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>, <span class="at">axes =</span> <span class="cn">FALSE</span>)</span>
<span id="cb132-12"><a href="k-neariest-neighber.html#cb132-12" tabindex="-1"></a><span class="do">## [1] &quot;digit  6  taken&quot;</span></span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-180-1.png" width="20%" style="display: block; margin: auto;" /></p>
<p>We use 3NN to predict all samples in the testing data. The model is fairly accurate.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="k-neariest-neighber.html#cb133-1" tabindex="-1"></a>  <span class="co"># fit 3nn model and calculate the error</span></span>
<span id="cb133-2"><a href="k-neariest-neighber.html#cb133-2" tabindex="-1"></a>  knn.fit <span class="ot">&lt;-</span> <span class="fu">knn</span>(zip.train[, <span class="dv">2</span><span class="sc">:</span><span class="dv">257</span>], zip.test[, <span class="dv">2</span><span class="sc">:</span><span class="dv">257</span>], zip.train[, <span class="dv">1</span>], <span class="at">k=</span><span class="dv">3</span>)</span>
<span id="cb133-3"><a href="k-neariest-neighber.html#cb133-3" tabindex="-1"></a>  </span>
<span id="cb133-4"><a href="k-neariest-neighber.html#cb133-4" tabindex="-1"></a>  <span class="co"># overall prediction error</span></span>
<span id="cb133-5"><a href="k-neariest-neighber.html#cb133-5" tabindex="-1"></a>  <span class="fu">mean</span>(knn.fit <span class="sc">!=</span> zip.test[,<span class="dv">1</span>])</span>
<span id="cb133-6"><a href="k-neariest-neighber.html#cb133-6" tabindex="-1"></a><span class="do">## [1] 0.05430992</span></span>
<span id="cb133-7"><a href="k-neariest-neighber.html#cb133-7" tabindex="-1"></a>  </span>
<span id="cb133-8"><a href="k-neariest-neighber.html#cb133-8" tabindex="-1"></a>  <span class="co"># the confusion matrix</span></span>
<span id="cb133-9"><a href="k-neariest-neighber.html#cb133-9" tabindex="-1"></a>  <span class="fu">table</span>(knn.fit, zip.test[,<span class="dv">1</span>])</span>
<span id="cb133-10"><a href="k-neariest-neighber.html#cb133-10" tabindex="-1"></a><span class="do">##        </span></span>
<span id="cb133-11"><a href="k-neariest-neighber.html#cb133-11" tabindex="-1"></a><span class="do">## knn.fit   0   1   2   3   4   5   6   7   8   9</span></span>
<span id="cb133-12"><a href="k-neariest-neighber.html#cb133-12" tabindex="-1"></a><span class="do">##       0 355   0   6   1   0   3   3   0   4   2</span></span>
<span id="cb133-13"><a href="k-neariest-neighber.html#cb133-13" tabindex="-1"></a><span class="do">##       1   0 258   0   0   2   0   0   1   0   0</span></span>
<span id="cb133-14"><a href="k-neariest-neighber.html#cb133-14" tabindex="-1"></a><span class="do">##       2   1   0 182   2   0   2   1   1   2   0</span></span>
<span id="cb133-15"><a href="k-neariest-neighber.html#cb133-15" tabindex="-1"></a><span class="do">##       3   0   0   1 153   0   4   0   1   5   0</span></span>
<span id="cb133-16"><a href="k-neariest-neighber.html#cb133-16" tabindex="-1"></a><span class="do">##       4   0   3   1   0 183   0   2   4   0   3</span></span>
<span id="cb133-17"><a href="k-neariest-neighber.html#cb133-17" tabindex="-1"></a><span class="do">##       5   0   0   0   7   2 146   0   0   1   0</span></span>
<span id="cb133-18"><a href="k-neariest-neighber.html#cb133-18" tabindex="-1"></a><span class="do">##       6   0   2   2   0   2   0 164   0   0   0</span></span>
<span id="cb133-19"><a href="k-neariest-neighber.html#cb133-19" tabindex="-1"></a><span class="do">##       7   2   1   2   1   2   0   0 138   1   4</span></span>
<span id="cb133-20"><a href="k-neariest-neighber.html#cb133-20" tabindex="-1"></a><span class="do">##       8   0   0   4   0   1   1   0   1 151   0</span></span>
<span id="cb133-21"><a href="k-neariest-neighber.html#cb133-21" tabindex="-1"></a><span class="do">##       9   1   0   0   2   8   4   0   1   2 168</span></span></code></pre></div>
</div>
<div id="curse-of-dimensionality" class="section level2 hasAnchor" number="12.11">
<h2><span class="header-section-number">12.11</span> Curse of Dimensionality<a href="k-neariest-neighber.html#curse-of-dimensionality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many of the practical problems we encounter today are high-dimensional. The resolution of the handwritten digit example is <span class="math inline">\(16 \times 16 = 256\)</span>. Genetic studies often involves more than 25K gene expressions, etc. For a given sample size <span class="math inline">\(n\)</span>, as the number of variables <span class="math inline">\(p\)</span> increases, the data becomes very sparse. Nearest neighbor methods usually do not perform very well on high-dimensional data. This is because for any given target point, there will not be enough training data that lies close enough. To see this, let’s consider a <span class="math inline">\(p\)</span>-dimensional hyper-cube. Suppose we have <span class="math inline">\(n=1000\)</span> observations uniformly spread out in this cube, and we are interested in predicting a target point with <span class="math inline">\(k=10\)</span> neighbors. If these neighbors are really close to the target point, then this would be a good estimation with small bias. Suppose <span class="math inline">\(p=2\)</span>, then we know that if we take a cube (square) with height and width both <span class="math inline">\(l = 0.1\)</span>, then there will be <span class="math inline">\(1000 \times 0.1^2 = 10\)</span> observations within the square. In general, we have the relationship</p>
<p><span class="math display">\[l^p = \frac{k}{n}\]</span>
Try different <span class="math inline">\(p\)</span>, we have</p>
<ul>
<li>If <span class="math inline">\(p = 2\)</span>, <span class="math inline">\(l = 0.1\)</span></li>
<li>If <span class="math inline">\(p = 10\)</span>, <span class="math inline">\(l = 0.63\)</span></li>
<li>If <span class="math inline">\(p = 100\)</span>, <span class="math inline">\(l = 0.955\)</span></li>
</ul>
<p>This implies that if we have 100 dimensions, then the nearest 10 observations would be 0.955 away from the target point at each dimension, this is almost at the other corner of the cube. Hence there will be a very large bias. Decreasing <span class="math inline">\(k\)</span> does not help much in this situation since even the closest point can be really far away, and the model would have large variance.</p>
<center>
<img src="images/highd.png" style="width:30.0%" />
</center>
<p>However, why our model performs well in the handwritten digit example? There is possibly (approximately) a lower dimensional representation of the data so that when you evaluate the distance on the high-dimensional space, it is just as effective as working on the low dimensional space. Dimension reduction is an important topic in statistical learning and machine learning. Many methods, such as sliced inverse regression <span class="citation">(<a href="#ref-li1991sliced">Li 1991</a>)</span> and UMAP <span class="citation">(<a href="#ref-mcinnes2018umap">McInnes, Healy, and Melville 2018</a>)</span> have been developed based on this concept.</p>
<center>
<p><img src="images/manifold.png" style="width:70.0%" /></p>
Image from <span class="citation">Cayton (<a href="#ref-cayton2005algorithms">2005</a>)</span>.
</center>

<div style="display:none;">
<!-- Conflict \def\bf{\mathbf{f}} -->
</div>
</div>
</div>
<h3> Reference<a href="reference.html#reference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-cayton2005algorithms" class="csl-entry">
Cayton, Lawrence. 2005. <span>“Algorithms for Manifold Learning.”</span> <em>Univ. Of California at San Diego Tech. Rep</em> 12 (1-17): 1.
</div>
<div id="ref-li1991sliced" class="csl-entry">
Li, Ker-Chau. 1991. <span>“Sliced Inverse Regression for Dimension Reduction.”</span> <em>Journal of the American Statistical Association</em> 86 (414): 316–27.
</div>
<div id="ref-mcinnes2018umap" class="csl-entry">
McInnes, Leland, John Healy, and James Melville. 2018. <span>“Umap: Uniform Manifold Approximation and Projection for Dimension Reduction.”</span> <em>arXiv Preprint arXiv:1802.03426</em>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="discriminant-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="kernel-smoothing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "sepia",
    "family": "serif",
    "size": 1
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
