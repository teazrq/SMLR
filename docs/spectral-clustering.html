<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 25 Spectral Clustering | Statistical Machine Learning with R</title>
  <meta name="description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 25 Spectral Clustering | Statistical Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 25 Spectral Clustering | Statistical Machine Learning with R" />
  
  <meta name="twitter:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2025-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="self-organizing-map.html"/>
<link rel="next" href="uniform-manifold-approximation-and-projection.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>Whatâ€™s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visual-studio-code.html"><a href="visual-studio-code.html"><i class="fa fa-check"></i><b>3</b> Visual Studio Code</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visual-studio-code.html"><a href="visual-studio-code.html#basics-and-resources-1"><i class="fa fa-check"></i><b>3.1</b> Basics and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linear-regression"><i class="fa fa-check"></i><b>4.2</b> Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#matrix-inversion"><i class="fa fa-check"></i><b>4.3</b> Matrix Inversion</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linearalgebra-SM"><i class="fa fa-check"></i><b>4.3.1</b> Rank-one Update</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#rank-k-update"><i class="fa fa-check"></i><b>4.3.2</b> Rank-<span class="math inline">\(k\)</span> Update</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#times-2-block-matrix-inversion"><i class="fa fa-check"></i><b>4.3.3</b> 2 <span class="math inline">\(\times\)</span> 2 Block Matrix Inversion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>5</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>5.1</b> Basic Concept</a></li>
<li class="chapter" data-level="5.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>5.2</b> Global vs.Â Local Optima</a></li>
<li class="chapter" data-level="5.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>5.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="5.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>5.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="5.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>5.5</b> Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>5.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>5.6.1</b> Newtonâ€™s Method</a></li>
<li class="chapter" data-level="5.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>5.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>5.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>5.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>5.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>5.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>6</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>6.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>6.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>6.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>6.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>6.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>6.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>6.4.1</b> Using Marrowsâ€™ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>6.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>6.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>6.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>6.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>6.6</b> Derivation of Marrowsâ€™ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>7</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>7.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="7.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>7.2</b> Ridge Penalty and the Reduced Variation</a></li>
<li class="chapter" data-level="7.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>7.3</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="7.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>7.4</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="7.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>7.5</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>7.5.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="7.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>7.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>7.6</b> Cross-validation</a></li>
<li class="chapter" data-level="7.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.7</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>7.7.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>7.8</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>7.8.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8</b> Lasso</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>8.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>8.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="8.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>8.3</b> The Solution Path</a></li>
<li class="chapter" data-level="8.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>8.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="8.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>8.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="8.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>8.6</b> Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>9</b> Spline</a>
<ul>
<li class="chapter" data-level="9.1" data-path="spline.html"><a href="spline.html#using-linear-models-for-nonlinear-trends"><i class="fa fa-check"></i><b>9.1</b> Using Linear models for Nonlinear Trends</a></li>
<li class="chapter" data-level="9.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>9.2</b> A Motivating Example and Polynomials</a></li>
<li class="chapter" data-level="9.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>9.3</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="9.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>9.4</b> Splines</a></li>
<li class="chapter" data-level="9.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>9.5</b> Spline Basis</a></li>
<li class="chapter" data-level="9.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>9.6</b> Natural Cubic Spline</a></li>
<li class="chapter" data-level="9.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>9.7</b> Smoothing Spline</a></li>
<li class="chapter" data-level="9.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>9.8</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="9.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>9.9</b> Extending Splines to Multiple Varibles</a></li>
</ul></li>
<li class="part"><span><b>III Linear Classification Models</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>10.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>10.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>10.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>10.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>11</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>11.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="11.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>11.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="11.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>11.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="11.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>11.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="11.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>11.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>11.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Models</b></span></li>
<li class="chapter" data-level="12" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>12</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>12.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="12.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>12.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="12.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>12.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="12.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>12.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="12.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#degrees-of-freedom-1"><i class="fa fa-check"></i><b>12.6</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="12.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>12.7</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="12.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>12.8</b> Distance Measures</a></li>
<li class="chapter" data-level="12.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>12.9</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="12.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>12.10</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="12.11" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>12.11</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>13</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>13.1</b> KNN vs.Â Kernel</a></li>
<li class="chapter" data-level="13.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>13.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="13.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>13.3</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="13.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>13.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off-1"><i class="fa fa-check"></i><b>13.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>13.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="13.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>13.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="13.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>13.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="13.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>13.8</b> R Implementations</a></li>
</ul></li>
<li class="part"><span><b>V Kernel Machines</b></span></li>
<li class="chapter" data-level="14" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>14</b> Reproducing Kernel Hilbert Space</a>
<ul>
<li class="chapter" data-level="14.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-motivation"><i class="fa fa-check"></i><b>14.1</b> The Motivation</a></li>
<li class="chapter" data-level="14.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#hilbert-space-preliminaries"><i class="fa fa-check"></i><b>14.2</b> Hilbert Space Preliminaries</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-space-of-square-integrable-functions"><i class="fa fa-check"></i><b>14.2.1</b> The Space of Square-Integrable Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-kernel-function"><i class="fa fa-check"></i><b>14.3</b> A Kernel Function</a></li>
<li class="chapter" data-level="14.4" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-space-of-functions"><i class="fa fa-check"></i><b>14.4</b> A Space of Functions</a></li>
<li class="chapter" data-level="14.5" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-inner-product"><i class="fa fa-check"></i><b>14.5</b> The Inner Product</a></li>
<li class="chapter" data-level="14.6" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-rkhs"><i class="fa fa-check"></i><b>14.6</b> The RKHS</a></li>
<li class="chapter" data-level="14.7" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-reproducing-property"><i class="fa fa-check"></i><b>14.7</b> The Reproducing Property</a></li>
<li class="chapter" data-level="14.8" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#smoothness"><i class="fa fa-check"></i><b>14.8</b> Smoothness</a></li>
<li class="chapter" data-level="14.9" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-moorearonszajn-theorem"><i class="fa fa-check"></i><b>14.9</b> The Mooreâ€“Aronszajn Theorem</a></li>
<li class="chapter" data-level="14.10" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#examples"><i class="fa fa-check"></i><b>14.10</b> Examples</a>
<ul>
<li class="chapter" data-level="14.10.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#brownian-motion-kernel"><i class="fa fa-check"></i><b>14.10.1</b> Brownian Motion Kernel</a></li>
<li class="chapter" data-level="14.10.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#non-positive-definite-kernel"><i class="fa fa-check"></i><b>14.10.2</b> Non-positive Definite Kernel</a></li>
<li class="chapter" data-level="14.10.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#defining-new-kernels"><i class="fa fa-check"></i><b>14.10.3</b> Defining New Kernels</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>15</b> Kernel Ridge Regression</a>
<ul>
<li class="chapter" data-level="15.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#linear-regression-as-a-constraint-optimization"><i class="fa fa-check"></i><b>15.1</b> Linear Regression as a Constraint Optimization</a></li>
<li class="chapter" data-level="15.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#the-kernel-ridge-regression"><i class="fa fa-check"></i><b>15.2</b> The Kernel Ridge Regression</a></li>
<li class="chapter" data-level="15.3" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#ridge-regression-as-a-linear-kernel-model"><i class="fa fa-check"></i><b>15.3</b> Ridge Regression as a Linear Kernel Model</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>16</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="16.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>16.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="16.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>16.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>16.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>16.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="16.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>16.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="16.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>16.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="16.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>16.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="16.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>16.7</b> SVM as a Penalized Model</a></li>
<li class="chapter" data-level="16.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>16.8</b> Kernel and Feature Maps: Another Example</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html"><i class="fa fa-check"></i><b>17</b> The Representer Theorem</a>
<ul>
<li class="chapter" data-level="17.1" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#the-representer-theorem-1"><i class="fa fa-check"></i><b>17.1</b> The Representer Theorem</a></li>
<li class="chapter" data-level="17.2" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#notes-on-application"><i class="fa fa-check"></i><b>17.2</b> Notes on Application</a></li>
</ul></li>
<li class="part"><span><b>VI Trees and Ensembles</b></span></li>
<li class="chapter" data-level="18" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>18</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="18.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>18.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="18.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>18.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="18.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>18.3</b> Regression Trees</a></li>
<li class="chapter" data-level="18.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>18.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="18.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>18.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>19</b> Random Forests</a>
<ul>
<li class="chapter" data-level="19.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>19.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="19.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>19.2</b> Random Forests</a></li>
<li class="chapter" data-level="19.3" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forests"><i class="fa fa-check"></i><b>19.3</b> Kernel view of Random Forests</a></li>
<li class="chapter" data-level="19.4" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>19.4</b> Variable Importance</a></li>
<li class="chapter" data-level="19.5" data-path="random-forests.html"><a href="random-forests.html#adaptiveness-of-random-forest-kernel"><i class="fa fa-check"></i><b>19.5</b> Adaptiveness of Random Forest Kernel</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>20</b> Boosting</a>
<ul>
<li class="chapter" data-level="20.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>20.1</b> AdaBoost</a></li>
<li class="chapter" data-level="20.2" data-path="boosting.html"><a href="boosting.html#training-error-of-adaboost"><i class="fa fa-check"></i><b>20.2</b> Training Error of AdaBoost</a></li>
<li class="chapter" data-level="20.3" data-path="boosting.html"><a href="boosting.html#tuning-the-number-of-trees"><i class="fa fa-check"></i><b>20.3</b> Tuning the Number of Trees</a></li>
<li class="chapter" data-level="20.4" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>20.4</b> Gradient Boosting</a></li>
<li class="chapter" data-level="20.5" data-path="boosting.html"><a href="boosting.html#gradient-boosting-with-logistic-link"><i class="fa fa-check"></i><b>20.5</b> Gradient Boosting with Logistic Link</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Learning</b></span></li>
<li class="chapter" data-level="21" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>21</b> K-Means</a>
<ul>
<li class="chapter" data-level="21.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>21.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="21.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>21.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="21.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>21.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>22</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="22.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>22.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="22.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>22.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="22.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>22.3</b> Example 2: RNA Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>23</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="23.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>23.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="23.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>23.1.1</b> Note: Scaling</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>23.2</b> Example 1: <code>iris</code> Data</a></li>
<li class="chapter" data-level="23.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>23.3</b> Example 2: Handwritten Digits</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>24</b> Self-Organizing Map</a>
<ul>
<li class="chapter" data-level="24.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>24.1</b> Basic Concepts</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>25</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="25.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#an-example"><i class="fa fa-check"></i><b>25.1</b> An Example</a></li>
<li class="chapter" data-level="25.2" data-path="spectral-clustering.html"><a href="spectral-clustering.html#adjacency-matrix"><i class="fa fa-check"></i><b>25.2</b> Adjacency Matrix</a></li>
<li class="chapter" data-level="25.3" data-path="spectral-clustering.html"><a href="spectral-clustering.html#laplacian-matrix"><i class="fa fa-check"></i><b>25.3</b> Laplacian Matrix</a></li>
<li class="chapter" data-level="25.4" data-path="spectral-clustering.html"><a href="spectral-clustering.html#derivation-of-the-feature-embedding"><i class="fa fa-check"></i><b>25.4</b> Derivation of the Feature Embedding</a></li>
<li class="chapter" data-level="25.5" data-path="spectral-clustering.html"><a href="spectral-clustering.html#feature-embedding"><i class="fa fa-check"></i><b>25.5</b> Feature Embedding</a></li>
<li class="chapter" data-level="25.6" data-path="spectral-clustering.html"><a href="spectral-clustering.html#clustering-with-embedded-features"><i class="fa fa-check"></i><b>25.6</b> Clustering with Embedded Features</a></li>
<li class="chapter" data-level="25.7" data-path="spectral-clustering.html"><a href="spectral-clustering.html#normalized-graph-laplacian"><i class="fa fa-check"></i><b>25.7</b> Normalized Graph Laplacian</a></li>
<li class="chapter" data-level="25.8" data-path="spectral-clustering.html"><a href="spectral-clustering.html#using-a-different-adjacency-matrix"><i class="fa fa-check"></i><b>25.8</b> Using a Different Adjacency Matrix</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html"><i class="fa fa-check"></i><b>26</b> Uniform Manifold Approximation and Projection</a>
<ul>
<li class="chapter" data-level="26.1" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#an-example-1"><i class="fa fa-check"></i><b>26.1</b> An Example</a></li>
<li class="chapter" data-level="26.2" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#tuning"><i class="fa fa-check"></i><b>26.2</b> Tuning</a></li>
<li class="chapter" data-level="26.3" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#another-example"><i class="fa fa-check"></i><b>26.3</b> Another Example</a></li>
</ul></li>
<li class="part"><span><b>VIII Reference</b></span></li>
<li class="chapter" data-level="27" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>27</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2023 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="spectral-clustering" class="section level1 hasAnchor" number="25">
<h1><span class="header-section-number">Chapter 25</span> Spectral Clustering<a href="spectral-clustering.html#spectral-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Spectral clustering aims at clustering observations based on their proximity information. It essentially consists of two steps. The first step is a feature embedding, or dimension reduction. We first construct the graph Laplacian <span class="math inline">\(\mathbf{L}\)</span> (or normalized version), which represent the proximity information, and perform eigen-decomposition of the matrix. This allows us to use a lower dimensional matrix to represent the proximity information of the original data. Once we have the low-dimensional data, we can perform the regular clustering algorithm, i.e., <span class="math inline">\(k\)</span>-means on the new dataset.</p>
<div id="an-example" class="section level2 hasAnchor" number="25.1">
<h2><span class="header-section-number">25.1</span> An Example<a href="spectral-clustering.html#an-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Letâ€™s look at an example that regular <span class="math inline">\(k\)</span>-means would fail.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="spectral-clustering.html#cb218-1" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb218-2"><a href="spectral-clustering.html#cb218-2" tabindex="-1"></a></span>
<span id="cb218-3"><a href="spectral-clustering.html#cb218-3" tabindex="-1"></a>  n <span class="ot">=</span> <span class="dv">200</span></span>
<span id="cb218-4"><a href="spectral-clustering.html#cb218-4" tabindex="-1"></a>  </span>
<span id="cb218-5"><a href="spectral-clustering.html#cb218-5" tabindex="-1"></a>  r <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, n), <span class="fu">rep</span>(<span class="dv">2</span>, n), <span class="fu">rep</span>(<span class="dv">3</span>, n)) <span class="sc">+</span> <span class="fu">runif</span>(n<span class="sc">*</span><span class="dv">3</span>, <span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>)</span>
<span id="cb218-6"><a href="spectral-clustering.html#cb218-6" tabindex="-1"></a>  theta <span class="ot">=</span> <span class="fu">runif</span>(n) <span class="sc">*</span> <span class="dv">2</span> <span class="sc">*</span> pi</span>
<span id="cb218-7"><a href="spectral-clustering.html#cb218-7" tabindex="-1"></a></span>
<span id="cb218-8"><a href="spectral-clustering.html#cb218-8" tabindex="-1"></a>  x1 <span class="ot">=</span> r <span class="sc">*</span> <span class="fu">cos</span>(theta)</span>
<span id="cb218-9"><a href="spectral-clustering.html#cb218-9" tabindex="-1"></a>  x2 <span class="ot">=</span> r <span class="sc">*</span> <span class="fu">sin</span>(theta)</span>
<span id="cb218-10"><a href="spectral-clustering.html#cb218-10" tabindex="-1"></a>  X <span class="ot">=</span> <span class="fu">cbind</span>(x1, x2)</span>
<span id="cb218-11"><a href="spectral-clustering.html#cb218-11" tabindex="-1"></a>  </span>
<span id="cb218-12"><a href="spectral-clustering.html#cb218-12" tabindex="-1"></a>  <span class="fu">plot</span>(X)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-338-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="spectral-clustering.html#cb219-1" tabindex="-1"></a></span>
<span id="cb219-2"><a href="spectral-clustering.html#cb219-2" tabindex="-1"></a>  kmeanfit <span class="ot">=</span> <span class="fu">kmeans</span>(X, <span class="at">centers =</span> <span class="dv">3</span>)</span>
<span id="cb219-3"><a href="spectral-clustering.html#cb219-3" tabindex="-1"></a>  </span>
<span id="cb219-4"><a href="spectral-clustering.html#cb219-4" tabindex="-1"></a>  <span class="fu">plot</span>(X, <span class="at">col =</span> kmeanfit<span class="sc">$</span>cluster <span class="sc">+</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-338-2.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Since <span class="math inline">\(k\)</span>-means use Euclidean distance, it is not appropriate for such problems.</p>
</div>
<div id="adjacency-matrix" class="section level2 hasAnchor" number="25.2">
<h2><span class="header-section-number">25.2</span> Adjacency Matrix<a href="spectral-clustering.html#adjacency-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Maybe we should use a nonlinear way to describe the distance/closeness between subjects. For example, letâ€™s define two sample to be close if they are within the k-nearest neighbors of each other. We use <span class="math inline">\(k = 10\)</span>, and create an adjacency matrix <span class="math inline">\(\mathbf{W}\)</span>.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="spectral-clustering.html#cb220-1" tabindex="-1"></a>  <span class="fu">library</span>(FNN)</span>
<span id="cb220-2"><a href="spectral-clustering.html#cb220-2" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb220-3"><a href="spectral-clustering.html#cb220-3" tabindex="-1"></a><span class="do">## Attaching package: &#39;FNN&#39;</span></span>
<span id="cb220-4"><a href="spectral-clustering.html#cb220-4" tabindex="-1"></a><span class="do">## The following objects are masked from &#39;package:class&#39;:</span></span>
<span id="cb220-5"><a href="spectral-clustering.html#cb220-5" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb220-6"><a href="spectral-clustering.html#cb220-6" tabindex="-1"></a><span class="do">##     knn, knn.cv</span></span>
<span id="cb220-7"><a href="spectral-clustering.html#cb220-7" tabindex="-1"></a>  </span>
<span id="cb220-8"><a href="spectral-clustering.html#cb220-8" tabindex="-1"></a>  W <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">3</span><span class="sc">*</span>n, <span class="dv">3</span><span class="sc">*</span>n)</span>
<span id="cb220-9"><a href="spectral-clustering.html#cb220-9" tabindex="-1"></a>  </span>
<span id="cb220-10"><a href="spectral-clustering.html#cb220-10" tabindex="-1"></a>  <span class="co"># get neighbor index for each observation</span></span>
<span id="cb220-11"><a href="spectral-clustering.html#cb220-11" tabindex="-1"></a>  nn <span class="ot">=</span> <span class="fu">get.knn</span>(X, <span class="at">k=</span><span class="dv">10</span>)</span>
<span id="cb220-12"><a href="spectral-clustering.html#cb220-12" tabindex="-1"></a></span>
<span id="cb220-13"><a href="spectral-clustering.html#cb220-13" tabindex="-1"></a>  <span class="co"># write into W</span></span>
<span id="cb220-14"><a href="spectral-clustering.html#cb220-14" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(<span class="dv">3</span><span class="sc">*</span>n))</span>
<span id="cb220-15"><a href="spectral-clustering.html#cb220-15" tabindex="-1"></a>    W[i, nn<span class="sc">$</span>nn.index[i, ]] <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb220-16"><a href="spectral-clustering.html#cb220-16" tabindex="-1"></a>  </span>
<span id="cb220-17"><a href="spectral-clustering.html#cb220-17" tabindex="-1"></a>  <span class="co"># W is not necessary symmetric</span></span>
<span id="cb220-18"><a href="spectral-clustering.html#cb220-18" tabindex="-1"></a>  W <span class="ot">=</span> <span class="fl">0.5</span><span class="sc">*</span>(W <span class="sc">+</span> <span class="fu">t</span>(W))</span>
<span id="cb220-19"><a href="spectral-clustering.html#cb220-19" tabindex="-1"></a>  <span class="co"># we may also use</span></span>
<span id="cb220-20"><a href="spectral-clustering.html#cb220-20" tabindex="-1"></a>  <span class="co"># W = pmax(W, t(W))</span></span></code></pre></div>
<p>Letâ€™s use a heatmap to display what the adjacency information look like. Please note that our data are ordered with clusters 1, 2 and 3.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="spectral-clustering.html#cb221-1" tabindex="-1"></a>  <span class="co"># plot the adjacency matrix</span></span>
<span id="cb221-2"><a href="spectral-clustering.html#cb221-2" tabindex="-1"></a>  <span class="fu">heatmap</span>(W, <span class="at">Rowv =</span> <span class="cn">NA</span>, <span class="at">Colv=</span><span class="cn">NA</span>, <span class="at">symm =</span> <span class="cn">TRUE</span>, <span class="at">revC =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-340-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="laplacian-matrix" class="section level2 hasAnchor" number="25.3">
<h2><span class="header-section-number">25.3</span> Laplacian Matrix<a href="spectral-clustering.html#laplacian-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The next step is to calculate the Laplacian</p>
<p><span class="math display">\[\mathbf{L} = \mathbf{D} - \mathbf{W},\]</span>
where <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix with its elements equation to the row sums (or column sums) of <span class="math inline">\(\mathbf{W}\)</span>. We will then perform eigen-decomposition on <span class="math inline">\(\mathbf{L}\)</span> to extract/define some underlying features.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="spectral-clustering.html#cb222-1" tabindex="-1"></a>  <span class="co"># compute the degree of each vertex</span></span>
<span id="cb222-2"><a href="spectral-clustering.html#cb222-2" tabindex="-1"></a>  d <span class="ot">=</span> <span class="fu">colSums</span>(W)</span>
<span id="cb222-3"><a href="spectral-clustering.html#cb222-3" tabindex="-1"></a>  </span>
<span id="cb222-4"><a href="spectral-clustering.html#cb222-4" tabindex="-1"></a>  <span class="co"># the laplacian matrix</span></span>
<span id="cb222-5"><a href="spectral-clustering.html#cb222-5" tabindex="-1"></a>  L <span class="ot">=</span> <span class="fu">diag</span>(d) <span class="sc">-</span> W</span></code></pre></div>
</div>
<div id="derivation-of-the-feature-embedding" class="section level2 hasAnchor" number="25.4">
<h2><span class="header-section-number">25.4</span> Derivation of the Feature Embedding<a href="spectral-clustering.html#derivation-of-the-feature-embedding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Assuming that we want to create artificial embedded features that reflects the underlying structure of the data, especially the graph structure. This can be done through an eigen-decomposition of the Laplacian matrix. To see this, letâ€™s assume that the new artificial embedded feature would assign a value <span class="math inline">\(f_i\)</span> to each observation <span class="math inline">\(i\)</span>. Hence, we want <span class="math inline">\(f_i\)</span> to be close to <span class="math inline">\(f_j\)</span> if <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are close in the graph, i.e., when <span class="math inline">\(w_{ij}\)</span> is large. This can be achieved by</p>
<p><span class="math display">\[\underset{f}{\min} \sum_{i,j} w_{ij} (f_i - f_j)^2.\]</span>
To solve this problem, we start with the following matrix form</p>
<p><span class="math display">\[\begin{align*}
f^\text{T}\mathbf{L} f &amp;= f^\text{T}\mathbf{D} f - f^\text{T}\mathbf{W} f \\
&amp;= \sum_{i} d_{i} f_i^2 - \sum_{i,j} w_{ij} f_i f_j \\
&amp;= \frac{1}{2} \bigg\{ \sum_{ij} w_{ij} f_i^2 - 2 \sum_{i,j} \mathbf{w}_{ij} f_i f_j + \sum_{ij} w_{ij} f_j^2 \bigg\}\\
&amp;= \frac{1}{2} \sum_{ij} w_{ij} (f_i - f_j)^2.
\end{align*}\]</span></p>
<p>Hence, finding the embedded feature simply becomes an eigen-decomposition problem, i.e., getting the smallest eigen-values of <span class="math inline">\(\mathbf{L}\)</span>. However, it should be noted that when all <span class="math inline">\(f_i\)</span>â€™s are identical, <span class="math inline">\(f^\text{T}\mathbf{L} f\)</span> would be zero. Hence, we should be using the second smallest eigen-values.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="spectral-clustering.html#cb223-1" tabindex="-1"></a>  <span class="co"># eigen-decomposition</span></span>
<span id="cb223-2"><a href="spectral-clustering.html#cb223-2" tabindex="-1"></a>  f <span class="ot">=</span> <span class="fu">eigen</span>(L, <span class="at">symmetric =</span> <span class="cn">TRUE</span>)</span>
<span id="cb223-3"><a href="spectral-clustering.html#cb223-3" tabindex="-1"></a>  </span>
<span id="cb223-4"><a href="spectral-clustering.html#cb223-4" tabindex="-1"></a>  <span class="co"># plot the smallest eigen-values </span></span>
<span id="cb223-5"><a href="spectral-clustering.html#cb223-5" tabindex="-1"></a>  <span class="co"># there are three zero eigen-values, why?</span></span>
<span id="cb223-6"><a href="spectral-clustering.html#cb223-6" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="fu">rev</span>(f<span class="sc">$</span>values)[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, </span>
<span id="cb223-7"><a href="spectral-clustering.html#cb223-7" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;red&quot;</span>, <span class="dv">3</span>), <span class="fu">rep</span>(<span class="st">&quot;blue&quot;</span>, <span class="dv">17</span>)))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-342-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="feature-embedding" class="section level2 hasAnchor" number="25.5">
<h2><span class="header-section-number">25.5</span> Feature Embedding<a href="spectral-clustering.html#feature-embedding" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In fact the smallest eigen-value will always be zero. However, we can use the eigen-vectors associated with the second and third smallest eigen-values. They define some feature embedding or dimension reduction to represent the original data. Since we know the underlying model, two dimensions are enough (to separate three clusters). However, based on the eigen-value plot, there is a big gap between the third and the fourth one. Hence, we only need three. Further removing the smallest one, only two are needed.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="spectral-clustering.html#cb224-1" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb224-2"><a href="spectral-clustering.html#cb224-2" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="fu">length</span>(f<span class="sc">$</span>values)<span class="sc">-</span><span class="dv">1</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>))</span>
<span id="cb224-3"><a href="spectral-clustering.html#cb224-3" tabindex="-1"></a>  <span class="fu">plot</span>(f<span class="sc">$</span>vectors[, <span class="fu">length</span>(f<span class="sc">$</span>values)<span class="sc">-</span><span class="dv">2</span>], <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;eigen-values&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>))</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-343-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="clustering-with-embedded-features" class="section level2 hasAnchor" number="25.6">
<h2><span class="header-section-number">25.6</span> Clustering with Embedded Features<a href="spectral-clustering.html#clustering-with-embedded-features" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can then perform <span class="math inline">\(k\)</span>-means on these two new features. And it will give us the correct clustering.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="spectral-clustering.html#cb225-1" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb225-2"><a href="spectral-clustering.html#cb225-2" tabindex="-1"></a>  scfit <span class="ot">=</span> <span class="fu">kmeans</span>(f<span class="sc">$</span>vectors[, (<span class="fu">length</span>(f<span class="sc">$</span>values)<span class="sc">-</span><span class="dv">2</span>) <span class="sc">:</span> (<span class="fu">length</span>(f<span class="sc">$</span>values)<span class="sc">-</span><span class="dv">1</span>) ], <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">20</span>)</span>
<span id="cb225-3"><a href="spectral-clustering.html#cb225-3" tabindex="-1"></a>  <span class="fu">plot</span>(X, <span class="at">col =</span> scfit<span class="sc">$</span>cluster <span class="sc">+</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-344-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="normalized-graph-laplacian" class="section level2 hasAnchor" number="25.7">
<h2><span class="header-section-number">25.7</span> Normalized Graph Laplacian<a href="spectral-clustering.html#normalized-graph-laplacian" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are other choices of the Laplacian matrix. For example, a normalized graph Laplacian is defined as
<span class="math display">\[\mathbf{L}_\text{sym} = \mathbf{I} - \mathbf{D^{-1/2} \mathbf{W} D^{-1/2}}\]</span></p>
<p>For our problem, it achieves pretty much the same effect.</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="spectral-clustering.html#cb226-1" tabindex="-1"></a>  <span class="co"># the normed laplacian matrix</span></span>
<span id="cb226-2"><a href="spectral-clustering.html#cb226-2" tabindex="-1"></a>  L <span class="ot">=</span> <span class="fu">diag</span>(<span class="fu">nrow</span>(W)) <span class="sc">-</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(d)) <span class="sc">%*%</span> W <span class="sc">%*%</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(d))</span>
<span id="cb226-3"><a href="spectral-clustering.html#cb226-3" tabindex="-1"></a>  </span>
<span id="cb226-4"><a href="spectral-clustering.html#cb226-4" tabindex="-1"></a>  <span class="co"># eigen-decomposition</span></span>
<span id="cb226-5"><a href="spectral-clustering.html#cb226-5" tabindex="-1"></a>  f <span class="ot">=</span> <span class="fu">eigen</span>(L, <span class="at">symmetric =</span> <span class="cn">TRUE</span>)</span>
<span id="cb226-6"><a href="spectral-clustering.html#cb226-6" tabindex="-1"></a>  </span>
<span id="cb226-7"><a href="spectral-clustering.html#cb226-7" tabindex="-1"></a>  <span class="co"># perform clustering</span></span>
<span id="cb226-8"><a href="spectral-clustering.html#cb226-8" tabindex="-1"></a>  scfit <span class="ot">=</span> <span class="fu">kmeans</span>(f<span class="sc">$</span>vectors[, (<span class="fu">length</span>(f<span class="sc">$</span>values)<span class="sc">-</span><span class="dv">2</span>) <span class="sc">:</span> (<span class="fu">length</span>(f<span class="sc">$</span>values)<span class="sc">-</span><span class="dv">1</span>) ], <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">20</span>)</span>
<span id="cb226-9"><a href="spectral-clustering.html#cb226-9" tabindex="-1"></a>  <span class="fu">plot</span>(X, <span class="at">col =</span> scfit<span class="sc">$</span>cluster <span class="sc">+</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-345-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="using-a-different-adjacency-matrix" class="section level2 hasAnchor" number="25.8">
<h2><span class="header-section-number">25.8</span> Using a Different Adjacency Matrix<a href="spectral-clustering.html#using-a-different-adjacency-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Letâ€™s also try to use the Gaussian kernel to define the adjacency matrix. Here, tuning (the bandwidth parameter in the kernel function) becomes important for the performance.</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="spectral-clustering.html#cb227-1" tabindex="-1"></a>  <span class="co"># the Gaussian kernel</span></span>
<span id="cb227-2"><a href="spectral-clustering.html#cb227-2" tabindex="-1"></a>  W <span class="ot">=</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fu">as.matrix</span>(<span class="fu">dist</span>(X<span class="sc">/</span><span class="fl">0.2</span>))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb227-3"><a href="spectral-clustering.html#cb227-3" tabindex="-1"></a>  </span>
<span id="cb227-4"><a href="spectral-clustering.html#cb227-4" tabindex="-1"></a>  <span class="co"># view the adjacency matrix</span></span>
<span id="cb227-5"><a href="spectral-clustering.html#cb227-5" tabindex="-1"></a>  <span class="fu">heatmap</span>(W, <span class="at">Rowv =</span> <span class="cn">NA</span>, <span class="at">Colv=</span><span class="cn">NA</span>, <span class="at">symm =</span> <span class="cn">TRUE</span>, <span class="at">revC =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-346-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="spectral-clustering.html#cb228-1" tabindex="-1"></a>  </span>
<span id="cb228-2"><a href="spectral-clustering.html#cb228-2" tabindex="-1"></a>  <span class="co"># the normed laplacian matrix</span></span>
<span id="cb228-3"><a href="spectral-clustering.html#cb228-3" tabindex="-1"></a>  L <span class="ot">=</span> <span class="fu">diag</span>(<span class="fu">colSums</span>(W)) <span class="sc">-</span> W</span>
<span id="cb228-4"><a href="spectral-clustering.html#cb228-4" tabindex="-1"></a>  </span>
<span id="cb228-5"><a href="spectral-clustering.html#cb228-5" tabindex="-1"></a>  <span class="co"># eigen-decomposition</span></span>
<span id="cb228-6"><a href="spectral-clustering.html#cb228-6" tabindex="-1"></a>  f <span class="ot">=</span> <span class="fu">eigen</span>(L, <span class="at">symmetric =</span> <span class="cn">TRUE</span>)</span>
<span id="cb228-7"><a href="spectral-clustering.html#cb228-7" tabindex="-1"></a>  </span>
<span id="cb228-8"><a href="spectral-clustering.html#cb228-8" tabindex="-1"></a>  <span class="co"># perform clustering</span></span>
<span id="cb228-9"><a href="spectral-clustering.html#cb228-9" tabindex="-1"></a>  scfit <span class="ot">=</span> <span class="fu">kmeans</span>(f<span class="sc">$</span>vectors[, (<span class="fu">length</span>(f<span class="sc">$</span>values)<span class="sc">-</span><span class="dv">2</span>) <span class="sc">:</span> (<span class="fu">length</span>(f<span class="sc">$</span>values)<span class="sc">-</span><span class="dv">1</span>) ], <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">20</span>)</span>
<span id="cb228-10"><a href="spectral-clustering.html#cb228-10" tabindex="-1"></a>  <span class="fu">plot</span>(X, <span class="at">col =</span> scfit<span class="sc">$</span>cluster <span class="sc">+</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-346-2.png" width="45%" style="display: block; margin: auto;" /></p>

<div style="display:none;">
<!-- Conflict \def\bf{\mathbf{f}} -->
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="self-organizing-map.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="uniform-manifold-approximation-and-projection.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "sepia",
    "family": "serif",
    "size": 1
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
