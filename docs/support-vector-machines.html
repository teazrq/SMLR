<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 Support Vector Machines | Statistical Machine Learning with R</title>
  <meta name="description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 Support Vector Machines | Statistical Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Support Vector Machines | Statistical Machine Learning with R" />
  
  <meta name="twitter:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2025-10-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="kernel-ridge-regression.html"/>
<link rel="next" href="the-representer-theorem.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visual-studio-code.html"><a href="visual-studio-code.html"><i class="fa fa-check"></i><b>3</b> Visual Studio Code</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visual-studio-code.html"><a href="visual-studio-code.html#basics-and-resources-1"><i class="fa fa-check"></i><b>3.1</b> Basics and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linear-regression"><i class="fa fa-check"></i><b>4.2</b> Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#matrix-inversion"><i class="fa fa-check"></i><b>4.3</b> Matrix Inversion</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linearalgebra-SM"><i class="fa fa-check"></i><b>4.3.1</b> Rank-one Update</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#rank-k-update"><i class="fa fa-check"></i><b>4.3.2</b> Rank-<span class="math inline">\(k\)</span> Update</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#times-2-block-matrix-inversion"><i class="fa fa-check"></i><b>4.3.3</b> 2 <span class="math inline">\(\times\)</span> 2 Block Matrix Inversion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>5</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>5.1</b> Basic Concept</a></li>
<li class="chapter" data-level="5.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>5.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="5.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>5.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="5.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>5.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="5.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>5.5</b> Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>5.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>5.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="5.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>5.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>5.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>5.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>5.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>5.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>6</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>6.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>6.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>6.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>6.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>6.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>6.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>6.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>6.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>6.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>6.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>6.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>6.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>7</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>7.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="7.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>7.2</b> Ridge Penalty and the Reduced Variation</a></li>
<li class="chapter" data-level="7.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>7.3</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="7.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>7.4</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="7.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>7.5</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>7.5.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="7.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>7.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>7.6</b> Cross-validation</a></li>
<li class="chapter" data-level="7.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.7</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>7.7.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>7.8</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>7.8.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8</b> Lasso</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>8.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>8.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="8.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>8.3</b> The Solution Path</a></li>
<li class="chapter" data-level="8.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>8.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="8.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>8.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="8.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>8.6</b> Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>9</b> Spline</a>
<ul>
<li class="chapter" data-level="9.1" data-path="spline.html"><a href="spline.html#using-linear-models-for-nonlinear-trends"><i class="fa fa-check"></i><b>9.1</b> Using Linear models for Nonlinear Trends</a></li>
<li class="chapter" data-level="9.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>9.2</b> A Motivating Example and Polynomials</a></li>
<li class="chapter" data-level="9.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>9.3</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="9.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>9.4</b> Splines</a></li>
<li class="chapter" data-level="9.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>9.5</b> Spline Basis</a></li>
<li class="chapter" data-level="9.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>9.6</b> Natural Cubic Spline</a></li>
<li class="chapter" data-level="9.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>9.7</b> Smoothing Spline</a></li>
<li class="chapter" data-level="9.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>9.8</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="9.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>9.9</b> Extending Splines to Multiple Varibles</a></li>
</ul></li>
<li class="part"><span><b>III Linear Classification Models</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>10.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>10.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>10.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>10.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>11</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>11.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="11.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>11.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="11.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>11.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="11.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>11.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="11.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>11.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>11.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Models</b></span></li>
<li class="chapter" data-level="12" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>12</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>12.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="12.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>12.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="12.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>12.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="12.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>12.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="12.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#degrees-of-freedom-1"><i class="fa fa-check"></i><b>12.6</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="12.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>12.7</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="12.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>12.8</b> Distance Measures</a></li>
<li class="chapter" data-level="12.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>12.9</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="12.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>12.10</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="12.11" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>12.11</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>13</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>13.1</b> KNN vs. Kernel</a></li>
<li class="chapter" data-level="13.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>13.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="13.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#expectation-of-the-parzen-estimator"><i class="fa fa-check"></i><b>13.3</b> Expectation of the Parzen estimator</a></li>
<li class="chapter" data-level="13.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>13.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>13.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>13.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="13.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>13.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="13.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>13.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="13.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>13.8</b> R Implementations</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>14</b> Nonparemetric Estimation Rates</a>
<ul>
<li class="chapter" data-level="14.1" data-path="nonpara.html"><a href="nonpara.html#kernel-density-estimation"><i class="fa fa-check"></i><b>14.1</b> Kernel Density Estimation</a></li>
<li class="chapter" data-level="14.2" data-path="nonpara.html"><a href="nonpara.html#the-effect-of-smoothness"><i class="fa fa-check"></i><b>14.2</b> The Effect of Smoothness</a></li>
<li class="chapter" data-level="14.3" data-path="nonpara.html"><a href="nonpara.html#the-effect-of-dimensionality"><i class="fa fa-check"></i><b>14.3</b> The Effect of Dimensionality</a></li>
<li class="chapter" data-level="14.4" data-path="nonpara.html"><a href="nonpara.html#nadaraya-watson-regression-estimator"><i class="fa fa-check"></i><b>14.4</b> Nadaraya-Watson Regression Estimator</a></li>
</ul></li>
<li class="part"><span><b>V Kernel Machines</b></span></li>
<li class="chapter" data-level="15" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>15</b> Reproducing Kernel Hilbert Space</a>
<ul>
<li class="chapter" data-level="15.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-motivation"><i class="fa fa-check"></i><b>15.1</b> The Motivation</a></li>
<li class="chapter" data-level="15.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#hilbert-space-preliminaries"><i class="fa fa-check"></i><b>15.2</b> Hilbert Space Preliminaries</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-space-of-square-integrable-functions"><i class="fa fa-check"></i><b>15.2.1</b> The Space of Square-Integrable Functions</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-kernel-function"><i class="fa fa-check"></i><b>15.3</b> A Kernel Function</a></li>
<li class="chapter" data-level="15.4" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-space-of-functions"><i class="fa fa-check"></i><b>15.4</b> A Space of Functions</a></li>
<li class="chapter" data-level="15.5" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-inner-product"><i class="fa fa-check"></i><b>15.5</b> The Inner Product</a></li>
<li class="chapter" data-level="15.6" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-rkhs"><i class="fa fa-check"></i><b>15.6</b> The RKHS</a></li>
<li class="chapter" data-level="15.7" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-reproducing-property"><i class="fa fa-check"></i><b>15.7</b> The Reproducing Property</a></li>
<li class="chapter" data-level="15.8" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#smoothness"><i class="fa fa-check"></i><b>15.8</b> Smoothness</a></li>
<li class="chapter" data-level="15.9" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-moorearonszajn-theorem"><i class="fa fa-check"></i><b>15.9</b> The Moore–Aronszajn Theorem</a></li>
<li class="chapter" data-level="15.10" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#examples"><i class="fa fa-check"></i><b>15.10</b> Examples</a>
<ul>
<li class="chapter" data-level="15.10.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#brownian-motion-kernel"><i class="fa fa-check"></i><b>15.10.1</b> Brownian Motion Kernel</a></li>
<li class="chapter" data-level="15.10.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#non-positive-definite-kernel"><i class="fa fa-check"></i><b>15.10.2</b> Non-positive Definite Kernel</a></li>
<li class="chapter" data-level="15.10.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#defining-new-kernels"><i class="fa fa-check"></i><b>15.10.3</b> Defining New Kernels</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>16</b> Kernel Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#linear-regression-as-a-constraint-optimization"><i class="fa fa-check"></i><b>16.1</b> Linear Regression as a Constraint Optimization</a></li>
<li class="chapter" data-level="16.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#the-kernel-ridge-regression"><i class="fa fa-check"></i><b>16.2</b> The Kernel Ridge Regression</a></li>
<li class="chapter" data-level="16.3" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#ridge-regression-as-a-linear-kernel-model"><i class="fa fa-check"></i><b>16.3</b> Ridge Regression as a Linear Kernel Model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>17</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="17.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>17.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="17.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>17.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>17.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>17.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="17.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>17.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="17.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>17.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="17.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>17.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="17.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>17.7</b> SVM as a Penalized Model</a></li>
<li class="chapter" data-level="17.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>17.8</b> Kernel and Feature Maps: Another Example</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html"><i class="fa fa-check"></i><b>18</b> The Representer Theorem</a>
<ul>
<li class="chapter" data-level="18.1" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#the-representer-theorem-1"><i class="fa fa-check"></i><b>18.1</b> The Representer Theorem</a></li>
<li class="chapter" data-level="18.2" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#notes-on-application"><i class="fa fa-check"></i><b>18.2</b> Notes on Application</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="support-vector-regression.html"><a href="support-vector-regression.html"><i class="fa fa-check"></i><b>19</b> Support Vector Regression</a>
<ul>
<li class="chapter" data-level="19.1" data-path="support-vector-regression.html"><a href="support-vector-regression.html#the-epsilon-insensitive-loss"><i class="fa fa-check"></i><b>19.1</b> The <span class="math inline">\(\epsilon\)</span>-insensitive Loss</a></li>
<li class="chapter" data-level="19.2" data-path="support-vector-regression.html"><a href="support-vector-regression.html#primal-and-dual-formulation-of-svr"><i class="fa fa-check"></i><b>19.2</b> Primal and Dual Formulation of SVR</a></li>
<li class="chapter" data-level="19.3" data-path="support-vector-regression.html"><a href="support-vector-regression.html#penalized-svr-with-rkhs"><i class="fa fa-check"></i><b>19.3</b> Penalized SVR with RKHS</a></li>
</ul></li>
<li class="part"><span><b>VI Trees and Ensembles</b></span></li>
<li class="chapter" data-level="20" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>20</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="20.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>20.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="20.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>20.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="20.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>20.3</b> Regression Trees</a></li>
<li class="chapter" data-level="20.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>20.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="20.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>20.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>21</b> Random Forests</a>
<ul>
<li class="chapter" data-level="21.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>21.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="21.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>21.2</b> Random Forests</a></li>
<li class="chapter" data-level="21.3" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forests"><i class="fa fa-check"></i><b>21.3</b> Kernel view of Random Forests</a></li>
<li class="chapter" data-level="21.4" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>21.4</b> Variable Importance</a></li>
<li class="chapter" data-level="21.5" data-path="random-forests.html"><a href="random-forests.html#adaptiveness-of-random-forest-kernel"><i class="fa fa-check"></i><b>21.5</b> Adaptiveness of Random Forest Kernel</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="adaboost.html"><a href="adaboost.html"><i class="fa fa-check"></i><b>22</b> AdaBoost</a>
<ul>
<li class="chapter" data-level="22.1" data-path="adaboost.html"><a href="adaboost.html#the-algorithm"><i class="fa fa-check"></i><b>22.1</b> The Algorithm</a></li>
<li class="chapter" data-level="22.2" data-path="adaboost.html"><a href="adaboost.html#training-error-bound"><i class="fa fa-check"></i><b>22.2</b> Training Error Bound</a></li>
<li class="chapter" data-level="22.3" data-path="adaboost.html"><a href="adaboost.html#the-stagewise-additive-model-and-probability-calibration"><i class="fa fa-check"></i><b>22.3</b> The Stagewise Additive Model and Probability Calibration</a></li>
<li class="chapter" data-level="22.4" data-path="adaboost.html"><a href="adaboost.html#tuning-the-number-of-trees"><i class="fa fa-check"></i><b>22.4</b> Tuning the Number of Trees</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html"><i class="fa fa-check"></i><b>23</b> Gradient Boosting Machines</a>
<ul>
<li class="chapter" data-level="23.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#motivation-lasso-as-boosting"><i class="fa fa-check"></i><b>23.1</b> Motivation: Lasso as Boosting</a></li>
<li class="chapter" data-level="23.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gradient-boosting"><i class="fa fa-check"></i><b>23.2</b> Gradient Boosting</a></li>
<li class="chapter" data-level="23.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gradient-boosting-with-general-loss"><i class="fa fa-check"></i><b>23.3</b> Gradient Boosting with General Loss</a></li>
<li class="chapter" data-level="23.4" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#logistic-link"><i class="fa fa-check"></i><b>23.4</b> Logistic Link</a></li>
<li class="chapter" data-level="23.5" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#xgboost"><i class="fa fa-check"></i><b>23.5</b> xgboost</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Learning</b></span></li>
<li class="chapter" data-level="24" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>24</b> K-Means</a>
<ul>
<li class="chapter" data-level="24.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>24.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="24.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>24.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="24.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>24.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>25</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="25.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>25.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="25.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>25.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="25.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>25.3</b> Example 2: RNA Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>26</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="26.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>26.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="26.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>26.1.1</b> Note: Scaling</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>26.2</b> Example 1: <code>iris</code> Data</a></li>
<li class="chapter" data-level="26.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>26.3</b> Example 2: Handwritten Digits</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>27</b> Self-Organizing Map</a>
<ul>
<li class="chapter" data-level="27.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>27.1</b> Basic Concepts</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>28</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="28.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#an-example"><i class="fa fa-check"></i><b>28.1</b> An Example</a></li>
<li class="chapter" data-level="28.2" data-path="spectral-clustering.html"><a href="spectral-clustering.html#adjacency-matrix"><i class="fa fa-check"></i><b>28.2</b> Adjacency Matrix</a></li>
<li class="chapter" data-level="28.3" data-path="spectral-clustering.html"><a href="spectral-clustering.html#laplacian-matrix"><i class="fa fa-check"></i><b>28.3</b> Laplacian Matrix</a></li>
<li class="chapter" data-level="28.4" data-path="spectral-clustering.html"><a href="spectral-clustering.html#derivation-of-the-feature-embedding"><i class="fa fa-check"></i><b>28.4</b> Derivation of the Feature Embedding</a></li>
<li class="chapter" data-level="28.5" data-path="spectral-clustering.html"><a href="spectral-clustering.html#feature-embedding"><i class="fa fa-check"></i><b>28.5</b> Feature Embedding</a></li>
<li class="chapter" data-level="28.6" data-path="spectral-clustering.html"><a href="spectral-clustering.html#clustering-with-embedded-features"><i class="fa fa-check"></i><b>28.6</b> Clustering with Embedded Features</a></li>
<li class="chapter" data-level="28.7" data-path="spectral-clustering.html"><a href="spectral-clustering.html#normalized-graph-laplacian"><i class="fa fa-check"></i><b>28.7</b> Normalized Graph Laplacian</a></li>
<li class="chapter" data-level="28.8" data-path="spectral-clustering.html"><a href="spectral-clustering.html#using-a-different-adjacency-matrix"><i class="fa fa-check"></i><b>28.8</b> Using a Different Adjacency Matrix</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html"><i class="fa fa-check"></i><b>29</b> Uniform Manifold Approximation and Projection</a>
<ul>
<li class="chapter" data-level="29.1" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#an-example-1"><i class="fa fa-check"></i><b>29.1</b> An Example</a></li>
<li class="chapter" data-level="29.2" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#tuning"><i class="fa fa-check"></i><b>29.2</b> Tuning</a></li>
<li class="chapter" data-level="29.3" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#another-example"><i class="fa fa-check"></i><b>29.3</b> Another Example</a></li>
</ul></li>
<li class="part"><span><b>VIII Reference</b></span></li>
<li class="chapter" data-level="30" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>30</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2023 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="support-vector-machines" class="section level1 hasAnchor" number="17">
<h1><span class="header-section-number">Chapter 17</span> Support Vector Machines<a href="support-vector-machines.html#support-vector-machines" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Support Vector Machine (SVM) is one of the most popular classification models. The original SVM was proposed by Vladimir Vapnik and Alexey Chervonenkis in 1963. Then two important improvements was developed in the 90’s: the soft margin version <span class="citation">(<a href="#ref-cortes1995support">Cortes and Vapnik 1995</a>)</span> and the nonlinear SVM using the kernel trick <span class="citation">(<a href="#ref-boser1992training">Boser, Guyon, and Vapnik 1992</a>)</span>. We will start with the hard margin version, and then introduce all other techniques. This chapter utilizes the kernel trick we introduced in the kernel ridge regression, however, the formulation of SVM can be independent of the results in RKHS if we only consider the linear SVM.</p>
<div id="maximum-margin-classifier" class="section level2 hasAnchor" number="17.1">
<h2><span class="header-section-number">17.1</span> Maximum-margin Classifier<a href="support-vector-machines.html#maximum-margin-classifier" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This is the original SVM proposed in 1963. It shares similarities with the perception algorithm, but in certain sense is a stable version. We observe the training data <span class="math inline">\({\cal D}_n = \{\mathbf{x}_i, y_i\}_{i=1}^n\)</span>, where we code <span class="math inline">\(y_i\)</span> as a binary outcome from <span class="math inline">\(\{-1, 1\}\)</span>. The advantages of using this coding instead of <span class="math inline">\(0/1\)</span> will be seen later. The goal is to find a linear classification rule <span class="math inline">\(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol{\beta}\)</span> such that the classification rule is the sign of <span class="math inline">\(f(\mathbf{x})\)</span>:</p>
<p><span class="math display">\[
\hat{y} =
\begin{cases}
        +1, \quad \text{if} \quad f(\mathbf{x}) &gt; 0\\
        -1, \quad \text{if} \quad f(\mathbf{x}) &lt; 0
\end{cases}
\]</span>
Hence, a correct classification would satisfy <span class="math inline">\(y_i f(\mathbf{x}_i) &gt; 0\)</span>. Let’s look at the following example of data from two classes.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="support-vector-machines.html#cb141-1" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb141-2"><a href="support-vector-machines.html#cb141-2" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb141-3"><a href="support-vector-machines.html#cb141-3" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb141-4"><a href="support-vector-machines.html#cb141-4" tabindex="-1"></a>    </span>
<span id="cb141-5"><a href="support-vector-machines.html#cb141-5" tabindex="-1"></a>    <span class="co"># Generate positive and negative examples</span></span>
<span id="cb141-6"><a href="support-vector-machines.html#cb141-6" tabindex="-1"></a>    xneg <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="dv">1</span>),n,p)</span>
<span id="cb141-7"><a href="support-vector-machines.html#cb141-7" tabindex="-1"></a>    xpos <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p,<span class="at">mean=</span><span class="dv">3</span>,<span class="at">sd=</span><span class="dv">1</span>),n,p)</span>
<span id="cb141-8"><a href="support-vector-machines.html#cb141-8" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(xpos,xneg)</span>
<span id="cb141-9"><a href="support-vector-machines.html#cb141-9" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">as.factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>,n))))</span>
<span id="cb141-10"><a href="support-vector-machines.html#cb141-10" tabindex="-1"></a>    </span>
<span id="cb141-11"><a href="support-vector-machines.html#cb141-11" tabindex="-1"></a>    <span class="co"># plot </span></span>
<span id="cb141-12"><a href="support-vector-machines.html#cb141-12" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">&gt;</span><span class="dv">0</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb141-13"><a href="support-vector-machines.html#cb141-13" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;X1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;X2&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb141-14"><a href="support-vector-machines.html#cb141-14" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Positive&quot;</span>, <span class="st">&quot;Negative&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>),</span>
<span id="cb141-15"><a href="support-vector-machines.html#cb141-15" tabindex="-1"></a>           <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>), <span class="at">text.col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-216-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>There are many linear lines that can perfectly separate the two classes. But which is better? The SVM defines this as the line that maximizes the margin, which can be seen in the following.</p>
<p>We use the <code>e1071</code> package to fit the SVM. There is a cost parameter <span class="math inline">\(C\)</span>, with default value 1. This parameter has a significant impact on non-separable problems. However, for this <strong>separable case</strong>, we should set this to be a very large value, meaning that the cost for having a wrong classification is very large. We also need to specify the <code>linear</code> kernel.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="support-vector-machines.html#cb142-1" tabindex="-1"></a>    <span class="fu">library</span>(e1071)</span>
<span id="cb142-2"><a href="support-vector-machines.html#cb142-2" tabindex="-1"></a>    svm.fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">type=</span><span class="st">&#39;C-classification&#39;</span>, </span>
<span id="cb142-3"><a href="support-vector-machines.html#cb142-3" tabindex="-1"></a>                   <span class="at">kernel=</span><span class="st">&#39;linear&#39;</span>, <span class="at">scale=</span><span class="cn">FALSE</span>, <span class="at">cost =</span> <span class="dv">10000</span>)</span></code></pre></div>
<p>The following code can recover the fitted linear separation margin. Note here that the points on the margins are the ones with <span class="math inline">\(\alpha_i &gt; 0\)</span> (will be introduced later):</p>
<ul>
<li><code>coefs</code> provides the <span class="math inline">\(y_i \alpha_i\)</span> for the support vectors</li>
<li><code>SV</code> are the <span class="math inline">\(x_i\)</span> values correspond to the support vectors</li>
<li><code>rho</code> is negative <span class="math inline">\(\beta_0\)</span></li>
</ul>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="support-vector-machines.html#cb143-1" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="fu">t</span>(svm.fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm.fit<span class="sc">$</span>SV</span>
<span id="cb143-2"><a href="support-vector-machines.html#cb143-2" tabindex="-1"></a>    b0 <span class="ot">&lt;-</span> <span class="sc">-</span>svm.fit<span class="sc">$</span>rho</span>
<span id="cb143-3"><a href="support-vector-machines.html#cb143-3" tabindex="-1"></a>    </span>
<span id="cb143-4"><a href="support-vector-machines.html#cb143-4" tabindex="-1"></a>    <span class="co"># an alternative of b0 as the lecture note</span></span>
<span id="cb143-5"><a href="support-vector-machines.html#cb143-5" tabindex="-1"></a>    b0 <span class="ot">&lt;-</span> <span class="sc">-</span>(<span class="fu">max</span>(x[y <span class="sc">==</span> <span class="sc">-</span><span class="dv">1</span>, ] <span class="sc">%*%</span> <span class="fu">t</span>(b)) <span class="sc">+</span> <span class="fu">min</span>(x[y <span class="sc">==</span> <span class="dv">1</span>, ] <span class="sc">%*%</span> <span class="fu">t</span>(b)))<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb143-6"><a href="support-vector-machines.html#cb143-6" tabindex="-1"></a>    </span>
<span id="cb143-7"><a href="support-vector-machines.html#cb143-7" tabindex="-1"></a>    <span class="co"># plot on the data </span></span>
<span id="cb143-8"><a href="support-vector-machines.html#cb143-8" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">&gt;</span><span class="dv">0</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb143-9"><a href="support-vector-machines.html#cb143-9" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;X1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;X2&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb143-10"><a href="support-vector-machines.html#cb143-10" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Positive&quot;</span>,<span class="st">&quot;Negative&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>),</span>
<span id="cb143-11"><a href="support-vector-machines.html#cb143-11" tabindex="-1"></a>           <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>),<span class="at">text.col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb143-12"><a href="support-vector-machines.html#cb143-12" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> <span class="sc">-</span>b0<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb143-13"><a href="support-vector-machines.html#cb143-13" tabindex="-1"></a>    </span>
<span id="cb143-14"><a href="support-vector-machines.html#cb143-14" tabindex="-1"></a>    <span class="co"># mark the support vectors</span></span>
<span id="cb143-15"><a href="support-vector-machines.html#cb143-15" tabindex="-1"></a>    <span class="fu">points</span>(x[svm.fit<span class="sc">$</span>index, ], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">cex=</span><span class="dv">3</span>)</span>
<span id="cb143-16"><a href="support-vector-machines.html#cb143-16" tabindex="-1"></a>    </span>
<span id="cb143-17"><a href="support-vector-machines.html#cb143-17" tabindex="-1"></a>    <span class="co"># the two margin lines </span></span>
<span id="cb143-18"><a href="support-vector-machines.html#cb143-18" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="dv">-1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb143-19"><a href="support-vector-machines.html#cb143-19" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-218-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>As we can see, the separation line is trying to have the maximum distance from both classes. This is why it is called the <strong>Maximum-margin Classifier</strong>.</p>
</div>
<div id="linearly-separable-svm" class="section level2 hasAnchor" number="17.2">
<h2><span class="header-section-number">17.2</span> Linearly Separable SVM<a href="support-vector-machines.html#linearly-separable-svm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In linearly SVM, <span class="math inline">\(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol{\beta}\)</span>. When <span class="math inline">\(f(\mathbf{x}) = 0\)</span>, it corresponds to a hyperplane that separates the two classes:</p>
<p><span class="math display">\[\{ \mathbf{x}: \beta_0 + \mathbf{x}^\text{T} \boldsymbol \beta = 0 \}\]</span></p>
<p>Hence, for this separable case, all observations with <span class="math inline">\(y_i = 1\)</span> are on one side <span class="math inline">\(f(\mathbf{x}) &gt; 0\)</span>, and observations with <span class="math inline">\(y_i = -1\)</span> are on the other side.</p>
<center>
<img src="images/SVMdist.png" style="width:40.0%" />
</center>
<p>First, let’s calculate the <strong>distance from any point <span class="math inline">\(\mathbf{x}\)</span> to this hyperplane</strong>. We can first find a point <span class="math inline">\(\mathbf{x}_0\)</span> on the hyperplane, such that <span class="math inline">\(\mathbf{x}_0^\text{T}\boldsymbol{\beta}= - \beta_0\)</span>. By taking the difference between <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{x}_0\)</span>, and project this vector to the direction of <span class="math inline">\(\boldsymbol{\beta}\)</span>, we have that the distance from <span class="math inline">\(\mathbf{x}\)</span> to the hyperplane is the projection of <span class="math inline">\(\mathbf{x}- \mathbf{x}_0\)</span> onto the normed vector <span class="math inline">\(\frac{\boldsymbol{\beta}}{\lVert \boldsymbol{\beta}\lVert}\)</span>:</p>
<p><span class="math display">\[\begin{align}
&amp; \left \langle  \frac{\boldsymbol{\beta}}{\lVert \boldsymbol{\beta}\lVert}, \mathbf{x}- \mathbf{x}_0 \right \rangle \\
=&amp; \frac{1}{\lVert \boldsymbol{\beta}\lVert} (\mathbf{x}- \mathbf{x}_0)^\text{T}\boldsymbol{\beta}\\
=&amp; \frac{1}{\lVert \boldsymbol{\beta}\lVert} (\mathbf{x}^\text{T}\boldsymbol{\beta}+ \beta_0) \\
=&amp; \frac{1}{\lVert \boldsymbol{\beta}\lVert} f(\mathbf{x}) \\
\end{align}\]</span></p>
<p>Since the goal of SVM is to create the maximum margin, let’s denote this as <span class="math inline">\(M\)</span>. Then we want all observations to be lied on the correct side, with at least an margin <span class="math inline">\(M\)</span>. This means <span class="math inline">\(y_i (\mathbf{x}_i^\text{T}\boldsymbol{\beta}+ \beta_0) \geq M\)</span>. But the scale of <span class="math inline">\(\boldsymbol{\beta}\)</span> is also playing a role in calculating the margin. Hence, we will use the normed version. Then, the linearly separable SVM is to solve this constrained optimization problem:</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol{\beta}, \beta_0}{\text{max}} \quad &amp; M \\
\text{subject to} \quad &amp; \frac{1}{\lVert \boldsymbol{\beta}\lVert} y_i(\mathbf{x}^\text{T}\boldsymbol{\beta}+ \beta_0) \geq M, \,\, i = 1, \ldots, n.
\end{align}\]</span></p>
<p>Note that the scale of <span class="math inline">\(\boldsymbol{\beta}\)</span> can be arbitrary, let’s set it as <span class="math inline">\(\lVert \boldsymbol{\beta}\rVert = 1/M\)</span>. The maximization becomes minimization, and its equivalent to minimizing <span class="math inline">\(\frac{1}{2} \lVert \boldsymbol{\beta}\rVert^2\)</span>. Then we have the <strong>primal form</strong> of the SVM optimization problem.</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol{\beta}, \beta_0}{\text{min}} \quad &amp; \frac{1}{2} \lVert \boldsymbol{\beta}\rVert^2 \\
\text{subject to} \quad &amp; y_i(\mathbf{x}_i^\text{T}\boldsymbol{\beta}+ \beta_0) \geq 1, \,\, i = 1, \ldots, n.
\end{align}\]</span></p>
<div id="from-primal-to-dual" class="section level3 hasAnchor" number="17.2.1">
<h3><span class="header-section-number">17.2.1</span> From Primal to Dual<a href="support-vector-machines.html#from-primal-to-dual" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This is a general inequality constrained optimization problem.</p>
<p><span class="math display">\[\begin{align}
\text{min} \quad &amp; g(\boldsymbol{\theta}) \\
\text{subject to} \quad &amp; h_i(\boldsymbol{\theta}) \leq 0, \,\, i = 1, \ldots, n.
\end{align}\]</span></p>
<p>We can consider the corresponding Lagrangian (with all <span class="math inline">\(\alpha_i\)</span>’s positive):</p>
<p><span class="math display">\[{\cal L}(\boldsymbol{\theta}, \boldsymbol{\alpha}) = g(\boldsymbol{\theta}) + \sum_{i = 1}^n \alpha_i h_i(\boldsymbol{\theta})\]</span>
Then there can be two ways to optimize this. If we maximize <span class="math inline">\(\alpha_i\)</span>’s first, for any fixed <span class="math inline">\(\boldsymbol{\theta}\)</span>, then for any <span class="math inline">\(\boldsymbol{\theta}\)</span> that violates the constraint, i.e., <span class="math inline">\(h_i(\boldsymbol{\theta}) &gt; 0\)</span> for some <span class="math inline">\(i\)</span>, we can always choose an extremely large <span class="math inline">\(\alpha_i\)</span> so that <span class="math inline">\(\cal{L}(\boldsymbol{\theta}, \boldsymbol{\alpha})\)</span> is infinity. Hence the solution of this <strong>primal form</strong> must satisfy the constraint.</p>
<p><span class="math display">\[\underset{\boldsymbol{\theta}}{\min} \underset{\boldsymbol{\alpha}\succeq 0}{\max} {\cal L}(\boldsymbol{\theta}, \boldsymbol{\alpha})\]</span>
On the other hand, if we minimize <span class="math inline">\(\boldsymbol{\theta}\)</span> first, then maximize for <span class="math inline">\(\boldsymbol{\alpha}\)</span>, we have the <strong>dual form</strong>:</p>
<p><span class="math display">\[\underset{\boldsymbol{\alpha}\succeq 0}{\max} \underset{\boldsymbol{\theta}}{\min} {\cal L}(\boldsymbol{\theta}, \boldsymbol{\alpha})\]</span>
In general, the two are not the same:</p>
<p><span class="math display">\[\underbrace{\underset{\boldsymbol{\alpha}\succeq 0}{\max} \underset{\boldsymbol{\theta}}{\min} {\cal L}(\boldsymbol{\theta}, \boldsymbol{\alpha})}_{\text{duel}} \leq \underbrace{\underset{\boldsymbol{\theta}}{\min} \underset{\boldsymbol{\alpha}\succeq 0}{\max} {\cal L}(\boldsymbol{\theta}, \boldsymbol{\alpha})}_{\text{primal}}\]</span>
But a sufficient condition is that if both <span class="math inline">\(g\)</span> and <span class="math inline">\(h_i\)</span>’s are convex and also the constraints <span class="math inline">\(h_i\)</span>’s are feasible. We will use this technique to solve the SVM problem.</p>
<p>First, rewrite the problem as</p>
<p><span class="math display">\[\begin{align}
\text{min} \quad &amp; \frac{1}{2} \lVert \boldsymbol{\beta}\rVert^2 \\
\text{subject to} \quad &amp; - \{ y_i(\mathbf{x}_i^\text{T}\boldsymbol{\beta}+ \beta_0) - 1\} \leq 0, i = 1, \ldots, n.
\end{align}\]</span></p>
<p>Then the Lagrangian is</p>
<p><span class="math display">\[{\cal L}(\boldsymbol{\beta}, \beta_0, \boldsymbol{\alpha}) = \frac{1}{2} \lVert \boldsymbol{\beta}\rVert^2 - \sum_{i = 1}^n \alpha_i \big\{ y_i(\mathbf{x}_i^\text{T}\boldsymbol{\beta}+ \beta_0) - 1 \big\}\]</span>
To solve this using the dual form, we first find the optimizer of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\beta_0\)</span>. We take derivatives with respect to them:</p>
<p><span class="math display">\[\begin{align}
    \boldsymbol{\beta}- \sum_{i = 1}^n \alpha_i y_i \mathbf{x}_i^\text{T}=&amp;~ 0 \quad (\nabla_\boldsymbol{\beta}{\cal L}= 0 ) \\
    \sum_{i = 1}^n \alpha_i y_i =&amp;~ 0 \quad (\nabla_{\beta_0} {\cal L}= 0 )
\end{align}\]</span></p>
<p>Take these solution and plug them back into the Lagrangian, we have</p>
<p><span class="math display">\[{\cal L}(\boldsymbol{\beta}, \beta_0, \boldsymbol{\alpha}) = \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \mathbf{x}_i^\text{T}\mathbf{x}_j\]</span>
Hence, the dual optimization problem is</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol{\alpha}}{\max} \quad &amp; \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \mathbf{x}_i^\text{T}\mathbf{x}_j \nonumber \\
\text{subject to} \quad &amp; \alpha_i \geq 0, \,\, i = 1, \ldots, n. \nonumber \\
&amp; \sum_{i = 1}^n \alpha_i y_i = 0
\end{align}\]</span></p>
<p>Compared with the original primal form, this version has a trivial feasible solution with all <span class="math inline">\(\alpha_i\)</span>’s being 0. One can start from this solution to search for the optimizer while maintaining within the contained region. However, the primal form is difficult since there is no apparent way to satisfy the constraint.</p>
<p>After solving the dual form, we have all the <span class="math inline">\(\alpha_i\)</span> values. The ones with <span class="math inline">\(\alpha_i &gt; 0\)</span> are called the support vectors. Based on our previous analysis, <span class="math inline">\(\widehat{\boldsymbol{\beta}} = \sum_{i = 1}^n \alpha_i y_i x_i^T\)</span>, and we can also obtain <span class="math inline">\(\beta_0\)</span> by calculating the midpoint of two “closest” support vectors to the separating hyperplane:</p>
<p><span class="math display">\[\widehat{\beta}_0 = - \,\, \frac{\max_{i: y_i = -1} \mathbf{x}_i^\text{T}\widehat{\boldsymbol{\beta}} + \min_{i: y_i = 1} \mathbf{x}_i^\text{T}\widehat{\boldsymbol{\beta}} }{2}\]</span>
And the decision is <span class="math inline">\(\text{sign}(\mathbf{x}_i^\text{T}\widehat{\boldsymbol{\beta}} + \widehat{\beta}_0)\)</span>. An example has been demonstrated previously with the <code>e1071</code> package.</p>
</div>
</div>
<div id="linearly-non-separable-svm-with-slack-variables" class="section level2 hasAnchor" number="17.3">
<h2><span class="header-section-number">17.3</span> Linearly Non-separable SVM with Slack Variables<a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we cannot have a perfect separation of the two classes, the original SVM cannot find a solution. Hence, a slack was introduce to incorporate such observations:</p>
<p><span class="math display">\[y_i (\mathbf{x}_i^\text{T}\boldsymbol{\beta}+ \beta_0) \geq (1 - \xi_i)\]</span>
for a positive <span class="math inline">\(\xi\)</span>. Note that when <span class="math inline">\(\xi = 0\)</span>, the observation is lying at the correct side, with enough margin. When <span class="math inline">\(1 &gt; \xi &gt; 0\)</span>, the observation is lying at the correct side, but the margin is not sufficiently large. When <span class="math inline">\(\xi &gt; 1\)</span>, the observation is lying on the wrong side of the separation hyperplane.</p>
<center>
<img src="images/SVMslack.png" style="width:40.0%" />
</center>
<p>This new optimization problem can be formulated as</p>
<p><span class="math display">\[\begin{align}
\text{min} \quad &amp; \frac{1}{2}\lVert \boldsymbol{\beta}\rVert^2 + C \sum_{i=1}^n \xi_i \\
\text{subject to} \quad &amp; y_i (\mathbf{x}_i^\text{T}\boldsymbol{\beta}+ \beta_0) \geq (1 - \xi_i), \,\, i = 1, \ldots, n, \\
\text{and} \quad &amp; \xi_i \geq 0, \,\, i = 1, \ldots, n,
\end{align}\]</span></p>
<p>where <span class="math inline">\(C\)</span> is a tuning parameter that controls the emphasis on the slack variable. Large <span class="math inline">\(C\)</span> will be less tolerable on having positive slacks. We can again write the Lagrangian primal <span class="math inline">\({\cal L}(\boldsymbol{\beta}, \beta_0, \boldsymbol{\alpha}, \boldsymbol{\xi})\)</span> as</p>
<p><span class="math display">\[\frac{1}{2} \lVert \boldsymbol{\beta}\rVert^2 + C \sum_{i=1}^n \xi_i - \sum_{i = 1}^n \alpha_i \big\{ y_i(x_i^\text{T}\boldsymbol{\beta}+ \beta_0) - (1 - \xi_i) \big\} - \sum_{i = 1}^n \gamma_i \xi_i,\]</span>
where <span class="math inline">\(\alpha_i\)</span>’s and <span class="math inline">\(\gamma_i\)</span>’s are all positive. We can similarly obtain the solution corresponding to <span class="math inline">\(\boldsymbol{\beta}\)</span>, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\boldsymbol{\xi}\)</span>:</p>
<p><span class="math display">\[\begin{align}
\boldsymbol{\beta}- \sum_{i = 1}^n \alpha_i y_i x_i  =&amp;~ 0 \quad (\nabla_\boldsymbol{\beta}{\cal L}= 0 ) \\
\sum_{i = 1}^n \alpha_i y_i =&amp;~ 0 \quad (\nabla_{\beta_0} {\cal L}= 0 ) \\
C - \alpha_i - \gamma_i =&amp;~ 0 \quad (\nabla_{\xi_i} {\cal L}= 0 )
\end{align}\]</span></p>
<p>Substituting them back into the Lagrangian, we have the dual form:</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol{\alpha}}{\max} \quad &amp; \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \color{OrangeRed}{\langle \mathbf{x}_i, \mathbf{x}_j \rangle} \\
\text{subject to} \quad &amp; 0 \leq \alpha_i \leq C, \,\, i = 1, \ldots, n, \\
\text{and} \quad &amp; \sum_{i = 1}^n \alpha_i y_i = 0.
\end{align}\]</span></p>
<p>Here, the inner product <span class="math inline">\(\langle \mathbf{x}_i, \mathbf{x}_j \rangle\)</span> is nothing but <span class="math inline">\(\mathbf{x}_i^\text{T}\mathbf{x}_j\)</span>. The observations with <span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span> are the that lie on the margin. Hence, we can obtain these observations and perform the same calculations as before to obtain <span class="math inline">\(\widehat{\beta}_0\)</span>. The following code generates some data for this situation and fit SVM. We use the default <span class="math inline">\(C = 1\)</span>.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="support-vector-machines.html#cb144-1" tabindex="-1"></a></span>
<span id="cb144-2"><a href="support-vector-machines.html#cb144-2" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">70</span>)</span>
<span id="cb144-3"><a href="support-vector-machines.html#cb144-3" tabindex="-1"></a>    n <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co"># number of data points for each class</span></span>
<span id="cb144-4"><a href="support-vector-machines.html#cb144-4" tabindex="-1"></a>    p <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co"># dimension</span></span>
<span id="cb144-5"><a href="support-vector-machines.html#cb144-5" tabindex="-1"></a></span>
<span id="cb144-6"><a href="support-vector-machines.html#cb144-6" tabindex="-1"></a>    <span class="co"># Generate the positive and negative examples</span></span>
<span id="cb144-7"><a href="support-vector-machines.html#cb144-7" tabindex="-1"></a>    xneg <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="dv">1</span>),n,p)</span>
<span id="cb144-8"><a href="support-vector-machines.html#cb144-8" tabindex="-1"></a>    xpos <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n<span class="sc">*</span>p,<span class="at">mean=</span><span class="fl">1.5</span>,<span class="at">sd=</span><span class="dv">1</span>),n,p)</span>
<span id="cb144-9"><a href="support-vector-machines.html#cb144-9" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(xpos,xneg)</span>
<span id="cb144-10"><a href="support-vector-machines.html#cb144-10" tabindex="-1"></a>    y <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">as.factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n),<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>,n))))</span>
<span id="cb144-11"><a href="support-vector-machines.html#cb144-11" tabindex="-1"></a></span>
<span id="cb144-12"><a href="support-vector-machines.html#cb144-12" tabindex="-1"></a>    <span class="co"># Visualize the data</span></span>
<span id="cb144-13"><a href="support-vector-machines.html#cb144-13" tabindex="-1"></a>    </span>
<span id="cb144-14"><a href="support-vector-machines.html#cb144-14" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">&gt;</span><span class="dv">0</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb144-15"><a href="support-vector-machines.html#cb144-15" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;X1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;X2&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb144-16"><a href="support-vector-machines.html#cb144-16" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Positive&quot;</span>,<span class="st">&quot;Negative&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>),</span>
<span id="cb144-17"><a href="support-vector-machines.html#cb144-17" tabindex="-1"></a>           <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>),<span class="at">text.col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb144-18"><a href="support-vector-machines.html#cb144-18" tabindex="-1"></a></span>
<span id="cb144-19"><a href="support-vector-machines.html#cb144-19" tabindex="-1"></a>    svm.fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">type=</span><span class="st">&#39;C-classification&#39;</span>, </span>
<span id="cb144-20"><a href="support-vector-machines.html#cb144-20" tabindex="-1"></a>                   <span class="at">kernel=</span><span class="st">&#39;linear&#39;</span>,<span class="at">scale=</span><span class="cn">FALSE</span>, <span class="at">cost =</span> <span class="dv">1</span>)</span>
<span id="cb144-21"><a href="support-vector-machines.html#cb144-21" tabindex="-1"></a></span>
<span id="cb144-22"><a href="support-vector-machines.html#cb144-22" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="fu">t</span>(svm.fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm.fit<span class="sc">$</span>SV</span>
<span id="cb144-23"><a href="support-vector-machines.html#cb144-23" tabindex="-1"></a>    b0 <span class="ot">&lt;-</span> <span class="sc">-</span>svm.fit<span class="sc">$</span>rho</span>
<span id="cb144-24"><a href="support-vector-machines.html#cb144-24" tabindex="-1"></a>    </span>
<span id="cb144-25"><a href="support-vector-machines.html#cb144-25" tabindex="-1"></a>    <span class="fu">points</span>(x[svm.fit<span class="sc">$</span>index, ], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">cex=</span><span class="dv">3</span>)     </span>
<span id="cb144-26"><a href="support-vector-machines.html#cb144-26" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> <span class="sc">-</span>b0<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb144-27"><a href="support-vector-machines.html#cb144-27" tabindex="-1"></a>    </span>
<span id="cb144-28"><a href="support-vector-machines.html#cb144-28" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="dv">-1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb144-29"><a href="support-vector-machines.html#cb144-29" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-220-1.png" width="40%" style="display: block; margin: auto;" /></p>
<p>If we instead use a smaller <span class="math inline">\(C\)</span>:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="support-vector-machines.html#cb145-1" tabindex="-1"></a>    <span class="co"># Visualize the data</span></span>
<span id="cb145-2"><a href="support-vector-machines.html#cb145-2" tabindex="-1"></a>    <span class="fu">plot</span>(x,<span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">&gt;</span><span class="dv">0</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.2</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb145-3"><a href="support-vector-machines.html#cb145-3" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;X1&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;X2&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb145-4"><a href="support-vector-machines.html#cb145-4" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Positive&quot;</span>,<span class="st">&quot;Negative&quot;</span>),<span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>),</span>
<span id="cb145-5"><a href="support-vector-machines.html#cb145-5" tabindex="-1"></a>           <span class="at">pch=</span><span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>),<span class="at">text.col=</span><span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;red&quot;</span>), <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb145-6"><a href="support-vector-machines.html#cb145-6" tabindex="-1"></a></span>
<span id="cb145-7"><a href="support-vector-machines.html#cb145-7" tabindex="-1"></a>    <span class="co"># fit SVM with C = 10</span></span>
<span id="cb145-8"><a href="support-vector-machines.html#cb145-8" tabindex="-1"></a>    svm.fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> <span class="fu">data.frame</span>(x, y), <span class="at">type=</span><span class="st">&#39;C-classification&#39;</span>, </span>
<span id="cb145-9"><a href="support-vector-machines.html#cb145-9" tabindex="-1"></a>                   <span class="at">kernel=</span><span class="st">&#39;linear&#39;</span>,<span class="at">scale=</span><span class="cn">FALSE</span>, <span class="at">cost =</span> <span class="fl">0.1</span>)</span>
<span id="cb145-10"><a href="support-vector-machines.html#cb145-10" tabindex="-1"></a></span>
<span id="cb145-11"><a href="support-vector-machines.html#cb145-11" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="fu">t</span>(svm.fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm.fit<span class="sc">$</span>SV</span>
<span id="cb145-12"><a href="support-vector-machines.html#cb145-12" tabindex="-1"></a>    b0 <span class="ot">&lt;-</span> <span class="sc">-</span>svm.fit<span class="sc">$</span>rho</span>
<span id="cb145-13"><a href="support-vector-machines.html#cb145-13" tabindex="-1"></a>    </span>
<span id="cb145-14"><a href="support-vector-machines.html#cb145-14" tabindex="-1"></a>    <span class="fu">points</span>(x[svm.fit<span class="sc">$</span>index, ], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">cex=</span><span class="dv">3</span>)     </span>
<span id="cb145-15"><a href="support-vector-machines.html#cb145-15" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> <span class="sc">-</span>b0<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb145-16"><a href="support-vector-machines.html#cb145-16" tabindex="-1"></a>    </span>
<span id="cb145-17"><a href="support-vector-machines.html#cb145-17" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="dv">-1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb145-18"><a href="support-vector-machines.html#cb145-18" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b0<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>b[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>b[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-221-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-saheart-data" class="section level2 hasAnchor" number="17.4">
<h2><span class="header-section-number">17.4</span> Example: <code>SAheart</code> Data<a href="support-vector-machines.html#example-saheart-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If you want to use the <code>1071e</code> package and perform cross-validation, you could consider using the <code>caret</code> package. Make sure that you specify <code>method = "svmLinear2"</code>. The following code is using the <code>SAheart</code> as an example.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="support-vector-machines.html#cb146-1" tabindex="-1"></a>  <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb146-2"><a href="support-vector-machines.html#cb146-2" tabindex="-1"></a>  <span class="fu">data</span>(SAheart)</span>
<span id="cb146-3"><a href="support-vector-machines.html#cb146-3" tabindex="-1"></a>  <span class="fu">library</span>(caret)</span>
<span id="cb146-4"><a href="support-vector-machines.html#cb146-4" tabindex="-1"></a></span>
<span id="cb146-5"><a href="support-vector-machines.html#cb146-5" tabindex="-1"></a>  cost.grid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">cost =</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="dv">2</span>, <span class="at">length =</span> <span class="dv">20</span>))</span>
<span id="cb146-6"><a href="support-vector-machines.html#cb146-6" tabindex="-1"></a>  train_control <span class="ot">=</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;repeatedcv&quot;</span>, <span class="at">number=</span><span class="dv">10</span>, <span class="at">repeats=</span><span class="dv">3</span>)</span>
<span id="cb146-7"><a href="support-vector-machines.html#cb146-7" tabindex="-1"></a>  </span>
<span id="cb146-8"><a href="support-vector-machines.html#cb146-8" tabindex="-1"></a>  svm2 <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="fu">as.factor</span>(chd) <span class="sc">~</span>., <span class="at">data =</span> SAheart, <span class="at">method =</span> <span class="st">&quot;svmLinear2&quot;</span>, </span>
<span id="cb146-9"><a href="support-vector-machines.html#cb146-9" tabindex="-1"></a>                <span class="at">trControl =</span> train_control,  </span>
<span id="cb146-10"><a href="support-vector-machines.html#cb146-10" tabindex="-1"></a>                <span class="at">tuneGrid =</span> cost.grid)</span>
<span id="cb146-11"><a href="support-vector-machines.html#cb146-11" tabindex="-1"></a>  </span>
<span id="cb146-12"><a href="support-vector-machines.html#cb146-12" tabindex="-1"></a>  <span class="co"># see the fitted model</span></span>
<span id="cb146-13"><a href="support-vector-machines.html#cb146-13" tabindex="-1"></a>  svm2</span>
<span id="cb146-14"><a href="support-vector-machines.html#cb146-14" tabindex="-1"></a><span class="do">## Support Vector Machines with Linear Kernel </span></span>
<span id="cb146-15"><a href="support-vector-machines.html#cb146-15" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb146-16"><a href="support-vector-machines.html#cb146-16" tabindex="-1"></a><span class="do">## 462 samples</span></span>
<span id="cb146-17"><a href="support-vector-machines.html#cb146-17" tabindex="-1"></a><span class="do">##   9 predictor</span></span>
<span id="cb146-18"><a href="support-vector-machines.html#cb146-18" tabindex="-1"></a><span class="do">##   2 classes: &#39;0&#39;, &#39;1&#39; </span></span>
<span id="cb146-19"><a href="support-vector-machines.html#cb146-19" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb146-20"><a href="support-vector-machines.html#cb146-20" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb146-21"><a href="support-vector-machines.html#cb146-21" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold, repeated 3 times) </span></span>
<span id="cb146-22"><a href="support-vector-machines.html#cb146-22" tabindex="-1"></a><span class="do">## Summary of sample sizes: 416, 416, 416, 415, 416, 416, ... </span></span>
<span id="cb146-23"><a href="support-vector-machines.html#cb146-23" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb146-24"><a href="support-vector-machines.html#cb146-24" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb146-25"><a href="support-vector-machines.html#cb146-25" tabindex="-1"></a><span class="do">##   cost       Accuracy   Kappa    </span></span>
<span id="cb146-26"><a href="support-vector-machines.html#cb146-26" tabindex="-1"></a><span class="do">##   0.0100000  0.7142923  0.2844994</span></span>
<span id="cb146-27"><a href="support-vector-machines.html#cb146-27" tabindex="-1"></a><span class="do">##   0.1147368  0.7200123  0.3520308</span></span>
<span id="cb146-28"><a href="support-vector-machines.html#cb146-28" tabindex="-1"></a><span class="do">##   0.2194737  0.7164354  0.3454492</span></span>
<span id="cb146-29"><a href="support-vector-machines.html#cb146-29" tabindex="-1"></a><span class="do">##   0.3242105  0.7171600  0.3467866</span></span>
<span id="cb146-30"><a href="support-vector-machines.html#cb146-30" tabindex="-1"></a><span class="do">##   0.4289474  0.7164354  0.3453015</span></span>
<span id="cb146-31"><a href="support-vector-machines.html#cb146-31" tabindex="-1"></a><span class="do">##   0.5336842  0.7164354  0.3450704</span></span>
<span id="cb146-32"><a href="support-vector-machines.html#cb146-32" tabindex="-1"></a><span class="do">##   0.6384211  0.7157108  0.3438517</span></span>
<span id="cb146-33"><a href="support-vector-machines.html#cb146-33" tabindex="-1"></a><span class="do">##   0.7431579  0.7171600  0.3472755</span></span>
<span id="cb146-34"><a href="support-vector-machines.html#cb146-34" tabindex="-1"></a><span class="do">##   0.8478947  0.7157108  0.3437850</span></span>
<span id="cb146-35"><a href="support-vector-machines.html#cb146-35" tabindex="-1"></a><span class="do">##   0.9526316  0.7157108  0.3437850</span></span>
<span id="cb146-36"><a href="support-vector-machines.html#cb146-36" tabindex="-1"></a><span class="do">##   1.0573684  0.7171600  0.3479914</span></span>
<span id="cb146-37"><a href="support-vector-machines.html#cb146-37" tabindex="-1"></a><span class="do">##   1.1621053  0.7164354  0.3459484</span></span>
<span id="cb146-38"><a href="support-vector-machines.html#cb146-38" tabindex="-1"></a><span class="do">##   1.2668421  0.7164354  0.3459484</span></span>
<span id="cb146-39"><a href="support-vector-machines.html#cb146-39" tabindex="-1"></a><span class="do">##   1.3715789  0.7178847  0.3500130</span></span>
<span id="cb146-40"><a href="support-vector-machines.html#cb146-40" tabindex="-1"></a><span class="do">##   1.4763158  0.7171600  0.3479914</span></span>
<span id="cb146-41"><a href="support-vector-machines.html#cb146-41" tabindex="-1"></a><span class="do">##   1.5810526  0.7178847  0.3500130</span></span>
<span id="cb146-42"><a href="support-vector-machines.html#cb146-42" tabindex="-1"></a><span class="do">##   1.6857895  0.7171600  0.3479914</span></span>
<span id="cb146-43"><a href="support-vector-machines.html#cb146-43" tabindex="-1"></a><span class="do">##   1.7905263  0.7171600  0.3479914</span></span>
<span id="cb146-44"><a href="support-vector-machines.html#cb146-44" tabindex="-1"></a><span class="do">##   1.8952632  0.7171600  0.3479914</span></span>
<span id="cb146-45"><a href="support-vector-machines.html#cb146-45" tabindex="-1"></a><span class="do">##   2.0000000  0.7164354  0.3459484</span></span>
<span id="cb146-46"><a href="support-vector-machines.html#cb146-46" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb146-47"><a href="support-vector-machines.html#cb146-47" tabindex="-1"></a><span class="do">## Accuracy was used to select the optimal model using the largest value.</span></span>
<span id="cb146-48"><a href="support-vector-machines.html#cb146-48" tabindex="-1"></a><span class="do">## The final value used for the model was cost = 0.1147368.</span></span></code></pre></div>
<p>Note that when you fit the model, there are a few things you could consider:</p>
<ul>
<li>You can consider centering and scaling the covariates. This can be done during pre-processing. Or you may specify <code>preProcess = c("center", "scale")</code> in the <code>train()</code> function.</li>
<li>You may want to start with a wider range of cost values, then narrow down to a smaller range, since SVM can be quite sensitive to tuning in some cases.</li>
<li>There are many other SVM libraries, such as <code>kernlab</code>. This can be specified by using <code>method = "svmLinear"</code>. However, <code>kernlab</code> uses <code>C</code> as the parameter name for cost. We will show an example later.</li>
</ul>
</div>
<div id="nonlinear-svm-via-kernel-trick" class="section level2 hasAnchor" number="17.5">
<h2><span class="header-section-number">17.5</span> Nonlinear SVM via Kernel Trick<a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The essential idea of kernel trick can be summarized as using the kernel function of two observations <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{z}\)</span> to replace the inner product between some feature mapping of the two covariate vectors. In other words, if we want to create some nonlinear features of <span class="math inline">\(\mathbf{x}\)</span>, such as <span class="math inline">\(x_1^2\)</span>, <span class="math inline">\(\exp(x_2)\)</span>, <span class="math inline">\(\sqrt{x_3}\)</span>, etc., we may in general write them as</p>
<p><span class="math display">\[\Phi : {\cal X}\rightarrow {\cal F}, \,\,\, \Phi(\mathbf{x}) = (\phi_1(\mathbf{x}), \phi_2(\mathbf{x}), \ldots ),\]</span>
where <span class="math inline">\({\cal F}\)</span> has either finite or infinite dimensions. Then, we can still treat this as a linear SVM by constructing the decision rule as</p>
<p><span class="math display">\[f(x) = \langle \Phi(\mathbf{x}), \boldsymbol{\beta}\rangle = \Phi(\mathbf{x})^\text{T}\boldsymbol{\beta}.\]</span>
This is why we used the <span class="math inline">\(\langle \cdot, \cdot\rangle\)</span> operator in the previous example. Now, the kernel trick is essentially skipping the explicit calculation of <span class="math inline">\(\Phi(\mathbf{x})\)</span> by utilizing the property that</p>
<p><span class="math display">\[K(\mathbf{x}, \mathbf{z}) = \langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\]</span>
for some kernel function <span class="math inline">\(K(\mathbf{x}, \mathbf{z})\)</span>. Since <span class="math inline">\(\langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\)</span> is all we need in the dual form, we can simply replace it by <span class="math inline">\(K(\mathbf{x}, \mathbf{z})\)</span>, which gives the kernel form:</p>
<p><span class="math display">\[\begin{align}
\underset{\boldsymbol{\alpha}}{\max} \quad &amp; \sum_{i = 1}^n \alpha_i - \frac{1}{2} \sum_{i, j = 1}^n y_i y_j \alpha_i\alpha_j \color{OrangeRed}{K(\mathbf{x}_i, \mathbf{x}_j)} \\
\text{subject to} \quad &amp; 0 \leq \alpha_i \leq C, \,\, i = 1, \ldots, n, \\
\text{and} \quad &amp; \sum_{i = 1}^n \alpha_i y_i = 0.
\end{align}\]</span></p>
<p>One most apparent advantage of doing this is to save computational cost. This maybe understood using the following example:</p>
<ul>
<li>Consider kernel function <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = (\mathbf{x}^\text{T}\mathbf{z})^2\)</span></li>
<li>Consider <span class="math inline">\(\Phi(\mathbf{x})\)</span> being the basis expansion that contains all second order interactions: <span class="math inline">\(x_k x_l\)</span> for <span class="math inline">\(1 \leq k, l \leq p\)</span></li>
</ul>
<p>We can show that the two gives equivalent results, however, the kernel version is much faster. <span class="math inline">\(K(\mathbf{x}, \mathbf{z})\)</span> takes <span class="math inline">\(p+1\)</span> operations, while <span class="math inline">\(\langle \Phi(\mathbf{x}_i),  \Phi(\mathbf{x}_j) \rangle\)</span> requires <span class="math inline">\(3p^2\)</span>.</p>
<p><span class="math display">\[\begin{align}
K(\mathbf{x}, \mathbf{z}) &amp;=~ \left(\sum_{k=1}^p x_k z_k\right) \left(\sum_{l=1}^p x_l z_l\right) \\
&amp;=~ \sum_{k=1}^p \sum_{l=1}^p x_k z_k x_l z_l \\
&amp;=~ \sum_{k, l=1}^p (x_k x_l) (z_k z_l) \\
&amp;=~ \langle \Phi(\mathbf{x}),  \Phi(\mathbf{z}) \rangle
\end{align}\]</span></p>
<p>Formally, this property is guaranteed by the <strong>Mercer’s theorem</strong><span class="citation">(<a href="#ref-mercer1909xvi">Mercer 1909</a>)</span> that ensures: The kernel matrix <span class="math inline">\(K\)</span> is positive semi-definite if and only if the function <span class="math inline">\(K(x_i ,x_j)\)</span> is equivalent to some inner product <span class="math inline">\(\langle \Phi(\mathbf{x}), \Phi(\mathbf{z}) \rangle\)</span>. Although the feature maps produced by Mercer’s theorem may not be unique, it does guarantee the existence of such a feature map.</p>
<p>Besides making the calculation of nonlinear functions easier, using the kernel trick also implies that if we use a proper kernel function, then it defines a space of functions <span class="math inline">\({\cal H}\)</span> (reproducing kernel Hilbert space, RKHS) that can be represented in the form of <span class="math inline">\(f(x) = \sum_i \alpha_i K(x, x_i)\)</span> for some <span class="math inline">\(x_i\)</span> in <span class="math inline">\({\cal X}\)</span> (see the Moore–Aronszajn theorem) with a proper definition of inner product. However, this space is of infinite dimension, noticing that <span class="math inline">\(i\)</span> goes from 1 to infinity. However, as long as we search for the solution within <span class="math inline">\({\cal H}\)</span>, and also apply a proper penalty of the estimated function <span class="math inline">\(\widehat{f}(\mathbf{x})\)</span>, then our computational job will reduce to solving the <span class="math inline">\(\alpha_i\)</span>’s that corresponds to the observed <span class="math inline">\(n\)</span> data points, meaning that we only need to solve the solution within a finite space. This is guaranteed by the <strong>Representer theorem</strong>. There are numerous articles on the RKHS. Hence, we will not focus on introducing this technique. However, we will later on use this property in the penalized formulation of SVM.</p>
</div>
<div id="example-mixture.example-data" class="section level2 hasAnchor" number="17.6">
<h2><span class="header-section-number">17.6</span> Example: <code>mixture.example</code> Data<a href="support-vector-machines.html#example-mixture.example-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use the <code>mixture.example</code> data in the <code>ElemStatLearn</code> package. In addition, we use a different package <code>kernlab</code>. The red dotted line indicates the true decision boundary.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="support-vector-machines.html#cb147-1" tabindex="-1"></a>    <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb147-2"><a href="support-vector-machines.html#cb147-2" tabindex="-1"></a>    <span class="fu">data</span>(mixture.example)</span>
<span id="cb147-3"><a href="support-vector-machines.html#cb147-3" tabindex="-1"></a></span>
<span id="cb147-4"><a href="support-vector-machines.html#cb147-4" tabindex="-1"></a>    <span class="co"># redefine data</span></span>
<span id="cb147-5"><a href="support-vector-machines.html#cb147-5" tabindex="-1"></a>    px1 <span class="ot">=</span> mixture.example<span class="sc">$</span>px1</span>
<span id="cb147-6"><a href="support-vector-machines.html#cb147-6" tabindex="-1"></a>    px2 <span class="ot">=</span> mixture.example<span class="sc">$</span>px2</span>
<span id="cb147-7"><a href="support-vector-machines.html#cb147-7" tabindex="-1"></a>    x <span class="ot">=</span> mixture.example<span class="sc">$</span>x</span>
<span id="cb147-8"><a href="support-vector-machines.html#cb147-8" tabindex="-1"></a>    y <span class="ot">=</span> mixture.example<span class="sc">$</span>y</span>
<span id="cb147-9"><a href="support-vector-machines.html#cb147-9" tabindex="-1"></a>    </span>
<span id="cb147-10"><a href="support-vector-machines.html#cb147-10" tabindex="-1"></a>    <span class="co"># plot the data and true decision boundary</span></span>
<span id="cb147-11"><a href="support-vector-machines.html#cb147-11" tabindex="-1"></a>    prob <span class="ot">&lt;-</span> mixture.example<span class="sc">$</span>prob</span>
<span id="cb147-12"><a href="support-vector-machines.html#cb147-12" tabindex="-1"></a>    prob.bayes <span class="ot">&lt;-</span> <span class="fu">matrix</span>(prob, </span>
<span id="cb147-13"><a href="support-vector-machines.html#cb147-13" tabindex="-1"></a>                         <span class="fu">length</span>(px1), </span>
<span id="cb147-14"><a href="support-vector-machines.html#cb147-14" tabindex="-1"></a>                         <span class="fu">length</span>(px2))</span>
<span id="cb147-15"><a href="support-vector-machines.html#cb147-15" tabindex="-1"></a>    <span class="fu">contour</span>(px1, px2, prob.bayes, <span class="at">levels=</span><span class="fl">0.5</span>, <span class="at">lty=</span><span class="dv">2</span>, </span>
<span id="cb147-16"><a href="support-vector-machines.html#cb147-16" tabindex="-1"></a>            <span class="at">labels=</span><span class="st">&quot;&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;x1&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;x2&quot;</span>,</span>
<span id="cb147-17"><a href="support-vector-machines.html#cb147-17" tabindex="-1"></a>            <span class="at">main=</span><span class="st">&quot;SVM with linear kernal&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb147-18"><a href="support-vector-machines.html#cb147-18" tabindex="-1"></a>    <span class="fu">points</span>(x, <span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb147-19"><a href="support-vector-machines.html#cb147-19" tabindex="-1"></a></span>
<span id="cb147-20"><a href="support-vector-machines.html#cb147-20" tabindex="-1"></a>    <span class="co"># train linear SVM using the kernlab package</span></span>
<span id="cb147-21"><a href="support-vector-machines.html#cb147-21" tabindex="-1"></a>    <span class="fu">library</span>(kernlab)</span>
<span id="cb147-22"><a href="support-vector-machines.html#cb147-22" tabindex="-1"></a>    cost <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb147-23"><a href="support-vector-machines.html#cb147-23" tabindex="-1"></a>    svm.fit <span class="ot">&lt;-</span> <span class="fu">ksvm</span>(x, y, <span class="at">type=</span><span class="st">&quot;C-svc&quot;</span>, <span class="at">kernel=</span><span class="st">&#39;vanilladot&#39;</span>, <span class="at">C=</span>cost)</span>
<span id="cb147-24"><a href="support-vector-machines.html#cb147-24" tabindex="-1"></a><span class="do">##  Setting default kernel parameters</span></span>
<span id="cb147-25"><a href="support-vector-machines.html#cb147-25" tabindex="-1"></a></span>
<span id="cb147-26"><a href="support-vector-machines.html#cb147-26" tabindex="-1"></a>    <span class="co"># plot the SVM decision boundary</span></span>
<span id="cb147-27"><a href="support-vector-machines.html#cb147-27" tabindex="-1"></a>    <span class="co"># Extract the indices of the support vectors on the margin:</span></span>
<span id="cb147-28"><a href="support-vector-machines.html#cb147-28" tabindex="-1"></a>    sv.alpha<span class="ot">&lt;-</span><span class="fu">alpha</span>(svm.fit)[[<span class="dv">1</span>]][<span class="fu">which</span>(<span class="fu">alpha</span>(svm.fit)[[<span class="dv">1</span>]]<span class="sc">&lt;</span>cost)]</span>
<span id="cb147-29"><a href="support-vector-machines.html#cb147-29" tabindex="-1"></a>    sv.index<span class="ot">&lt;-</span><span class="fu">alphaindex</span>(svm.fit)[[<span class="dv">1</span>]][<span class="fu">which</span>(<span class="fu">alpha</span>(svm.fit)[[<span class="dv">1</span>]]<span class="sc">&lt;</span>cost)]</span>
<span id="cb147-30"><a href="support-vector-machines.html#cb147-30" tabindex="-1"></a>    sv.matrix<span class="ot">&lt;-</span>x[sv.index,]</span>
<span id="cb147-31"><a href="support-vector-machines.html#cb147-31" tabindex="-1"></a>    <span class="fu">points</span>(sv.matrix, <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span><span class="fu">ifelse</span>(y[sv.index] <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">cex=</span><span class="fl">1.5</span>)</span>
<span id="cb147-32"><a href="support-vector-machines.html#cb147-32" tabindex="-1"></a></span>
<span id="cb147-33"><a href="support-vector-machines.html#cb147-33" tabindex="-1"></a>    <span class="co"># Plot the hyperplane and the margins:</span></span>
<span id="cb147-34"><a href="support-vector-machines.html#cb147-34" tabindex="-1"></a>    w <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">cbind</span>(<span class="fu">coef</span>(svm.fit)[[<span class="dv">1</span>]])) <span class="sc">%*%</span> <span class="fu">xmatrix</span>(svm.fit)[[<span class="dv">1</span>]]</span>
<span id="cb147-35"><a href="support-vector-machines.html#cb147-35" tabindex="-1"></a>    b <span class="ot">&lt;-</span> <span class="sc">-</span> <span class="fu">b</span>(svm.fit)</span>
<span id="cb147-36"><a href="support-vector-machines.html#cb147-36" tabindex="-1"></a></span>
<span id="cb147-37"><a href="support-vector-machines.html#cb147-37" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> <span class="sc">-</span>b<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>w[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb147-38"><a href="support-vector-machines.html#cb147-38" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b<span class="dv">-1</span>)<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>w[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb147-39"><a href="support-vector-machines.html#cb147-39" tabindex="-1"></a>    <span class="fu">abline</span>(<span class="at">a=</span> (<span class="sc">-</span>b<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">b=</span><span class="sc">-</span>w[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">col=</span><span class="st">&quot;black&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-223-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>Let’s also try a nonlinear SVM, using the radial kernel.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="support-vector-machines.html#cb148-1" tabindex="-1"></a>    <span class="co"># fit SVM with radial kernel, with cost = 5</span></span>
<span id="cb148-2"><a href="support-vector-machines.html#cb148-2" tabindex="-1"></a>    dat <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">factor</span>(y), x)</span>
<span id="cb148-3"><a href="support-vector-machines.html#cb148-3" tabindex="-1"></a>    fit <span class="ot">=</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> dat, <span class="at">scale =</span> <span class="cn">FALSE</span>, <span class="at">kernel =</span> <span class="st">&quot;radial&quot;</span>, <span class="at">cost =</span> <span class="dv">5</span>)</span>
<span id="cb148-4"><a href="support-vector-machines.html#cb148-4" tabindex="-1"></a>    </span>
<span id="cb148-5"><a href="support-vector-machines.html#cb148-5" tabindex="-1"></a>    <span class="co"># extract the prediction</span></span>
<span id="cb148-6"><a href="support-vector-machines.html#cb148-6" tabindex="-1"></a>    xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">X1 =</span> px1, <span class="at">X2 =</span> px2)</span>
<span id="cb148-7"><a href="support-vector-machines.html#cb148-7" tabindex="-1"></a>    func <span class="ot">=</span> <span class="fu">predict</span>(fit, xgrid, <span class="at">decision.values =</span> <span class="cn">TRUE</span>)</span>
<span id="cb148-8"><a href="support-vector-machines.html#cb148-8" tabindex="-1"></a>    func <span class="ot">=</span> <span class="fu">attributes</span>(func)<span class="sc">$</span>decision</span>
<span id="cb148-9"><a href="support-vector-machines.html#cb148-9" tabindex="-1"></a>    </span>
<span id="cb148-10"><a href="support-vector-machines.html#cb148-10" tabindex="-1"></a>    <span class="co"># visualize the decision rule</span></span>
<span id="cb148-11"><a href="support-vector-machines.html#cb148-11" tabindex="-1"></a>    ygrid <span class="ot">=</span> <span class="fu">predict</span>(fit, xgrid)</span>
<span id="cb148-12"><a href="support-vector-machines.html#cb148-12" tabindex="-1"></a>    <span class="fu">plot</span>(xgrid, <span class="at">col =</span> <span class="fu">ifelse</span>(ygrid <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;bisque&quot;</span>, <span class="st">&quot;cadetblue1&quot;</span>), </span>
<span id="cb148-13"><a href="support-vector-machines.html#cb148-13" tabindex="-1"></a>         <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> <span class="fl">0.2</span>, <span class="at">main=</span><span class="st">&quot;SVM with radial kernal&quot;</span>)</span>
<span id="cb148-14"><a href="support-vector-machines.html#cb148-14" tabindex="-1"></a>    <span class="fu">points</span>(x, <span class="at">col=</span><span class="fu">ifelse</span>(y<span class="sc">==</span><span class="dv">1</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb148-15"><a href="support-vector-machines.html#cb148-15" tabindex="-1"></a>    </span>
<span id="cb148-16"><a href="support-vector-machines.html#cb148-16" tabindex="-1"></a>    <span class="co"># our estimated function value, cut at 0</span></span>
<span id="cb148-17"><a href="support-vector-machines.html#cb148-17" tabindex="-1"></a>    <span class="fu">contour</span>(px1, px2, <span class="fu">matrix</span>(func, <span class="dv">69</span>, <span class="dv">99</span>), <span class="at">level =</span> <span class="dv">0</span>, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb148-18"><a href="support-vector-machines.html#cb148-18" tabindex="-1"></a>    </span>
<span id="cb148-19"><a href="support-vector-machines.html#cb148-19" tabindex="-1"></a>    <span class="co"># the true probability, cut at 0.5</span></span>
<span id="cb148-20"><a href="support-vector-machines.html#cb148-20" tabindex="-1"></a>    <span class="fu">contour</span>(px1, px2, <span class="fu">matrix</span>(prob, <span class="dv">69</span>, <span class="dv">99</span>), <span class="at">level =</span> <span class="fl">0.5</span>, <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb148-21"><a href="support-vector-machines.html#cb148-21" tabindex="-1"></a>            <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-224-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>You may also consider some other popular kernels. The following ones are implemented in the <code>e1071</code> package, with additional tuning parameters <span class="math inline">\(\text{coef}_0\)</span> and <span class="math inline">\(\gamma\)</span>.</p>
<ul>
<li>Linear: <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = \mathbf{x}^\text{T}\mathbf{z}\)</span></li>
<li><span class="math inline">\(d\)</span>th degree polynomial: <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = (\text{coef}_0 + \gamma \mathbf{x}^\text{T}\mathbf{z})^d\)</span></li>
<li>Radial basis: <span class="math inline">\(K(\mathbf{x}, \mathbf{z}) = \exp(- \gamma \lVert \mathbf{x}- \mathbf{z}\lVert^2)\)</span></li>
<li>Sigmoid: <span class="math inline">\(\tanh(\gamma \mathbf{x}^\text{T}\mathbf{z}+ \text{coef}_0)\)</span></li>
</ul>
<p>Cross-validation can also be doing using the <code>caret</code> package. To specify the kernel, one must correctly specify the <code>method</code> parameter in the <code>train()</code> function. For this example, we use the <code>method = "svmRadial"</code> that uses the <code>kernlab</code> package to fit the model. For this choice, you need to tune just <code>sigma</code> and <code>C</code> (cost). More details are refereed to the <a href="https://topepo.github.io/caret/train-models-by-tag.html#support-vector-machines"><code>caret</code> documentation</a>.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="support-vector-machines.html#cb149-1" tabindex="-1"></a>  svm.radial <span class="ot">&lt;-</span> <span class="fu">train</span>(y <span class="sc">~</span> ., <span class="at">data =</span> dat, <span class="at">method =</span> <span class="st">&quot;svmRadial&quot;</span>,</span>
<span id="cb149-2"><a href="support-vector-machines.html#cb149-2" tabindex="-1"></a>                <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb149-3"><a href="support-vector-machines.html#cb149-3" tabindex="-1"></a>                <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">C =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>), <span class="at">sigma =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)),</span>
<span id="cb149-4"><a href="support-vector-machines.html#cb149-4" tabindex="-1"></a>                <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>))</span>
<span id="cb149-5"><a href="support-vector-machines.html#cb149-5" tabindex="-1"></a>  svm.radial</span>
<span id="cb149-6"><a href="support-vector-machines.html#cb149-6" tabindex="-1"></a><span class="do">## Support Vector Machines with Radial Basis Function Kernel </span></span>
<span id="cb149-7"><a href="support-vector-machines.html#cb149-7" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb149-8"><a href="support-vector-machines.html#cb149-8" tabindex="-1"></a><span class="do">## 200 samples</span></span>
<span id="cb149-9"><a href="support-vector-machines.html#cb149-9" tabindex="-1"></a><span class="do">##   2 predictor</span></span>
<span id="cb149-10"><a href="support-vector-machines.html#cb149-10" tabindex="-1"></a><span class="do">##   2 classes: &#39;0&#39;, &#39;1&#39; </span></span>
<span id="cb149-11"><a href="support-vector-machines.html#cb149-11" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb149-12"><a href="support-vector-machines.html#cb149-12" tabindex="-1"></a><span class="do">## Pre-processing: centered (2), scaled (2) </span></span>
<span id="cb149-13"><a href="support-vector-machines.html#cb149-13" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (5 fold) </span></span>
<span id="cb149-14"><a href="support-vector-machines.html#cb149-14" tabindex="-1"></a><span class="do">## Summary of sample sizes: 160, 160, 160, 160, 160 </span></span>
<span id="cb149-15"><a href="support-vector-machines.html#cb149-15" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb149-16"><a href="support-vector-machines.html#cb149-16" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb149-17"><a href="support-vector-machines.html#cb149-17" tabindex="-1"></a><span class="do">##   C     sigma  Accuracy  Kappa</span></span>
<span id="cb149-18"><a href="support-vector-machines.html#cb149-18" tabindex="-1"></a><span class="do">##   0.01  1      0.715     0.43 </span></span>
<span id="cb149-19"><a href="support-vector-machines.html#cb149-19" tabindex="-1"></a><span class="do">##   0.01  2      0.760     0.52 </span></span>
<span id="cb149-20"><a href="support-vector-machines.html#cb149-20" tabindex="-1"></a><span class="do">##   0.01  3      0.770     0.54 </span></span>
<span id="cb149-21"><a href="support-vector-machines.html#cb149-21" tabindex="-1"></a><span class="do">##   0.10  1      0.720     0.44 </span></span>
<span id="cb149-22"><a href="support-vector-machines.html#cb149-22" tabindex="-1"></a><span class="do">##   0.10  2      0.790     0.58 </span></span>
<span id="cb149-23"><a href="support-vector-machines.html#cb149-23" tabindex="-1"></a><span class="do">##   0.10  3      0.800     0.60 </span></span>
<span id="cb149-24"><a href="support-vector-machines.html#cb149-24" tabindex="-1"></a><span class="do">##   0.50  1      0.795     0.59 </span></span>
<span id="cb149-25"><a href="support-vector-machines.html#cb149-25" tabindex="-1"></a><span class="do">##   0.50  2      0.815     0.63 </span></span>
<span id="cb149-26"><a href="support-vector-machines.html#cb149-26" tabindex="-1"></a><span class="do">##   0.50  3      0.830     0.66 </span></span>
<span id="cb149-27"><a href="support-vector-machines.html#cb149-27" tabindex="-1"></a><span class="do">##   1.00  1      0.795     0.59 </span></span>
<span id="cb149-28"><a href="support-vector-machines.html#cb149-28" tabindex="-1"></a><span class="do">##   1.00  2      0.825     0.65 </span></span>
<span id="cb149-29"><a href="support-vector-machines.html#cb149-29" tabindex="-1"></a><span class="do">##   1.00  3      0.835     0.67 </span></span>
<span id="cb149-30"><a href="support-vector-machines.html#cb149-30" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb149-31"><a href="support-vector-machines.html#cb149-31" tabindex="-1"></a><span class="do">## Accuracy was used to select the optimal model using the largest value.</span></span>
<span id="cb149-32"><a href="support-vector-machines.html#cb149-32" tabindex="-1"></a><span class="do">## The final values used for the model were sigma = 3 and C = 1.</span></span></code></pre></div>
</div>
<div id="svm-as-a-penalized-model" class="section level2 hasAnchor" number="17.7">
<h2><span class="header-section-number">17.7</span> SVM as a Penalized Model<a href="support-vector-machines.html#svm-as-a-penalized-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that in SVM, we need <span class="math inline">\(y_i f(\mathbf{x}_i)\)</span> to be at least <span class="math inline">\(1 - \xi_i\)</span>, this implies that we would prefer <span class="math inline">\(1 - y_i f(\mathbf{x}_i)\)</span> to be negative or 0. And observation with <span class="math inline">\(1 - y_i f(\mathbf{x}_i)\)</span> should be penalized. Hence, recall that the objective function of dual form in SVM is <span class="math inline">\(\frac{1}{2}\lVert \boldsymbol{\beta}\rVert^2 + C \sum_{i=1}^n \xi_i\)</span>, we may rewrite this as a new version:</p>
<p><span class="math display">\[\min \,\, \sum_{i=1}^n \big[ 1 - y_i f(\mathbf{x}_i) \big]_{+} \, +\, \lambda \lVert \boldsymbol{\beta}\rVert^2.\]</span>
Here, we converted <span class="math inline">\(1/(2C)\)</span> to <span class="math inline">\(\lambda\)</span>. And this resembles a familiar form of “Loss <span class="math inline">\(+\)</span> Penalty”, where the slack variables becomes the loss and the norm of <span class="math inline">\(\boldsymbol{\beta}\)</span> is the penalty. This particular loss function is called the <strong>Hinge loss</strong>, with</p>
<p><span class="math display">\[L(y, f(\mathbf{x})) = [1 - yf(\mathbf{x})]_+ = \max(0, 1 - yf(\mathbf{x}))\]</span>
However, the Hinge loss is not differentiable. There are some other loss functions that can be used as substitute:</p>
<ul>
<li>Logistic loss:
<span class="math display">\[L(y, f(\mathbf{x})) = \log_2( 1 + e^{-y f(\mathbf{x})})\]</span></li>
<li>Modified Huber Loss:
<span class="math display">\[L(y, f(\mathbf{x})) = \begin{cases}
\max(0, 1 - yf(\mathbf{x}))^2 &amp; \text{for} \quad yf(\mathbf{x}) \geq -1 \\
-4 yf(\mathbf{x})  &amp; \text{otherwise}  \\
\end{cases}\]</span></li>
</ul>
<p>Here is a visualization of several different loss functions.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="support-vector-machines.html#cb150-1" tabindex="-1"></a>  t <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="fl">0.01</span>)</span>
<span id="cb150-2"><a href="support-vector-machines.html#cb150-2" tabindex="-1"></a></span>
<span id="cb150-3"><a href="support-vector-machines.html#cb150-3" tabindex="-1"></a>  <span class="co"># different loss functions</span></span>
<span id="cb150-4"><a href="support-vector-machines.html#cb150-4" tabindex="-1"></a>  hinge <span class="ot">=</span> <span class="fu">pmax</span>(<span class="dv">0</span>, <span class="dv">1</span> <span class="sc">-</span> t) </span>
<span id="cb150-5"><a href="support-vector-machines.html#cb150-5" tabindex="-1"></a>  zeroone <span class="ot">=</span> (t <span class="sc">&lt;=</span> <span class="dv">0</span>)</span>
<span id="cb150-6"><a href="support-vector-machines.html#cb150-6" tabindex="-1"></a>  logistic <span class="ot">=</span> <span class="fu">log2</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>t))</span>
<span id="cb150-7"><a href="support-vector-machines.html#cb150-7" tabindex="-1"></a>  modifiedhuber <span class="ot">=</span> <span class="fu">ifelse</span>(t <span class="sc">&gt;=</span> <span class="sc">-</span><span class="dv">1</span>, (<span class="fu">pmax</span>(<span class="dv">0</span>, <span class="dv">1</span> <span class="sc">-</span> t))<span class="sc">^</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">4</span><span class="sc">*</span>t)</span>
<span id="cb150-8"><a href="support-vector-machines.html#cb150-8" tabindex="-1"></a>  </span>
<span id="cb150-9"><a href="support-vector-machines.html#cb150-9" tabindex="-1"></a>  <span class="co"># plot</span></span>
<span id="cb150-10"><a href="support-vector-machines.html#cb150-10" tabindex="-1"></a>  <span class="fu">plot</span>(t, zeroone, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>),</span>
<span id="cb150-11"><a href="support-vector-machines.html#cb150-11" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Loss Functions&quot;</span>)</span>
<span id="cb150-12"><a href="support-vector-machines.html#cb150-12" tabindex="-1"></a>  <span class="fu">points</span>(t, hinge, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, )</span>
<span id="cb150-13"><a href="support-vector-machines.html#cb150-13" tabindex="-1"></a>  <span class="fu">points</span>(t, logistic, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb150-14"><a href="support-vector-machines.html#cb150-14" tabindex="-1"></a>  <span class="fu">points</span>(t, modifiedhuber, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;deepskyblue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb150-15"><a href="support-vector-machines.html#cb150-15" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Zero-one&quot;</span>, <span class="st">&quot;Hinge&quot;</span>, <span class="st">&quot;Logistic&quot;</span>, <span class="st">&quot;Modified Huber&quot;</span>),</span>
<span id="cb150-16"><a href="support-vector-machines.html#cb150-16" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="st">&quot;darkorange&quot;</span>, <span class="st">&quot;deepskyblue&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>), </span>
<span id="cb150-17"><a href="support-vector-machines.html#cb150-17" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-226-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>For linear decision rules, with <span class="math inline">\(f(\mathbf{x}) = \beta_0 + \mathbf{x}^\text{T}\boldsymbol{\beta}\)</span>, this should be trivial to solve. However, we also want to consider nonlinear decision functions. But the above form does not contain a kernel function to use the kernel trick. The <strong>Representer Theorem</strong> <span class="citation">(<a href="#ref-kimeldorf1970correspondence">Kimeldorf and Wahba 1970</a>)</span> can help us in this case. This theorem was originally developed for in the setting of Chebyshev splines, but later on generalized. The theorem ensures that if we solve the function <span class="math inline">\(f\)</span> with regularization with respect to the norm in the RKHS induced from a kernel function <span class="math inline">\(K\)</span>, then the solution must admits a finite representation of the form (although the space <span class="math inline">\({\cal H}\)</span> we search for the solution is infinite):</p>
<p><span class="math display">\[\widehat{f}(\mathbf{x}) = \sum_{i = 1}^n \beta_i K(\mathbf{x}, \mathbf{x}_i).\]</span>
This suggests that the optimization problem becomes</p>
<p><span class="math display">\[\sum_{i=1}^n L(y_i, \mathbf{K}_i^\text{T}\boldsymbol{\beta}) + \lambda \boldsymbol{\beta}^\text{T}\mathbf{K}\boldsymbol{\beta},\]</span>
where <span class="math inline">\(\mathbf{K}_{n \times n}\)</span> is the kernel matrix with <span class="math inline">\(\mathbf{K}_{ij} = K(x_i, x_j)\)</span>, and <span class="math inline">\(\mathbf{K}_i\)</span> is the <span class="math inline">\(i\)</span> the column of <span class="math inline">\(\mathbf{K}\)</span>. This is an unconstrained optimization problem that can be solved using gradient decent if <span class="math inline">\(L\)</span> is differentiable. More details will be presented in the next Chapter.</p>
</div>
<div id="kernel-and-feature-maps-another-example" class="section level2 hasAnchor" number="17.8">
<h2><span class="header-section-number">17.8</span> Kernel and Feature Maps: Another Example<a href="support-vector-machines.html#kernel-and-feature-maps-another-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We give another example about the equivalence of kernel and the inner product of feature maps. Consider the Gaussian kernel <span class="math inline">\(e^{-\gamma \lVert \mathbf{x}- \mathbf{z}\rVert}\)</span>. We can write, using Tayler expansion,</p>
<p><span class="math display">\[\begin{align}
&amp;e^{\gamma \lVert \mathbf{x}- \mathbf{z}\rVert} \nonumber \\
=&amp; e^{-\gamma \lVert \mathbf{x}\rVert + 2 \gamma \mathbf{x}^\text{T}\mathbf{z}- \gamma \lVert \mathbf{z}\rVert} \nonumber \\
=&amp; e^{-\gamma \lVert \mathbf{x}\rVert - \gamma \lVert \mathbf{z}\rVert} \bigg[ 1 + \frac{2 \gamma \mathbf{x}^\text{T}\mathbf{z}}{1!} + \frac{(2 \gamma \mathbf{x}^\text{T}\mathbf{z})^2}{2!} + \frac{(2 \gamma \mathbf{x}^\text{T}\mathbf{z})^3}{3!} + \cdots \bigg]
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{x}^\text{T}\mathbf{z}\)</span> is the inner product of all first order feature maps. We also showed previously <span class="math inline">\((\mathbf{x}^\text{T}\mathbf{z})^2\)</span> is equivalent to the inner product of all second order feature maps (<span class="math inline">\(\Phi_2(\mathbf{x})\)</span>), and <span class="math inline">\((\mathbf{x}^\text{T}\mathbf{z})^3\)</span> would be equivalent to the third order version (<span class="math inline">\(\Phi_3(\mathbf{x})\)</span>), etc.. Hence, the previous equation can be written as the inner product of feature maps in the form of</p>
<p><span class="math display">\[e^{-\gamma \lVert \mathbf{x}\rVert} \bigg[ 1, \sqrt{\frac{2\gamma}{1!}} \mathbf{x}^\text{T}, \sqrt{\frac{(2\gamma)^2}{2!}} \Phi_2^\text{T}(\mathbf{x}), \sqrt{\frac{(2\gamma)^3}{3!}} \Phi_3^\text{T}(\mathbf{x}), \cdots \bigg]\]</span></p>
<p>This shows the Gaussian kernel is corresponding to all polynomials with a scaling factor of <span class="math inline">\(e^{-\gamma \lVert \mathbf{x}\rVert}\)</span></p>

<div style="display:none;">
<!-- Conflict \def\bf{\mathbf{f}} -->
</div>
</div>
</div>
<h3> Reference<a href="reference.html#reference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-boser1992training" class="csl-entry">
Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. <span>“A Training Algorithm for Optimal Margin Classifiers.”</span> In <em>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</em>, 144–52.
</div>
<div id="ref-cortes1995support" class="csl-entry">
Cortes, Corinna, and Vladimir Vapnik. 1995. <span>“Support-Vector Networks.”</span> <em>Machine Learning</em> 20 (3): 273–97.
</div>
<div id="ref-kimeldorf1970correspondence" class="csl-entry">
Kimeldorf, George S, and Grace Wahba. 1970. <span>“A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines.”</span> <em>The Annals of Mathematical Statistics</em> 41 (2): 495–502.
</div>
<div id="ref-mercer1909xvi" class="csl-entry">
Mercer, James. 1909. <span>“Xvi. Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations.”</span> <em>Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character</em> 209 (441-458): 415–46.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="kernel-ridge-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-representer-theorem.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "sepia",
    "family": "serif",
    "size": 1
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
