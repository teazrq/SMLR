<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 14 Classification and Regression Trees | Statistical Learning and Machine Learning with R</title>
  <meta name="description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 14 Classification and Regression Trees | Statistical Learning and Machine Learning with R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://teazrq.github.io/SMLR/" />
  
  <meta property="og:description" content="A textbook for STAT 542 and 432 at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 14 Classification and Regression Trees | Statistical Learning and Machine Learning with R" />
  
  <meta name="twitter:description" content="A textbook for STAT 542 and 432 at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2021-11-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="support-vector-machines.html"/>
<link rel="next" href="random-forests.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.19/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Learning and Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>3</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>3.1</b> Definition</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>4</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>4.1</b> Basic Concept</a></li>
<li class="chapter" data-level="4.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>4.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="4.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>4.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="4.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>4.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="4.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>4.5</b> Algorithm</a></li>
<li class="chapter" data-level="4.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>4.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>4.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="4.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>4.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>4.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>4.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="4.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>4.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>4.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>4.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>4.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>5</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>5.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>5.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="5.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>5.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>5.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>5.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>5.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>5.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="5.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>5.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>5.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>5.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="5.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>5.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>5.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>6</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>6.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="6.2" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>6.2</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="6.3" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>6.3</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="6.4" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>6.4</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>6.4.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="6.4.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>6.4.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>6.5</b> Cross-validation</a></li>
<li class="chapter" data-level="6.6" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.6</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>6.6.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>6.7</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>6.7.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>7</b> Lasso</a>
<ul>
<li class="chapter" data-level="7.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>7.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="7.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>7.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="7.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>7.3</b> The Solution Path</a></li>
<li class="chapter" data-level="7.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>7.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="7.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>7.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="7.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>7.6</b> Elastic-Net</a></li>
</ul></li>
<li class="part"><span><b>III Nonparametric Models</b></span></li>
<li class="chapter" data-level="8" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>8</b> Spline</a></li>
<li class="chapter" data-level="9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>9</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="9.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>9.1</b> Definition</a></li>
<li class="chapter" data-level="9.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>9.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="9.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>9.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="9.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>9.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="9.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>9.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="9.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>9.6</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="9.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>9.7</b> Distance Measures</a></li>
<li class="chapter" data-level="9.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>9.8</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="9.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>9.9</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="9.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>9.10</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>10</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="10.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>10.1</b> KNN vs. Kernel</a></li>
<li class="chapter" data-level="10.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>10.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="10.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>10.3</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="10.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>10.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off-1"><i class="fa fa-check"></i><b>10.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>10.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="10.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>10.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="10.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>10.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="10.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>10.8</b> R Implementations</a></li>
</ul></li>
<li class="part"><span><b>IV Classification Models</b></span></li>
<li class="chapter" data-level="11" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>11</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>11.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="11.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>11.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="11.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>11.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="11.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>11.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="11.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>11.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="11.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>11.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>12</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="12.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>12.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="12.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>12.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="12.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>12.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="12.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>12.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="12.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>12.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="12.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>12.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning Algorithms</b></span></li>
<li class="chapter" data-level="13" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>13</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="13.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>13.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="13.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>13.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>13.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>13.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="13.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>13.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="13.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>13.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="13.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>13.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="13.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>13.7</b> SVM as a Penalized Model</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>14</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="14.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>14.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="14.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>14.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="14.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>14.3</b> Regression Trees</a></li>
<li class="chapter" data-level="14.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>14.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="14.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>14.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>15</b> Random Forests</a>
<ul>
<li class="chapter" data-level="15.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>15.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="15.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>15.2</b> Random Forests</a></li>
<li class="chapter" data-level="15.3" data-path="random-forests.html"><a href="random-forests.html#effect-of-mtry"><i class="fa fa-check"></i><b>15.3</b> Effect of <code>mtry</code></a></li>
<li class="chapter" data-level="15.4" data-path="random-forests.html"><a href="random-forests.html#effect-of-nodesize"><i class="fa fa-check"></i><b>15.4</b> Effect of <code>nodesize</code></a></li>
<li class="chapter" data-level="15.5" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>15.5</b> Variable Importance</a></li>
<li class="chapter" data-level="15.6" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forets"><i class="fa fa-check"></i><b>15.6</b> Kernel view of Random Forets</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>16</b> Boosting</a></li>
<li class="part"><span><b>VI Unsupervised Learning</b></span></li>
<li class="chapter" data-level="17" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>17</b> K-Means</a>
<ul>
<li class="chapter" data-level="17.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>17.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="17.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>17.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="17.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>17.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="part"><span><b>VII Reference</b></span></li>
<li class="chapter" data-level="18" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>18</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2021 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Learning and Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-and-regression-trees" class="section level1" number="14">
<h1><span class="header-section-number">Chapter 14</span> Classification and Regression Trees</h1>
<p>A tree model is very simple to fit and enjoys interpretability. It is also the core component of random forest and boosting. Both trees and random forests can be used for classification and regression problems, although trees are not ideal for regressions problems due to its large bias. There are two main stream of tree models, Classification and Regression Trees (CART, <span class="citation"><a href="#ref-breiman1984classification" role="doc-biblioref">Breiman et al.</a> (<a href="#ref-breiman1984classification" role="doc-biblioref">1984</a>)</span>) and C4.5 <span class="citation">(<a href="#ref-quinlan1993c4" role="doc-biblioref">Quinlan 1993</a>)</span>, which is an improvement of the ID3 (Iterative Dichotomiser 3) algorithm. The main difference is to use binary or multiple splits and the criteria of the splitting rule. In fact the splitting rule criteria is probably the most essential part of a tree.</p>
<div id="example-classification-tree" class="section level2" number="14.1">
<h2><span class="header-section-number">14.1</span> Example: Classification Tree</h2>
<p>Let’s generate a model with nonlinear classification rule.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="classification-and-regression-trees.html#cb130-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb130-2"><a href="classification-and-regression-trees.html#cb130-2" aria-hidden="true" tabindex="-1"></a>    n <span class="ot">=</span> <span class="dv">500</span></span>
<span id="cb130-3"><a href="classification-and-regression-trees.html#cb130-3" aria-hidden="true" tabindex="-1"></a>    x1 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb130-4"><a href="classification-and-regression-trees.html#cb130-4" aria-hidden="true" tabindex="-1"></a>    x2 <span class="ot">=</span> <span class="fu">runif</span>(n, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb130-5"><a href="classification-and-regression-trees.html#cb130-5" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> <span class="fu">ifelse</span>(x1<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> x2<span class="sc">^</span><span class="dv">2</span> <span class="sc">&lt;</span> <span class="fl">0.6</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span>))</span>
<span id="cb130-6"><a href="classification-and-regression-trees.html#cb130-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb130-7"><a href="classification-and-regression-trees.html#cb130-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="dv">2</span>,<span class="dv">4</span>))</span>
<span id="cb130-8"><a href="classification-and-regression-trees.html#cb130-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(x1, x2, <span class="at">col =</span> <span class="fu">ifelse</span>(y <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&quot;deepskyblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb130-9"><a href="classification-and-regression-trees.html#cb130-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">symbols</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="at">circles =</span> <span class="fu">sqrt</span>(<span class="fl">0.6</span>), <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">inches =</span> <span class="cn">FALSE</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-188-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>A classification tree model is recursively splitting the feature space such that eventually each region is dominated by one class. We will use <code>rpart</code> as an example to fit trees, which stands for recursively partitioning.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="classification-and-regression-trees.html#cb131-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb131-2"><a href="classification-and-regression-trees.html#cb131-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(rpart)</span>
<span id="cb131-3"><a href="classification-and-regression-trees.html#cb131-3" aria-hidden="true" tabindex="-1"></a>    rpart.fit <span class="ot">=</span> <span class="fu">rpart</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y))</span>
<span id="cb131-4"><a href="classification-and-regression-trees.html#cb131-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb131-5"><a href="classification-and-regression-trees.html#cb131-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the tree structure    </span></span>
<span id="cb131-6"><a href="classification-and-regression-trees.html#cb131-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">mar=</span><span class="fu">rep</span>(<span class="fl">0.5</span>, <span class="dv">4</span>))</span>
<span id="cb131-7"><a href="classification-and-regression-trees.html#cb131-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(rpart.fit)</span>
<span id="cb131-8"><a href="classification-and-regression-trees.html#cb131-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">text</span>(rpart.fit)    </span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-189-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="classification-and-regression-trees.html#cb132-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-2"><a href="classification-and-regression-trees.html#cb132-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if you want to peek into the tree </span></span>
<span id="cb132-3"><a href="classification-and-regression-trees.html#cb132-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># note that we set cp = 0.041, which is a tuning parameter</span></span>
<span id="cb132-4"><a href="classification-and-regression-trees.html#cb132-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we will discuss this later</span></span>
<span id="cb132-5"><a href="classification-and-regression-trees.html#cb132-5" aria-hidden="true" tabindex="-1"></a>    rpart.fit<span class="sc">$</span>cptable</span>
<span id="cb132-6"><a href="classification-and-regression-trees.html#cb132-6" aria-hidden="true" tabindex="-1"></a><span class="do">##           CP nsplit rel error    xerror       xstd</span></span>
<span id="cb132-7"><a href="classification-and-regression-trees.html#cb132-7" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 0.17040359      0 1.0000000 1.0000000 0.04984280</span></span>
<span id="cb132-8"><a href="classification-and-regression-trees.html#cb132-8" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 0.14798206      3 0.4843049 0.7264574 0.04692735</span></span>
<span id="cb132-9"><a href="classification-and-regression-trees.html#cb132-9" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 0.01121076      4 0.3363229 0.4484305 0.04010884</span></span>
<span id="cb132-10"><a href="classification-and-regression-trees.html#cb132-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 0.01000000      7 0.3004484 0.4035874 0.03852329</span></span>
<span id="cb132-11"><a href="classification-and-regression-trees.html#cb132-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">prune</span>(rpart.fit, <span class="at">cp =</span> <span class="fl">0.041</span>)</span>
<span id="cb132-12"><a href="classification-and-regression-trees.html#cb132-12" aria-hidden="true" tabindex="-1"></a><span class="do">## n= 500 </span></span>
<span id="cb132-13"><a href="classification-and-regression-trees.html#cb132-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-14"><a href="classification-and-regression-trees.html#cb132-14" aria-hidden="true" tabindex="-1"></a><span class="do">## node), split, n, loss, yval, (yprob)</span></span>
<span id="cb132-15"><a href="classification-and-regression-trees.html#cb132-15" aria-hidden="true" tabindex="-1"></a><span class="do">##       * denotes terminal node</span></span>
<span id="cb132-16"><a href="classification-and-regression-trees.html#cb132-16" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb132-17"><a href="classification-and-regression-trees.html#cb132-17" aria-hidden="true" tabindex="-1"></a><span class="do">##  1) root 500 223 0 (0.55400000 0.44600000)  </span></span>
<span id="cb132-18"><a href="classification-and-regression-trees.html#cb132-18" aria-hidden="true" tabindex="-1"></a><span class="do">##    2) x2&lt; -0.6444322 90   6 0 (0.93333333 0.06666667) *</span></span>
<span id="cb132-19"><a href="classification-and-regression-trees.html#cb132-19" aria-hidden="true" tabindex="-1"></a><span class="do">##    3) x2&gt;=-0.6444322 410 193 1 (0.47073171 0.52926829)  </span></span>
<span id="cb132-20"><a href="classification-and-regression-trees.html#cb132-20" aria-hidden="true" tabindex="-1"></a><span class="do">##      6) x1&gt;=0.6941279 68   8 0 (0.88235294 0.11764706) *</span></span>
<span id="cb132-21"><a href="classification-and-regression-trees.html#cb132-21" aria-hidden="true" tabindex="-1"></a><span class="do">##      7) x1&lt; 0.6941279 342 133 1 (0.38888889 0.61111111)  </span></span>
<span id="cb132-22"><a href="classification-and-regression-trees.html#cb132-22" aria-hidden="true" tabindex="-1"></a><span class="do">##       14) x2&gt;=0.7484327 53   7 0 (0.86792453 0.13207547) *</span></span>
<span id="cb132-23"><a href="classification-and-regression-trees.html#cb132-23" aria-hidden="true" tabindex="-1"></a><span class="do">##       15) x2&lt; 0.7484327 289  87 1 (0.30103806 0.69896194)  </span></span>
<span id="cb132-24"><a href="classification-and-regression-trees.html#cb132-24" aria-hidden="true" tabindex="-1"></a><span class="do">##         30) x1&lt; -0.6903174 51   9 0 (0.82352941 0.17647059) *</span></span>
<span id="cb132-25"><a href="classification-and-regression-trees.html#cb132-25" aria-hidden="true" tabindex="-1"></a><span class="do">##         31) x1&gt;=-0.6903174 238  45 1 (0.18907563 0.81092437) *</span></span></code></pre></div>
<p>The model proceed with the following steps. Note that steps 5 and 6 may not be really beneficial (consider that we know the true model).</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-190-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p>Alternatively, there are many other packages that can perform the same analysis. For example, the <code>tree</code> package. However, be careful that this package uses a different splitting rule by default If you want to match the result, use <code>split = "gini"</code>. Note that this plot is very crowded because it will split until pretty much only one class in each terminal node. Hence, you can imaging that there will be a tuning parameter issue. We will discuss this later.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="classification-and-regression-trees.html#cb133-1" aria-hidden="true" tabindex="-1"></a>    <span class="fu">library</span>(tree)</span>
<span id="cb133-2"><a href="classification-and-regression-trees.html#cb133-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Registered S3 method overwritten by &#39;tree&#39;:</span></span>
<span id="cb133-3"><a href="classification-and-regression-trees.html#cb133-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   method     from</span></span>
<span id="cb133-4"><a href="classification-and-regression-trees.html#cb133-4" aria-hidden="true" tabindex="-1"></a><span class="do">##   print.tree cli</span></span>
<span id="cb133-5"><a href="classification-and-regression-trees.html#cb133-5" aria-hidden="true" tabindex="-1"></a>    tree.fit <span class="ot">=</span> <span class="fu">tree</span>(<span class="fu">as.factor</span>(y)<span class="sc">~</span>x1<span class="sc">+</span>x2, <span class="at">data =</span> <span class="fu">data.frame</span>(x1, x2, y), <span class="at">split =</span> <span class="st">&quot;gini&quot;</span>)</span>
<span id="cb133-6"><a href="classification-and-regression-trees.html#cb133-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">plot</span>(tree.fit)</span>
<span id="cb133-7"><a href="classification-and-regression-trees.html#cb133-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">text</span>(tree.fit)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-192-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="splitting-a-node" class="section level2" number="14.2">
<h2><span class="header-section-number">14.2</span> Splitting a Node</h2>
<p>In a tree model, the splitting mechanism performs in the following way, which is just comparing all possible splits on all variables. For simplicity, we will assume that a binary splitting rule is used, i.e., we split the current node into to two child nodes, and apply the procedure recursively.</p>
<ul>
<li>At the current node, go through each variable to find the best cut-off point that splits the node.</li>
<li>Compare all the best cut-off points across all variable and choose the best one to split the current node and then iterate.</li>
</ul>
<p>So, what error criterion should we use to compare different cut-off points? There are three of them at least:</p>
<ul>
<li>Gini impurity (CART)</li>
<li>Shannon entropy (C4.5)</li>
<li>Mis-classification error</li>
</ul>
<p>Gini impurity is used in CART, while ID3/C4.5 uses the Shannon entropy. These criteria have different effects than the mis-classifications error. They usually prefer more “pure” nodes, meaning that it is more likely to single out a set of pure class terminal node if we use Gini impurity and Shannon entropy. This is because their measures are nonlinear.</p>
<p>Suppose that we have a population (or a set of observations) with <span class="math inline">\(p_k\)</span> proportion of class <span class="math inline">\(k\)</span>, for <span class="math inline">\(k = 1, \ldots, K\)</span>. Then, the Gini impurity is given by</p>
<p><span class="math display">\[ \text{Gini} = \sum_{k = 1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2.\]</span>
The Shannon theory is defined as</p>
<p><span class="math display">\[- \sum_{k=1}^K p_k \log(p_k).\]</span>
And the classification error simply adds up all mis-classified portions if we predict the population into the most prevalent one:</p>
<p><span class="math display">\[ 1 - \underset{k = 1, \ldots, K}{\max} \,\, p_k\]</span>
The following plot shows all three quantities as a function of <span class="math inline">\(p\)</span>, when there are only two classes, i.e., <span class="math inline">\(K = 2\)</span>.</p>
<p><img src="SMLR_files/figure-html/unnamed-chunk-193-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>For each quantity, smaller value means that the node is more “pure,” hence, there is a higher certainty when we predict a new value. The idea of splitting a node is that, we want the two resulting child node to contain less variation. In other words, we want each child node to be as “pure” as possible. Hence, the idea is to calculate this error criterion both before and after the split and see what cut-off point gives us the best reduction of error. Of course, all of these quantities will be calculated based on the sample version, instead of the truth. For example, if we use the Gini impurity to compare different splits, we use the following quantity for an <strong>internal node</strong> <span class="math inline">\({\cal A}\)</span>:</p>
<p><span class="math display">\[\begin{align}
\text{score}(j, c) = \text{Gini}({\cal A}) - \left( \frac{N_{{\cal A}_L}}{N_{{\cal A}}} \text{Gini}({\cal A}_L) + \frac{N_{{\cal A}_R}}{N_{{\cal A}}} \text{Gini}({\cal A}_R)  \right).
\end{align}\]</span></p>
<p>Here, <span class="math inline">\({\cal A}_L\)</span> (left child node) and <span class="math inline">\({\cal A}_R\)</span> (right child node) denote the two child nodes resulted from a potential split on the <span class="math inline">\(j\)</span>th variable at a cut-off point <span class="math inline">\(c\)</span>, such that</p>
<p><span class="math display">\[{\cal A}_L = \{\mathbf{x}: \mathbf{x}\in {\cal A}, \, x_j \leq c\}\]</span>
and</p>
<p><span class="math display">\[{\cal A}_R = \{\mathbf{x}: \mathbf{x}\in {\cal A}, \, x_j &gt; c\}.\]</span>
Then <span class="math inline">\(N_{\cal A}\)</span>, <span class="math inline">\(N_{{\cal A}_L}\)</span>, <span class="math inline">\(N_{{\cal A}_R}\)</span> are the number of observations in these nodes, respectively. The implication of this is quite intuitive: <span class="math inline">\(\text{Gini}({\cal A})\)</span> calculates the uncertainty of the entire node <span class="math inline">\({\cal A}\)</span>, while the second quantity is a summary of the uncertainty of the two potential child nodes. Hence a larger score indicates a better split, and we may choose the best index <span class="math inline">\(j\)</span> and cut-off point <span class="math inline">\(c\)</span> to proceed,</p>
<p><span class="math display">\[\underset{j \, , \, c}{\mathop{\mathrm{arg\,max}}} \,\, \text{score}(j, c)\]</span></p>
<p>and then work on each child node separately using the same procedure.</p>
</div>
<div id="regression-trees" class="section level2" number="14.3">
<h2><span class="header-section-number">14.3</span> Regression Trees</h2>
<p>The basic procedure for a regression tree is pretty much the same as a classification tree, except that we will use a different way to evaluate how good a potential split is. Note that the variance is a simple quantity to describe the noise within a node, we can use</p>
<p><span class="math display">\[\begin{align}
\text{score}(j, c) = \text{Var}({\cal A}) - \left( \frac{N_{{\cal A}_L}}{N_{{\cal A}}} \text{Var}({\cal A}_L) + \frac{N_{{\cal A}_R}}{N_{{\cal A}}} \text{Var}({\cal A}_R)  \right).
\end{align}\]</span></p>
</div>
<div id="predicting-a-target-point" class="section level2" number="14.4">
<h2><span class="header-section-number">14.4</span> Predicting a Target Point</h2>
<p>When we have a new target point <span class="math inline">\(\mathbf{x}_0\)</span> to predict, the basic strategy is to “drop it down the tree.” This is simply starting from the root node and following the splitting rule to see which terminal node it ends up with. Note that a fitted tree will have a collection of terminal nodes, say, <span class="math inline">\(\{{\cal A}_1, {\cal A}_2, \ldots, {\cal A}_M\}\)</span>, then suppose <span class="math inline">\(\mathbf{x}_0\)</span> falls into terminal node <span class="math inline">\({\cal A}_m\)</span>, we use <span class="math inline">\(\bar{y}_{{\cal A}_m}\)</span>, the average of original training data that falls into this node, as the prediction. The final prediction can be written as</p>
<p><span class="math display">\[\begin{align}
\widehat{f}(\mathbf{x}_0) =&amp; \sum_{m = 1}^M \bar{y}_{{\cal A}_m} \mathbf{1}\{\mathbf{x}_0 \in {\cal A}_m\} \\
=&amp; \sum_{m = 1}^M \frac{\sum_{i=1}^n y_i \mathbf{1}\{\mathbf{x}_i \in {\cal A}_m\}}{\sum_{i=1}^n \mathbf{1}\{\mathbf{x}_i \in {\cal A}_m\}} \mathbf{1}\{\mathbf{x}_0 \in {\cal A}_m\}.
\end{align}\]</span></p>
</div>
<div id="tuning-a-tree-model" class="section level2" number="14.5">
<h2><span class="header-section-number">14.5</span> Tuning a Tree Model</h2>
<p>Tree tuning is essentially about when to stop splitting. Or we could look at this reversely by first fitting a very large tree, then see if we could remove some branches of a tree to make it simpler without sacrificing much accuracy. One approach is called the <strong>cost-complexity pruning</strong>. This is another penalized framework that we use the accuracy as the loss function, and use the tree-size as the penalty part for complexity. Formally, if we have any tree model <span class="math inline">\({\cal T}\)</span>, consider this can be written as</p>
<p><span class="math display">\[\begin{align}
C_\alpha({\cal T}) =&amp;~ \sum_{\text{all terminal nodes $t$ in ${\cal T}$}} N_t \cdot \text{Impurity}(t) + \alpha |{\cal T}| \nonumber \\
=&amp;~ C({\cal T}) + \alpha |{\cal T}|
\end{align}\]</span></p>
<p>Now, we can start with a very large tree, say, fitted until all pure terminal nodes. Call this tree as <span class="math inline">\({\cal T}_\text{max}\)</span>. We can then exhaust all its sub-trees by pruning any branches, and calculate this <span class="math inline">\(C(\cdot)\)</span> function of the sub-tree. Then the tree that gives the smallest value will be our best tree.</p>
<p>But this can be computationally too expensive. Hence, one compromise, instead of trying all possible sub-trees, is to use the <strong>weakest-link cutting</strong>. This means that, we cut the branch (essentially a certain split) that displays the weakest banefit towards the <span class="math inline">\(C(\cdot)\)</span> function. The procedure is the following:</p>
<ul>
<li>Look at an internal node <span class="math inline">\(t\)</span> of <span class="math inline">\({\cal T}_\text{max}\)</span>, and denote the entire branch starting from <span class="math inline">\(t\)</span> as <span class="math inline">\({\cal T}_t\)</span></li>
<li>Compare: remove the entire branch (collapse <span class="math inline">\({\cal T}_t\)</span> into a single terminal node) vs. keep <span class="math inline">\(T_t\)</span>. To do this, calculate
<span class="math display">\[\alpha \leq \frac{C(t) - C({\cal T}_t)}{|T_t| - 1}\]</span>
Note that <span class="math inline">\(|{\cal T}_t| - 1\)</span> is the size difference between the two trees.</li>
<li>Try all internal nodes <span class="math inline">\(t\)</span>, and cut the branch <span class="math inline">\(t\)</span> that has the smallest value on the right hand side. This gives the smallest <span class="math inline">\(\alpha\)</span> value to remove some branches. Then iterate the procedure based on this reduced tree.</li>
</ul>
<p>Note that the <span class="math inline">\(\alpha\)</span> values will get larger as we move more branches. Hence this produces a solution path. Now this is very similar to the Lasso solution path idea, and we could use cross-validation to select the best tuning. By default, the <code>rpart</code> function uses a 10-fold cross-validation. This can be controlled using the <code>rpart.control()</code> function and specify the <code>xval</code> argument. For details, please see the <a href="https://cran.r-project.org/web/packages/rpart/rpart.pdf">documentation</a>. The following plot using <code>plotcp()</code> in the <code>rpart</code> package gives a visualization of the relative cross-validation error. It also produces a horizontal line (the dotted line). It suggests the lowest (plus certain variation) that we could achieve. Hence, we will select the best <code>cp</code> value (<span class="math inline">\(alpha\)</span>) that is above this line. The way that this is constructed is similar to the <code>lambda.1se</code> choice in <code>glmnet</code>.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="classification-and-regression-trees.html#cb134-1" aria-hidden="true" tabindex="-1"></a>  <span class="co"># and the tuning parameter </span></span>
<span id="cb134-2"><a href="classification-and-regression-trees.html#cb134-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plotcp</span>(rpart.fit)  </span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-194-1.png" width="45%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="classification-and-regression-trees.html#cb135-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">printcp</span>(rpart.fit)</span>
<span id="cb135-2"><a href="classification-and-regression-trees.html#cb135-2" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb135-3"><a href="classification-and-regression-trees.html#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Classification tree:</span></span>
<span id="cb135-4"><a href="classification-and-regression-trees.html#cb135-4" aria-hidden="true" tabindex="-1"></a><span class="do">## rpart(formula = as.factor(y) ~ x1 + x2, data = data.frame(x1, </span></span>
<span id="cb135-5"><a href="classification-and-regression-trees.html#cb135-5" aria-hidden="true" tabindex="-1"></a><span class="do">##     x2, y))</span></span>
<span id="cb135-6"><a href="classification-and-regression-trees.html#cb135-6" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb135-7"><a href="classification-and-regression-trees.html#cb135-7" aria-hidden="true" tabindex="-1"></a><span class="do">## Variables actually used in tree construction:</span></span>
<span id="cb135-8"><a href="classification-and-regression-trees.html#cb135-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] x1 x2</span></span>
<span id="cb135-9"><a href="classification-and-regression-trees.html#cb135-9" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb135-10"><a href="classification-and-regression-trees.html#cb135-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Root node error: 223/500 = 0.446</span></span>
<span id="cb135-11"><a href="classification-and-regression-trees.html#cb135-11" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb135-12"><a href="classification-and-regression-trees.html#cb135-12" aria-hidden="true" tabindex="-1"></a><span class="do">## n= 500 </span></span>
<span id="cb135-13"><a href="classification-and-regression-trees.html#cb135-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb135-14"><a href="classification-and-regression-trees.html#cb135-14" aria-hidden="true" tabindex="-1"></a><span class="do">##         CP nsplit rel error  xerror     xstd</span></span>
<span id="cb135-15"><a href="classification-and-regression-trees.html#cb135-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 0.170404      0   1.00000 1.00000 0.049843</span></span>
<span id="cb135-16"><a href="classification-and-regression-trees.html#cb135-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 2 0.147982      3   0.48430 0.72646 0.046927</span></span>
<span id="cb135-17"><a href="classification-and-regression-trees.html#cb135-17" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 0.011211      4   0.33632 0.44843 0.040109</span></span>
<span id="cb135-18"><a href="classification-and-regression-trees.html#cb135-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 0.010000      7   0.30045 0.40359 0.038523</span></span></code></pre></div>

</div>
</div>
<h3> Reference</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-breiman1984classification" class="csl-entry">
Breiman, Leo, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 1984. <em>Classification and Regression Trees</em>. Monterey, CA: Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software.
</div>
<div id="ref-quinlan1993c4" class="csl-entry">
Quinlan, J Ross. 1993. <em>C4. 5: Programs for Machine Learning</em>. Elsevier.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="support-vector-machines.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="random-forests.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "serif",
"size": 1
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
