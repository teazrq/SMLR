<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Linear Regression and Model Selection | Statistical Machine Learning with R</title>
  <meta name="description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Linear Regression and Model Selection | Statistical Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Linear Regression and Model Selection | Statistical Machine Learning with R" />
  
  <meta name="twitter:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2025-10-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="optimization-basics.html"/>
<link rel="next" href="ridge-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visual-studio-code.html"><a href="visual-studio-code.html"><i class="fa fa-check"></i><b>3</b> Visual Studio Code</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visual-studio-code.html"><a href="visual-studio-code.html#basics-and-resources-1"><i class="fa fa-check"></i><b>3.1</b> Basics and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linear-regression"><i class="fa fa-check"></i><b>4.2</b> Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#matrix-inversion"><i class="fa fa-check"></i><b>4.3</b> Matrix Inversion</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linearalgebra-SM"><i class="fa fa-check"></i><b>4.3.1</b> Rank-one Update</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#rank-k-update"><i class="fa fa-check"></i><b>4.3.2</b> Rank-<span class="math inline">\(k\)</span> Update</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#times-2-block-matrix-inversion"><i class="fa fa-check"></i><b>4.3.3</b> 2 <span class="math inline">\(\times\)</span> 2 Block Matrix Inversion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>5</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>5.1</b> Basic Concept</a></li>
<li class="chapter" data-level="5.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>5.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="5.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>5.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="5.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>5.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="5.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>5.5</b> Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>5.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>5.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="5.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>5.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>5.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>5.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>5.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>5.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>6</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>6.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>6.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>6.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>6.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>6.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>6.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>6.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>6.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>6.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>6.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>6.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>6.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>7</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>7.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="7.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>7.2</b> Ridge Penalty and the Reduced Variation</a></li>
<li class="chapter" data-level="7.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>7.3</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="7.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>7.4</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="7.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>7.5</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>7.5.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="7.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>7.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>7.6</b> Cross-validation</a></li>
<li class="chapter" data-level="7.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.7</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>7.7.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>7.8</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>7.8.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8</b> Lasso</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>8.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>8.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="8.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>8.3</b> The Solution Path</a></li>
<li class="chapter" data-level="8.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>8.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="8.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>8.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="8.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>8.6</b> Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>9</b> Spline</a>
<ul>
<li class="chapter" data-level="9.1" data-path="spline.html"><a href="spline.html#using-linear-models-for-nonlinear-trends"><i class="fa fa-check"></i><b>9.1</b> Using Linear models for Nonlinear Trends</a></li>
<li class="chapter" data-level="9.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>9.2</b> A Motivating Example and Polynomials</a></li>
<li class="chapter" data-level="9.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>9.3</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="9.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>9.4</b> Splines</a></li>
<li class="chapter" data-level="9.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>9.5</b> Spline Basis</a></li>
<li class="chapter" data-level="9.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>9.6</b> Natural Cubic Spline</a></li>
<li class="chapter" data-level="9.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>9.7</b> Smoothing Spline</a></li>
<li class="chapter" data-level="9.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>9.8</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="9.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>9.9</b> Extending Splines to Multiple Varibles</a></li>
</ul></li>
<li class="part"><span><b>III Linear Classification Models</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>10.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>10.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>10.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>10.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>11</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>11.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="11.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>11.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="11.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>11.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="11.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>11.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="11.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>11.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>11.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Models</b></span></li>
<li class="chapter" data-level="12" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>12</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>12.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="12.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>12.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="12.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>12.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="12.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>12.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="12.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#degrees-of-freedom-1"><i class="fa fa-check"></i><b>12.6</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="12.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>12.7</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="12.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>12.8</b> Distance Measures</a></li>
<li class="chapter" data-level="12.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>12.9</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="12.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>12.10</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="12.11" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>12.11</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>13</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>13.1</b> KNN vs. Kernel</a></li>
<li class="chapter" data-level="13.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>13.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="13.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#expectation-of-the-parzen-estimator"><i class="fa fa-check"></i><b>13.3</b> Expectation of the Parzen estimator</a></li>
<li class="chapter" data-level="13.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>13.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>13.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>13.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="13.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>13.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="13.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>13.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="13.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>13.8</b> R Implementations</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>14</b> Nonparemetric Estimation Rates</a>
<ul>
<li class="chapter" data-level="14.1" data-path="nonpara.html"><a href="nonpara.html#kernel-density-estimation"><i class="fa fa-check"></i><b>14.1</b> Kernel Density Estimation</a></li>
<li class="chapter" data-level="14.2" data-path="nonpara.html"><a href="nonpara.html#the-effect-of-smoothness"><i class="fa fa-check"></i><b>14.2</b> The Effect of Smoothness</a></li>
<li class="chapter" data-level="14.3" data-path="nonpara.html"><a href="nonpara.html#the-effect-of-dimensionality"><i class="fa fa-check"></i><b>14.3</b> The Effect of Dimensionality</a></li>
<li class="chapter" data-level="14.4" data-path="nonpara.html"><a href="nonpara.html#nadaraya-watson-regression-estimator"><i class="fa fa-check"></i><b>14.4</b> Nadaraya-Watson Regression Estimator</a></li>
</ul></li>
<li class="part"><span><b>V Kernel Machines</b></span></li>
<li class="chapter" data-level="15" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>15</b> Reproducing Kernel Hilbert Space</a>
<ul>
<li class="chapter" data-level="15.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-motivation"><i class="fa fa-check"></i><b>15.1</b> The Motivation</a></li>
<li class="chapter" data-level="15.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#hilbert-space-preliminaries"><i class="fa fa-check"></i><b>15.2</b> Hilbert Space Preliminaries</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-space-of-square-integrable-functions"><i class="fa fa-check"></i><b>15.2.1</b> The Space of Square-Integrable Functions</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-kernel-function"><i class="fa fa-check"></i><b>15.3</b> A Kernel Function</a></li>
<li class="chapter" data-level="15.4" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-space-of-functions"><i class="fa fa-check"></i><b>15.4</b> A Space of Functions</a></li>
<li class="chapter" data-level="15.5" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-inner-product"><i class="fa fa-check"></i><b>15.5</b> The Inner Product</a></li>
<li class="chapter" data-level="15.6" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-rkhs"><i class="fa fa-check"></i><b>15.6</b> The RKHS</a></li>
<li class="chapter" data-level="15.7" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-reproducing-property"><i class="fa fa-check"></i><b>15.7</b> The Reproducing Property</a></li>
<li class="chapter" data-level="15.8" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#smoothness"><i class="fa fa-check"></i><b>15.8</b> Smoothness</a></li>
<li class="chapter" data-level="15.9" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-moorearonszajn-theorem"><i class="fa fa-check"></i><b>15.9</b> The Moore–Aronszajn Theorem</a></li>
<li class="chapter" data-level="15.10" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#examples"><i class="fa fa-check"></i><b>15.10</b> Examples</a>
<ul>
<li class="chapter" data-level="15.10.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#brownian-motion-kernel"><i class="fa fa-check"></i><b>15.10.1</b> Brownian Motion Kernel</a></li>
<li class="chapter" data-level="15.10.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#non-positive-definite-kernel"><i class="fa fa-check"></i><b>15.10.2</b> Non-positive Definite Kernel</a></li>
<li class="chapter" data-level="15.10.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#defining-new-kernels"><i class="fa fa-check"></i><b>15.10.3</b> Defining New Kernels</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>16</b> Kernel Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#linear-regression-as-a-constraint-optimization"><i class="fa fa-check"></i><b>16.1</b> Linear Regression as a Constraint Optimization</a></li>
<li class="chapter" data-level="16.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#the-kernel-ridge-regression"><i class="fa fa-check"></i><b>16.2</b> The Kernel Ridge Regression</a></li>
<li class="chapter" data-level="16.3" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#ridge-regression-as-a-linear-kernel-model"><i class="fa fa-check"></i><b>16.3</b> Ridge Regression as a Linear Kernel Model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>17</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="17.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>17.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="17.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>17.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>17.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>17.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="17.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>17.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="17.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>17.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="17.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>17.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="17.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>17.7</b> SVM as a Penalized Model</a></li>
<li class="chapter" data-level="17.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>17.8</b> Kernel and Feature Maps: Another Example</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html"><i class="fa fa-check"></i><b>18</b> The Representer Theorem</a>
<ul>
<li class="chapter" data-level="18.1" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#the-representer-theorem-1"><i class="fa fa-check"></i><b>18.1</b> The Representer Theorem</a></li>
<li class="chapter" data-level="18.2" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#notes-on-application"><i class="fa fa-check"></i><b>18.2</b> Notes on Application</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="support-vector-regression.html"><a href="support-vector-regression.html"><i class="fa fa-check"></i><b>19</b> Support Vector Regression</a>
<ul>
<li class="chapter" data-level="19.1" data-path="support-vector-regression.html"><a href="support-vector-regression.html#the-epsilon-insensitive-loss"><i class="fa fa-check"></i><b>19.1</b> The <span class="math inline">\(\epsilon\)</span>-insensitive Loss</a></li>
<li class="chapter" data-level="19.2" data-path="support-vector-regression.html"><a href="support-vector-regression.html#primal-and-dual-formulation-of-svr"><i class="fa fa-check"></i><b>19.2</b> Primal and Dual Formulation of SVR</a></li>
<li class="chapter" data-level="19.3" data-path="support-vector-regression.html"><a href="support-vector-regression.html#penalized-svr-with-rkhs"><i class="fa fa-check"></i><b>19.3</b> Penalized SVR with RKHS</a></li>
</ul></li>
<li class="part"><span><b>VI Trees and Ensembles</b></span></li>
<li class="chapter" data-level="20" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>20</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="20.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>20.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="20.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>20.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="20.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>20.3</b> Regression Trees</a></li>
<li class="chapter" data-level="20.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>20.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="20.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>20.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>21</b> Random Forests</a>
<ul>
<li class="chapter" data-level="21.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>21.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="21.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>21.2</b> Random Forests</a></li>
<li class="chapter" data-level="21.3" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forests"><i class="fa fa-check"></i><b>21.3</b> Kernel view of Random Forests</a></li>
<li class="chapter" data-level="21.4" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>21.4</b> Variable Importance</a></li>
<li class="chapter" data-level="21.5" data-path="random-forests.html"><a href="random-forests.html#adaptiveness-of-random-forest-kernel"><i class="fa fa-check"></i><b>21.5</b> Adaptiveness of Random Forest Kernel</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="adaboost.html"><a href="adaboost.html"><i class="fa fa-check"></i><b>22</b> AdaBoost</a>
<ul>
<li class="chapter" data-level="22.1" data-path="adaboost.html"><a href="adaboost.html#the-algorithm"><i class="fa fa-check"></i><b>22.1</b> The Algorithm</a></li>
<li class="chapter" data-level="22.2" data-path="adaboost.html"><a href="adaboost.html#training-error-bound"><i class="fa fa-check"></i><b>22.2</b> Training Error Bound</a></li>
<li class="chapter" data-level="22.3" data-path="adaboost.html"><a href="adaboost.html#the-stagewise-additive-model-and-probability-calibration"><i class="fa fa-check"></i><b>22.3</b> The Stagewise Additive Model and Probability Calibration</a></li>
<li class="chapter" data-level="22.4" data-path="adaboost.html"><a href="adaboost.html#tuning-the-number-of-trees"><i class="fa fa-check"></i><b>22.4</b> Tuning the Number of Trees</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html"><i class="fa fa-check"></i><b>23</b> Gradient Boosting Machines</a>
<ul>
<li class="chapter" data-level="23.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#motivation-lasso-as-boosting"><i class="fa fa-check"></i><b>23.1</b> Motivation: Lasso as Boosting</a></li>
<li class="chapter" data-level="23.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gradient-boosting"><i class="fa fa-check"></i><b>23.2</b> Gradient Boosting</a></li>
<li class="chapter" data-level="23.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gradient-boosting-with-general-loss"><i class="fa fa-check"></i><b>23.3</b> Gradient Boosting with General Loss</a></li>
<li class="chapter" data-level="23.4" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#logistic-link"><i class="fa fa-check"></i><b>23.4</b> Logistic Link</a></li>
<li class="chapter" data-level="23.5" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#xgboost"><i class="fa fa-check"></i><b>23.5</b> xgboost</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Learning</b></span></li>
<li class="chapter" data-level="24" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>24</b> K-Means</a>
<ul>
<li class="chapter" data-level="24.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>24.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="24.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>24.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="24.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>24.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>25</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="25.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>25.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="25.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>25.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="25.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>25.3</b> Example 2: RNA Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>26</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="26.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>26.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="26.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>26.1.1</b> Note: Scaling</a></li>
</ul></li>
<li class="chapter" data-level="26.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>26.2</b> Example 1: <code>iris</code> Data</a></li>
<li class="chapter" data-level="26.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>26.3</b> Example 2: Handwritten Digits</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>27</b> Self-Organizing Map</a>
<ul>
<li class="chapter" data-level="27.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>27.1</b> Basic Concepts</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>28</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="28.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#an-example"><i class="fa fa-check"></i><b>28.1</b> An Example</a></li>
<li class="chapter" data-level="28.2" data-path="spectral-clustering.html"><a href="spectral-clustering.html#adjacency-matrix"><i class="fa fa-check"></i><b>28.2</b> Adjacency Matrix</a></li>
<li class="chapter" data-level="28.3" data-path="spectral-clustering.html"><a href="spectral-clustering.html#laplacian-matrix"><i class="fa fa-check"></i><b>28.3</b> Laplacian Matrix</a></li>
<li class="chapter" data-level="28.4" data-path="spectral-clustering.html"><a href="spectral-clustering.html#derivation-of-the-feature-embedding"><i class="fa fa-check"></i><b>28.4</b> Derivation of the Feature Embedding</a></li>
<li class="chapter" data-level="28.5" data-path="spectral-clustering.html"><a href="spectral-clustering.html#feature-embedding"><i class="fa fa-check"></i><b>28.5</b> Feature Embedding</a></li>
<li class="chapter" data-level="28.6" data-path="spectral-clustering.html"><a href="spectral-clustering.html#clustering-with-embedded-features"><i class="fa fa-check"></i><b>28.6</b> Clustering with Embedded Features</a></li>
<li class="chapter" data-level="28.7" data-path="spectral-clustering.html"><a href="spectral-clustering.html#normalized-graph-laplacian"><i class="fa fa-check"></i><b>28.7</b> Normalized Graph Laplacian</a></li>
<li class="chapter" data-level="28.8" data-path="spectral-clustering.html"><a href="spectral-clustering.html#using-a-different-adjacency-matrix"><i class="fa fa-check"></i><b>28.8</b> Using a Different Adjacency Matrix</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html"><i class="fa fa-check"></i><b>29</b> Uniform Manifold Approximation and Projection</a>
<ul>
<li class="chapter" data-level="29.1" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#an-example-1"><i class="fa fa-check"></i><b>29.1</b> An Example</a></li>
<li class="chapter" data-level="29.2" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#tuning"><i class="fa fa-check"></i><b>29.2</b> Tuning</a></li>
<li class="chapter" data-level="29.3" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#another-example"><i class="fa fa-check"></i><b>29.3</b> Another Example</a></li>
</ul></li>
<li class="part"><span><b>VIII Reference</b></span></li>
<li class="chapter" data-level="30" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>30</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2023 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression-and-model-selection" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Linear Regression and Model Selection<a href="linear-regression-and-model-selection.html#linear-regression-and-model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldn’t find these concepts too difficult to understand. Secondly, we will mainly use the <code>lm()</code> function as an example to demonstrate some features of <code>R</code>. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. Finally, we will introduce several model selection criteria and algorithms to perform model selection.</p>
<div id="example-real-estate-data" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Example: real estate data<a href="linear-regression-and-model-selection.html#example-real-estate-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This <a href="https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set">Real Estate data</a> <span class="citation">(<a href="#ref-yeh2018building">Yeh and Hsu 2018</a>)</span> is provided on the <a href="https://archive.ics.uci.edu/ml/index.php">UCI machine learning repository</a>. The goal of this dataset is to predict the unit house price based on six different covariates:</p>
<ul>
<li><code>date</code>: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.)</li>
<li><code>age</code>: The house age (unit: year)</li>
<li><code>distance</code>: The distance to the nearest MRT station (unit: meter)</li>
<li><code>stores</code>: The number of convenience stores in the living circle on foot (integer)</li>
<li><code>latitude</code>: Latitude (unit: degree)</li>
<li><code>longitude</code>: Longitude (unit: degree)</li>
<li><code>price</code>: House price of unit area</li>
</ul>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="linear-regression-and-model-selection.html#cb46-1" tabindex="-1"></a>    realestate <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/realestate.csv&quot;</span>, <span class="at">row.names =</span> <span class="dv">1</span>)</span>
<span id="cb46-2"><a href="linear-regression-and-model-selection.html#cb46-2" tabindex="-1"></a></span>
<span id="cb46-3"><a href="linear-regression-and-model-selection.html#cb46-3" tabindex="-1"></a>    <span class="fu">library</span>(DT)</span>
<span id="cb46-4"><a href="linear-regression-and-model-selection.html#cb46-4" tabindex="-1"></a>    <span class="fu">datatable</span>(realestate, <span class="at">filter =</span> <span class="st">&quot;top&quot;</span>, <span class="at">rownames =</span> <span class="cn">FALSE</span>,</span>
<span id="cb46-5"><a href="linear-regression-and-model-selection.html#cb46-5" tabindex="-1"></a>              <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">8</span>))</span></code></pre></div>
<div class="datatables html-widget html-fill-item" id="htmlwidget-7e293be7c01b4d1e0b5c" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-7e293be7c01b4d1e0b5c">{"x":{"filter":"top","vertical":false,"filterHTML":"<tr>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"2012.667\" data-max=\"2013.583\" data-scale=\"3\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"0\" data-max=\"43.8\" data-scale=\"1\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"23.38284\" data-max=\"6488.021\" data-scale=\"5\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"integer\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"0\" data-max=\"10\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"24.93207\" data-max=\"25.01459\" data-scale=\"5\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"121.47353\" data-max=\"121.56627\" data-scale=\"5\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none;position: absolute;width: 200px;opacity: 1\">\n      <div data-min=\"7.6\" data-max=\"117.5\" data-scale=\"1\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n<\/tr>","data":[[2012.917,2012.917,2013.583,2013.5,2012.833,2012.667,2012.667,2013.417,2013.5,2013.417,2013.083,2013.333,2012.917,2012.667,2013.5,2013.583,2013.25,2012.75,2013.417,2012.667,2013.417,2013.417,2012.917,2013.083,2013,2013.083,2012.667,2013.25,2013.5,2013.083,2013.5,2012.75,2012.75,2013.25,2012.75,2013.5,2012.917,2013.167,2012.667,2013.167,2013,2013.5,2013.417,2012.75,2013.583,2013.083,2013.417,2013.583,2013.417,2012.667,2013.417,2013.083,2013.583,2013.083,2013.083,2012.833,2013.417,2012.917,2013.5,2013.083,2013.417,2013.5,2012.917,2013.583,2013.333,2013.417,2013,2013.5,2013.417,2012.833,2013.583,2013.083,2013.583,2013.167,2012.917,2013.5,2013.583,2012.833,2012.917,2013,2013.5,2013,2013.083,2012.917,2013.083,2012.75,2012.833,2013.583,2012.917,2013.5,2012.833,2013.25,2012.917,2012.917,2012.917,2012.917,2013.417,2013.083,2013.417,2013.417,2013.5,2012.833,2013.083,2012.75,2012.667,2012.833,2013.083,2013.333,2013.417,2013.583,2013.083,2013.583,2013.417,2013.333,2012.667,2013.083,2013,2013,2013.5,2013.5,2013.167,2013.5,2013.25,2013.417,2012.917,2013.167,2013.083,2013.25,2013.083,2013.417,2013.25,2013.5,2013.167,2012.833,2012.667,2012.917,2012.75,2013.5,2013.167,2012.667,2013.25,2013.333,2013.417,2013.5,2013.083,2012.917,2012.75,2012.75,2013.5,2012.667,2013.25,2013.5,2013.333,2013.25,2013.5,2013.167,2013.583,2013.25,2013,2012.667,2012.917,2013.417,2012.75,2013.5,2012.833,2012.917,2013.417,2013.417,2013.083,2013.417,2013.333,2013.083,2013.583,2013.083,2013.417,2013.083,2012.833,2013.083,2013.5,2013.083,2012.667,2013.167,2013.5,2013.5,2012.75,2012.75,2013.167,2013,2012.917,2012.917,2013.5,2013.167,2013.167,2013.417,2013.5,2013.333,2013,2013.25,2013.083,2013.417,2013.417,2013.417,2012.917,2012.667,2013,2013.083,2013.25,2013.083,2012.75,2012.833,2013.5,2013.083,2013.333,2013.083,2013.583,2013.333,2013.25,2012.917,2013.417,2012.75,2013.333,2013.333,2013.583,2013.25,2013.333,2013.25,2013,2012.917,2013.417,2013.583,2013.5,2012.833,2012.917,2013.333,2013.25,2012.75,2013.167,2013.167,2013.083,2013.5,2013.083,2013.5,2012.833,2013.417,2013.083,2013.417,2013.417,2013.333,2013,2012.833,2013.167,2012.917,2012.833,2012.667,2012.667,2013.417,2012.667,2013.25,2013.417,2013.083,2013.25,2013.167,2012.917,2013.417,2013.167,2012.833,2013.25,2012.833,2013.417,2013,2013.333,2012.917,2012.75,2013.417,2013.167,2012.667,2013,2013.417,2012.75,2013.417,2013.25,2013.333,2012.917,2013.417,2012.917,2013.167,2012.917,2013,2013.583,2013.333,2013.083,2012.833,2013.083,2012.667,2013.5,2013.167,2012.75,2012.833,2013.333,2013.167,2013.083,2012.75,2013.5,2013.5,2013.417,2013.083,2013.5,2012.833,2013.417,2013.25,2013.583,2013.167,2013.583,2013.333,2013.25,2013.083,2013.25,2012.75,2013.333,2013.25,2012.75,2012.917,2013,2013.417,2012.667,2013.083,2013.5,2013.417,2012.833,2013,2013.083,2013.333,2013.167,2012.75,2012.917,2013.583,2012.833,2012.833,2012.917,2013.333,2013.333,2013,2012.667,2013,2013.5,2012.667,2013.417,2013.583,2012.833,2012.75,2013,2012.833,2012.833,2013.5,2013.417,2013.25,2012.833,2013.417,2013.167,2013.5,2012.667,2013.083,2013.417,2013.5,2013.417,2012.917,2012.75,2012.833,2013.417,2012.667,2012.75,2013.5,2013,2013.083,2013.25,2013.25,2013.417,2013.333,2013.333,2013.333,2013.333,2013.417,2013,2012.667,2012.75,2013,2012.833,2013.25,2013.5,2013.25,2013.5,2013.583,2013.083,2013,2013.5,2012.917,2012.667,2013.417,2013.417,2012.917,2013.25,2013.083,2012.833,2012.667,2013.333,2012.667,2013.167,2013,2013.417,2013,2012.667,2013.25,2013,2013.5],[32,19.5,13.3,13.3,5,7.1,34.5,20.3,31.7,17.9,34.8,6.3,13,20.4,13.2,35.7,0,17.7,16.9,1.5,4.5,10.5,14.7,10.1,39.6,29.3,3.1,10.4,19.2,7.1,25.9,29.6,37.9,16.5,15.4,13.9,14.7,12,3.1,16.2,13.6,16.8,36.1,34.4,2.7,36.6,21.7,35.9,24.2,29.4,21.7,31.3,32.1,13.3,16.1,31.7,33.6,3.5,30.3,13.3,11,5.3,17.2,2.6,17.5,40.1,1,8.5,30.4,12.5,6.6,35.5,32.5,13.8,6.8,12.3,35.9,20.5,38.2,18,11.8,30.8,13.2,25.3,15.1,0,1.8,16.9,8.9,23,0,9.1,20.6,31.9,40.9,8,6.4,28.4,16.4,6.4,17.5,12.7,1.1,0,32.7,0,17.2,12.2,31.4,4,8.1,33.3,9.9,14.8,30.6,20.6,30.9,13.6,25.3,16.6,13.3,13.6,31.5,0,9.9,1.1,38.6,3.8,41.3,38.5,29.6,4,26.6,18,33.4,18.9,11.4,13.6,10,12.9,16.2,5.1,19.8,13.6,11.9,2.1,0,3.2,16.4,34.9,35.8,4.9,12,6.5,16.9,13.8,30.7,16.1,11.6,15.5,3.5,19.2,16,8.5,0,13.7,0,28.2,27.6,8.4,24,3.6,6.6,41.3,4.3,30.2,13.9,33,13.1,14,26.9,11.6,13.5,17,14.1,31.4,20.9,8.9,34.8,16.3,35.3,13.2,43.8,9.699999999999999,15.2,15.2,22.8,34.4,34,18.2,17.4,13.1,38.3,15.6,18,12.8,22.2,38.5,11.5,34.8,5.2,0,17.6,6.2,18.1,19.2,37.8,28,13.6,29.3,37.2,9,30.6,9.1,34.5,1.1,16.5,32.4,11.9,31,4,16.2,27.1,39.7,8,12.9,3.6,13,12.8,18.1,11,13.7,2,32.8,4.8,7.5,16.4,21.7,19,18,39.2,31.7,5.9,30.4,1.1,31.5,14.6,17.3,0,17.7,17,16.2,15.9,3.9,32.6,15.7,17.8,34.7,17.2,17.6,10.8,17.7,13,13.2,27.5,1.5,19.1,21.2,0,2.6,2.3,4.7,2,33.5,15,30.1,5.9,19.2,16.6,13.9,37.7,3.4,17.5,12.6,26.4,18.2,12.5,34.9,16.7,33.2,2.5,38,16.5,38.3,20,16.2,14.4,10.3,16.4,30.3,16.4,21.3,35.4,8.300000000000001,3.7,15.6,13.3,15.6,7.1,34.6,13.5,16.9,12.9,28.6,12.4,36.6,4.1,3.5,15.9,13.6,32,25.6,39.8,7.8,30,27.3,5.1,31.3,31.5,1.7,33.6,13,5.7,33.5,34.6,0,13.2,17.4,4.6,7.8,13.2,4,18.4,4.1,12.2,3.8,10.3,0,1.1,5.6,32.9,41.4,17.1,32.3,35.3,17.3,14.2,15,18.2,20.2,15.9,4.1,33.9,0,5.4,21.7,14.7,3.9,37.3,0,14.1,8,16.3,29.1,16.1,18.3,0,16.2,10.4,40.9,32.8,6.2,42.7,16.9,32.6,21.2,37.1,13.1,14.7,12.7,26.8,7.6,12.7,30.9,16.4,23,1.9,5.2,18.5,13.7,5.6,18.8,8.1,6.5],[84.87882,306.5947,561.9845,561.9845,390.5684,2175.03,623.4731,287.6025,5512.038,1783.18,405.2134,90.45605999999999,492.2313,2469.645,1164.838,579.2083,292.9978,350.8515,368.1363,23.38284,2275.877,279.1726,1360.139,279.1726,480.6977,1487.868,383.8624,276.449,557.478,451.2438,4519.69,769.4034,488.5727,323.655,205.367,4079.418,1935.009,1360.139,577.9615,289.3248,4082.015,4066.587,519.4617,512.7871,533.4761999999999,488.8193,463.9623,640.7391,4605.749,4510.359,512.5487000000001,1758.406,1438.579,492.2313,289.3248,1160.632,371.2495,56.47425,4510.359,336.0532,1931.207,259.6607,2175.877,533.4761999999999,995.7554,123.7429,193.5845,104.8101,464.223,561.9845,90.45605999999999,640.7391,424.5442,4082.015,379.5575,1360.139,616.4004,2185.128,552.4371,1414.837,533.4761999999999,377.7956,150.9347,2707.392,383.2805,338.9679,1455.798,4066.587,1406.43,3947.945,274.0144,1402.016,2469.645,1146.329,167.5989,104.8101,90.45605999999999,617.4424,289.3248,90.45605999999999,964.7496,170.1289,193.5845,208.3905,392.4459,292.9978,189.5181,1360.139,592.5006,2147.376,104.8101,196.6172,2102.427,393.2606,143.8383,737.9161,6396.283,4197.349,1583.722,289.3248,492.2313,492.2313,414.9476,185.4296,279.1726,193.5845,804.6897,383.8624,124.9912,216.8329,535.527,2147.376,482.7581,373.3937,186.9686,1009.235,390.5684,319.0708,942.4664,492.2313,289.3248,1559.827,640.6070999999999,492.2313,1360.139,451.2438,185.4296,489.8821,3780.59,179.4538,170.7311,387.7721,1360.139,376.1709,4066.587,4082.015,1264.73,815.9314000000001,390.5684,815.9314000000001,49.66105,616.4004,4066.587,104.8101,185.4296,1236.564,292.9978,330.0854,515.1122,1962.628,4527.687,383.8624,90.45605999999999,401.8807,432.0385,472.1745,4573.779,181.0766,1144.436,438.8513,4449.27,201.8939,2147.376,4082.015,2615.465,1447.286,2185.128,3078.176,190.0392,4066.587,616.5735,750.0703999999999,57.58945,421.479,3771.895,461.1016,707.9067,126.7286,157.6052,451.6419,995.7554,561.9845,642.6985,289.3248,1414.837,1449.722,379.5575,665.0636,1360.139,175.6294,390.5684,274.0144,1805.665,90.45605999999999,1783.18,383.7129,590.9292,372.6242,492.2313,529.7771,186.5101,1402.016,431.1114,1402.016,324.9419,193.5845,4082.015,265.0609,3171.329,1156.412,2147.376,4074.736,4412.765,333.3679,2216.612,250.631,373.8389,732.8528,732.8528,837.7233,1712.632,250.631,2077.39,204.1705,1559.827,639.6198000000001,389.8219,1055.067,1009.235,6306.153,424.7132,1159.454,90.45605999999999,1735.595,329.9747,5512.038,339.2289,444.1334,292.9978,837.7233,1485.097,2288.011,289.3248,2147.376,493.657,815.9314000000001,1783.18,482.7581,390.5684,837.7233,252.5822,451.6419,492.2313,170.1289,394.0173,23.38284,461.1016,2185.128,208.3905,1554.25,184.3302,387.7721,1455.798,1978.671,383.2805,718.2936999999999,90.45605999999999,461.1016,323.6912,289.3248,490.3446,56.47425,395.6747,383.2805,335.5273,2179.59,1144.436,567.0349,4082.015,121.7262,156.2442,461.7848,2288.011,439.7105,1626.083,289.3248,169.9803,3079.89,289.3248,1264.73,1643.499,537.7971,318.5292,104.8101,577.9615,1756.411,250.631,752.7669,379.5575,272.6783,4197.349,964.7496,187.4823,197.1338,1712.632,488.8193,56.47425,757.3377,1497.713,4197.349,1156.777,4519.69,617.7134,104.8101,1013.341,337.6016,1867.233,600.8604,258.186,329.9747,270.8895,750.0703999999999,90.45605999999999,563.2854,3085.17,185.4296,1712.632,6488.021,259.6607,104.8101,492.2313,2180.245,2674.961,2147.376,1360.139,383.8624,211.4473,338.9679,193.5845,2408.993,87.30222000000001,281.205,967.4,109.9455,614.1394,2261.432,1801.544,1828.319,350.8515,2185.128,289.3248,312.8963,157.6052,274.0144,390.5684,1157.988,1717.193,49.66105,587.8877,292.9978,289.3248,132.5469,3529.564,506.1144,4066.587,82.88643,185.4296,2103.555,2251.938,122.3619,377.8302,1939.749,443.802,967.4,4136.271,512.5487000000001,918.6357,1164.838,1717.193,170.1289,482.7581,2175.03,187.4823,161.942,289.3248,130.9945,372.1386,2408.993,2175.744,4082.015,90.45605999999999,390.9696,104.8101,90.45605999999999],[10,9,5,5,5,3,7,6,1,3,1,9,5,4,4,2,6,1,8,7,3,7,1,7,4,2,5,5,4,5,0,7,1,6,7,0,2,1,6,5,0,0,5,6,4,8,9,3,0,1,4,1,3,5,5,0,8,7,1,5,2,6,3,4,0,8,6,5,6,5,9,3,8,0,10,1,3,3,2,1,4,6,7,3,7,9,1,0,0,0,1,0,4,0,5,5,9,3,5,9,4,1,6,6,6,6,8,1,2,3,5,7,3,6,8,2,1,0,3,5,5,5,4,0,7,6,4,5,6,7,8,3,5,8,6,0,5,6,0,5,5,3,5,5,1,5,0,8,0,8,7,9,1,6,0,0,0,4,5,4,8,3,0,5,0,1,6,8,5,1,0,5,9,4,7,3,0,9,4,1,0,8,3,0,0,3,3,0,8,0,8,2,7,5,0,5,2,8,7,8,0,5,3,5,1,3,10,3,1,8,5,1,2,9,3,8,1,6,5,8,9,0,10,0,6,6,0,8,0,0,3,0,1,9,4,7,10,0,0,0,2,7,3,8,3,5,6,0,0,1,7,0,9,2,5,1,1,1,6,0,4,3,5,3,7,4,3,5,5,0,1,8,5,1,7,7,5,3,6,3,6,9,1,2,7,3,9,5,6,5,0,7,5,7,6,3,4,4,0,10,4,0,3,0,3,5,1,0,5,0,2,4,9,5,6,2,7,2,10,5,0,4,1,6,2,8,7,3,3,0,0,0,2,5,5,6,2,5,9,5,0,2,9,8,0,0,2,1,6,5,5,3,3,3,1,5,1,9,6,0,10,8,4,10,7,4,1,2,1,3,5,5,7,1,5,0,2,8,8,6,5,9,0,4,0,10,0,3,4,8,9,1,6,4,1,4,1,4,2,1,5,3,1,9,5,6,7,0,3,0,9,7,5,9],[24.98298,24.98034,24.98746,24.98746,24.97937,24.96305,24.97933,24.98042,24.95095,24.96731,24.97349,24.97433,24.96515,24.96108,24.99156,24.9824,24.97744,24.97544,24.9675,24.96772,24.96314,24.97528,24.95204,24.97528,24.97353,24.97542,24.98085,24.95593,24.97419,24.97563,24.94826,24.98281,24.97349,24.97841,24.98419,25.01459,24.96386,24.95204,24.97201,24.98203,24.94155,24.94297,24.96305,24.98748,24.97445,24.97015,24.9703,24.97563,24.94684,24.94925,24.974,24.95402,24.97419,24.96515,24.98203,24.94968,24.97254,24.95744,24.94925,24.95776,24.96365,24.97585,24.96303,24.97445,24.96305,24.97635,24.96571,24.96674,24.97964,24.98746,24.97433,24.97563,24.97587,24.94155,24.98343,24.95204,24.97723,24.96322,24.97598,24.95182,24.97445,24.96427,24.96725,24.96056,24.96735,24.96853,24.9512,24.94297,24.98573,24.94783,24.9748,24.98569,24.96108,24.9492,24.9663,24.96674,24.97433,24.97746,24.98203,24.97433,24.98872,24.97371,24.96571,24.95618,24.96398,24.97744,24.97707,24.95204,24.9726,24.96299,24.96674,24.97701,24.96044,24.96172,24.98155,24.98092,24.94375,24.93885,24.96622,24.98203,24.96515,24.96515,24.98199,24.9711,24.97528,24.96571,24.97838,24.98085,24.96674,24.98086,24.98092,24.96299,24.97433,24.9866,24.96604,24.96357,24.97937,24.96495,24.97843,24.96515,24.98203,24.97213,24.97017,24.96515,24.95204,24.97563,24.9711,24.97017,24.93293,24.97349,24.96719,24.98118,24.95204,24.95418,24.94297,24.94155,24.94883,24.97886,24.97937,24.97886,24.95836,24.97723,24.94297,24.96674,24.9711,24.97694,24.97744,24.97408,24.96299,24.95468,24.94741,24.98085,24.97433,24.98326,24.9805,24.97005,24.94867,24.97697,24.99176,24.97493,24.94898,24.98489,24.96299,24.94155,24.95495,24.97285,24.96322,24.95464,24.97707,24.94297,24.97945,24.97371,24.9675,24.98246,24.93363,24.95425,24.981,24.96881,24.96628,24.96945,24.96305,24.98746,24.97559,24.98203,24.95182,24.97289,24.98343,24.97503,24.95204,24.97347,24.97937,24.9748,24.98672,24.97433,24.96731,24.972,24.97153,24.97838,24.96515,24.98102,24.97703,24.98569,24.98123,24.98569,24.97814,24.96571,24.94155,24.98059,25.00115,24.9489,24.96299,24.94235,24.95032,24.98016,24.96007,24.96606,24.98322,24.97668,24.97668,24.96334,24.96412,24.96606,24.96357,24.98236,24.97213,24.97258,24.96412,24.96211,24.96357,24.95743,24.97429,24.9496,24.97433,24.96464,24.98254,24.95095,24.97519,24.97501,24.97744,24.96334,24.97073,24.95885,24.98203,24.96299,24.96968,24.97886,24.96731,24.97433,24.97937,24.96334,24.9746,24.96945,24.96515,24.97371,24.97305,24.96772,24.95425,24.96322,24.95618,24.97026,24.96581,24.98118,24.9512,24.98674,24.96735,24.97509,24.97433,24.95425,24.97841,24.98203,24.97217,24.95744,24.95674,24.96735,24.9796,24.96299,24.99176,24.97003,24.94155,24.98178,24.96696,24.97229,24.95885,24.97161,24.96622,24.98203,24.97369,24.9546,24.98203,24.94883,24.95394,24.97425,24.97071,24.96674,24.97201,24.9832,24.96606,24.97795,24.98343,24.95562,24.93885,24.98872,24.97388,24.97631,24.96412,24.97015,24.95744,24.97538,24.97003,24.93885,24.94935,24.94826,24.97577,24.96674,24.99006,24.96431,24.98407,24.96871,24.96867,24.98254,24.97281,24.97371,24.97433,24.98223,24.998,24.9711,24.96412,24.95719,24.97585,24.96674,24.96515,24.96324,24.96143,24.96299,24.95204,24.98085,24.97417,24.96853,24.96571,24.95505,24.983,24.97345,24.98872,24.98182,24.97913,24.96182,24.95153,24.96464,24.97544,24.96322,24.98203,24.95591,24.96628,24.9748,24.97937,24.96165,24.96447,24.95836,24.97077,24.97744,24.98203,24.98298,24.93207,24.97845,24.94297,24.983,24.9711,24.96042,24.95957,24.96756,24.97151,24.95155,24.97927,24.98872,24.95544,24.974,24.97198,24.99156,24.96447,24.97371,24.97433,24.96305,24.97388,24.98353,24.98203,24.95663,24.97293,24.95505,24.9633,24.94155,24.97433,24.97923,24.96674,24.97433],[121.54024,121.53951,121.54391,121.54391,121.54245,121.51254,121.53642,121.54228,121.48458,121.51486,121.53372,121.5431,121.53737,121.51046,121.53406,121.54619,121.54458,121.53119,121.54451,121.54102,121.51151,121.54541,121.54842,121.54541,121.53885,121.51726,121.54391,121.53913,121.53797,121.54694,121.49587,121.53408,121.53451,121.54281,121.54243,121.51816,121.51458,121.54842,121.54722,121.54348,121.50381,121.50342,121.53758,121.54301,121.54765,121.54494,121.54458,121.53715,121.49578,121.49542,121.53842,121.55282,121.5175,121.53737,121.54348,121.53009,121.54059,121.53711,121.49542,121.53438,121.51471,121.54516,121.51254,121.54765,121.54915,121.54329,121.54089,121.54067,121.53805,121.54391,121.5431,121.53715,121.53913,121.50381,121.53762,121.54842,121.53767,121.51237,121.53381,121.54887,121.54765,121.53964,121.54252,121.50831,121.54464,121.54413,121.549,121.50342,121.52758,121.50243,121.53059,121.5276,121.51046,121.53076,121.54026,121.54067,121.5431,121.53299,121.54348,121.5431,121.53411,121.52984,121.54089,121.53844,121.5425,121.54458,121.54308,121.54842,121.53561,121.51284,121.54067,121.54224,121.51462,121.53812,121.54142,121.54739,121.47883,121.50383,121.51709,121.54348,121.53737,121.53737,121.54464,121.5317,121.54541,121.54089,121.53477,121.54391,121.54039,121.54162,121.53653,121.51284,121.53863,121.54082,121.54211,121.54951,121.54245,121.54277,121.52406,121.53737,121.54348,121.51627,121.54647,121.53737,121.54842,121.54694,121.5317,121.54494,121.51203,121.54245,121.54269,121.53788,121.54842,121.53713,121.50342,121.50381,121.52954,121.53464,121.54245,121.53464,121.53756,121.53767,121.50342,121.54067,121.5317,121.55391,121.54458,121.54011,121.5432,121.55481,121.49628,121.54391,121.5431,121.5446,121.53778,121.53758,121.49507,121.54262,121.53456,121.5273,121.49621,121.54121,121.51284,121.50381,121.56174,121.5173,121.51237,121.56627,121.54312,121.50342,121.53642,121.54951,121.54069,121.54477,121.51158,121.5399,121.54713,121.54089,121.54196,121.5449,121.54915,121.54391,121.53713,121.54348,121.54887,121.51728,121.53762,121.53692,121.54842,121.54271,121.54245,121.53059,121.52091,121.5431,121.51486,121.54477,121.53559,121.54119,121.53737,121.53655,121.54265,121.5276,121.53743,121.5276,121.5417,121.54089,121.50381,121.53986,121.51776,121.53095,121.51284,121.50357,121.49587,121.53932,121.51361,121.54297,121.53765,121.52518,121.52518,121.54767,121.5167,121.54297,121.51329,121.53923,121.51627,121.54814,121.54273,121.54928,121.54951,121.47516,121.53917,121.53018,121.5431,121.51623,121.54395,121.48458,121.53151,121.5273,121.54458,121.54767,121.517,121.51359,121.54348,121.51284,121.54522,121.53464,121.51486,121.53863,121.54245,121.54767,121.53046,121.5449,121.53737,121.52984,121.53994,121.54102,121.5399,121.51237,121.53844,121.51642,121.54086,121.53788,121.549,121.51844,121.54464,121.53644,121.5431,121.5399,121.5428,121.54348,121.53471,121.53711,121.534,121.54464,121.5414,121.51252,121.53456,121.5458,121.50381,121.54059,121.53992,121.53445,121.51359,121.53423,121.51668,121.54348,121.52979,121.56627,121.54348,121.52954,121.55174,121.53814,121.54069,121.54067,121.54722,121.51812,121.54297,121.53451,121.53762,121.53872,121.50383,121.53411,121.52981,121.54436,121.5167,121.54494,121.53711,121.54971,121.51696,121.50383,121.53046,121.49587,121.53475,121.54067,121.5346,121.54063,121.51748,121.54651,121.54331,121.54395,121.53265,121.54951,121.5431,121.53597,121.5155,121.5317,121.5167,121.47353,121.54516,121.54067,121.53737,121.51241,121.50827,121.51284,121.54842,121.54391,121.52999,121.54413,121.54089,121.55964,121.54022,121.54093,121.53408,121.54086,121.53666,121.51222,121.55254,121.51531,121.53119,121.51237,121.54348,121.53956,121.54196,121.53059,121.54245,121.55011,121.51649,121.53756,121.54634,121.54458,121.54348,121.53981,121.51597,121.53889,121.50342,121.54026,121.5317,121.51462,121.51353,121.5423,121.5435,121.55387,121.53874,121.53408,121.4963,121.53842,121.55063,121.53406,121.51649,121.52984,121.53863,121.51254,121.52981,121.53966,121.54348,121.53765,121.54026,121.55964,121.51243,121.50381,121.5431,121.53986,121.54067,121.5431],[37.9,42.2,47.3,54.8,43.1,32.1,40.3,46.7,18.8,22.1,41.4,58.1,39.3,23.8,34.3,50.5,70.09999999999999,37.4,42.3,47.7,29.3,51.6,24.6,47.9,38.8,27,56.2,33.6,47,57.1,22.1,25,34.2,49.3,55.1,27.3,22.9,25.3,47.7,46.2,15.9,18.2,34.7,34.1,53.9,38.3,42,61.5,13.4,13.2,44.2,20.7,27,38.9,51.7,13.7,41.9,53.5,22.6,42.4,21.3,63.2,27.7,55,25.3,44.3,50.7,56.8,36.2,42,59,40.8,36.3,20,54.4,29.5,36.8,25.6,29.8,26.5,40.3,36.8,48.1,17.7,43.7,50.8,27,18.3,48,25.3,45.4,43.2,21.8,16.1,41,51.8,59.5,34.6,51,62.2,38.2,32.9,54.4,45.7,30.5,71,47.1,26.6,34.1,28.4,51.6,39.4,23.1,7.6,53.3,46.4,12.2,13,30.6,59.6,31.3,48,32.5,45.5,57.4,48.6,62.9,55,60.7,41,37.5,30.7,37.5,39.5,42.2,20.8,46.8,47.4,43.5,42.5,51.4,28.9,37.5,40.1,28.4,45.5,52.2,43.2,45.1,39.7,48.5,44.7,28.9,40.9,20.7,15.6,18.3,35.6,39.4,37.4,57.8,39.6,11.6,55.5,55.2,30.6,73.59999999999999,43.4,37.4,23.5,14.4,58.8,58.1,35.1,45.2,36.5,19.2,42,36.7,42.6,15.5,55.9,23.6,18.8,21.8,21.5,25.7,22,44.3,20.5,42.3,37.8,42.7,49.3,29.3,34.6,36.6,48.2,39.1,31.6,25.5,45.9,31.5,46.1,26.6,21.4,44,34.2,26.2,40.9,52.2,43.5,31.1,58,20.9,48.1,39.7,40.8,43.8,40.2,78.3,38.5,48.5,42.3,46,49,12.8,40.2,46.6,19,33.4,14.7,17.4,32.4,23.9,39.3,61.9,39,40.6,29.7,28.8,41.4,33.4,48.2,21.7,40.8,40.6,23.1,22.3,15,30,13.8,52.7,25.9,51.8,17.4,26.5,43.9,63.3,28.8,30.7,24.4,53,31.7,40.6,38.1,23.7,41.1,40.1,23,117.5,26.5,40.5,29.3,41,49.7,34,27.7,44,31.1,45.4,44.8,25.6,23.5,34.4,55.3,56.3,32.9,51,44.5,37,54.4,24.5,42.5,38.1,21.8,34.1,28.5,16.7,46.1,36.9,35.7,23.2,38.4,29.4,55,50.2,24.7,53,19.1,24.7,42.2,78,42.8,41.6,27.3,42,37.5,49.8,26.9,18.6,37.7,33.1,42.5,31.3,38.1,62.1,36.7,23.6,19.2,12.8,15.6,39.6,38.4,22.8,36.5,35.6,30.9,36.3,50.4,42.9,37,53.5,46.6,41.2,37.9,30.8,11.2,53.7,47,42.3,28.6,25.7,31.3,30.1,60.7,45.3,44.9,45.1,24.7,47.1,63.3,40,48,33.1,29.5,24.8,20.9,43.1,22.8,42.1,51.7,41.5,52.2,49.5,23.8,30.5,56.8,37.4,69.7,53.3,47.3,29.3,40.3,12.9,46.6,55.3,25.6,27.3,67.7,38.6,31.3,35.3,40.3,24.7,42.5,31.9,32.2,23,37.3,35.5,27.7,28.5,39.7,41.2,37.2,40.5,22.3,28.1,15.4,50,40.6,52.5,63.9]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>date<\/th>\n      <th>age<\/th>\n      <th>distance<\/th>\n      <th>stores<\/th>\n      <th>latitude<\/th>\n      <th>longitude<\/th>\n      <th>price<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":8,"columnDefs":[{"className":"dt-right","targets":[0,1,2,3,4,5,6]},{"name":"date","targets":0},{"name":"age","targets":1},{"name":"distance","targets":2},{"name":"stores","targets":3},{"name":"latitude","targets":4},{"name":"longitude","targets":5},{"name":"price","targets":6}],"order":[],"autoWidth":false,"orderClasses":false,"orderCellsTop":true,"lengthMenu":[8,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="linear-regression-and-model-selection.html#cb47-1" tabindex="-1"></a>    </span>
<span id="cb47-2"><a href="linear-regression-and-model-selection.html#cb47-2" tabindex="-1"></a>    <span class="fu">dim</span>(realestate)</span>
<span id="cb47-3"><a href="linear-regression-and-model-selection.html#cb47-3" tabindex="-1"></a><span class="do">## [1] 414   7</span></span></code></pre></div>
</div>
<div id="notation-and-basic-properties" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Notation and Basic Properties<a href="linear-regression-and-model-selection.html#notation-and-basic-properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We usually denote the observed covariates data as the design matrix <span class="math inline">\(\mathbf{X}\)</span>, with dimension <span class="math inline">\(n \times p\)</span>. Hence in this case, the dimension of <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(414 \times 7\)</span>. The <span class="math inline">\(j\)</span>th variable is simply the <span class="math inline">\(j\)</span>th column of this matrix, which is denoted as <span class="math inline">\(\mathbf{x}_j\)</span>. The outcome <span class="math inline">\(\mathbf{y}\)</span> (<code>price</code>) is a vector of length <span class="math inline">\(414\)</span>. Please note that we usually use a “bold” symbol to represent a vector, while for a single element (scalar), such as the <span class="math inline">\(j\)</span>th variable of subject <span class="math inline">\(i\)</span>, we use <span class="math inline">\(x_{ij}\)</span>.</p>
<p>A linear regression concerns modeling the relationship (in matrix form)</p>
<p><span class="math display">\[\mathbf{y}_{n \times 1} = \mathbf{X}_{n \times p} \boldsymbol{\beta}_{p \times 1} + \boldsymbol{\epsilon}_{n \times 1}\]</span>
And we know that the solution is obtained by minimizing the residual sum of squares (RSS):</p>
<p><span class="math display">\[
\begin{align}
\widehat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta}}{\arg\min} \sum_{i=1}^n \left(y_i - x_i^\text{T}\boldsymbol{\beta}\right)^2 \\
&amp;= \underset{\boldsymbol{\beta}}{\arg\min} \big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)^\text{T}\big( \mathbf y - \mathbf{X} \boldsymbol \beta \big)
\end{align}
\]</span>
Classic solution can be obtained by taking the derivative of RSS w.r.t <span class="math inline">\(\boldsymbol{\beta}\)</span> and set it to zero. This leads to the well known normal equation:</p>
<p><span class="math display">\[
\begin{align}
    \frac{\partial \text{RSS}}{\partial \boldsymbol{\beta}} &amp;= -2 \mathbf{X}^\text{T}(\mathbf{y}- \mathbf{X}\boldsymbol{\beta}) \doteq 0 \\
    \Longrightarrow \quad \mathbf{X}^\text{T}\mathbf{y}&amp;= \mathbf{X}^\text{T}\mathbf{X}\boldsymbol{\beta}
\end{align}
\]</span>
Assuming that <span class="math inline">\(\mathbf{X}\)</span> is full rank, then <span class="math inline">\(\mathbf{X}^\text{T}\mathbf{X}\)</span> is invertible. Then, we have</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{y}
\]</span>
Some additional concepts are frequently used. The fitted values <span class="math inline">\(\widehat{\mathbf{y}}\)</span> are essentially the prediction of the original <span class="math inline">\(n\)</span> training data points:</p>
<p><span class="math display">\[
\begin{align}
\widehat{\mathbf{y}} =&amp; \mathbf{X}\boldsymbol{\beta}\\
=&amp; \underbrace{\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}}_{\mathbf{H}} \mathbf{y}\\
\doteq&amp; \mathbf{H}_{n \times n} \mathbf{y}
\end{align}
\]</span>
where <span class="math inline">\(\mathbf{H}\)</span> is called the “hat” matrix. It is a projection matrix that projects any vector (<span class="math inline">\(\mathbf{y}\)</span> in our case) onto the column space of <span class="math inline">\(\mathbf{X}\)</span>. A project matrix enjoys two properties</p>
<ul>
<li>Symmetric: <span class="math inline">\(\mathbf{H}^\text{T}= \mathbf{H}\)</span></li>
<li>Idempotent <span class="math inline">\(\mathbf{H}\mathbf{H}= \mathbf{H}\)</span></li>
</ul>
<p>The residuals <span class="math inline">\(\mathbf{r}\)</span> can also be obtained using the hat matrix:</p>
<p><span class="math display">\[ \mathbf{r}= \mathbf{y}- \widehat{\mathbf{y}} = (\mathbf{I}- \mathbf{H}) \mathbf{y}\]</span>
From the properties of a projection matrix, we also know that <span class="math inline">\(\mathbf{r}\)</span> should be orthogonal to any vector from the column space of <span class="math inline">\(\mathbf{X}\)</span>. Hence,</p>
<p><span class="math display">\[\mathbf{X}^\text{T}\mathbf{r}= \mathbf{0}_{p \times 1}\]</span></p>
<p>The residuals is also used to estimate the error variance:</p>
<p><span class="math display">\[\widehat\sigma^2 = \frac{1}{n-p} \sum_{i=1}^n r_i^2 = \frac{\text{RSS}}{n-p}\]</span>
When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors <span class="math inline">\(\boldsymbol{\epsilon}\)</span>, we can conclude that <span class="math inline">\(\widehat{\boldsymbol{\beta}}\)</span> is an <strong>unbiased</strong> estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. Its variance-covariance matrix satisfies</p>
<p><span class="math display">\[
\begin{align}
    \text{Var}(\widehat{\boldsymbol{\beta}}) &amp;= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{y}\big) \nonumber \\
    &amp;= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}(\mathbf{X}\boldsymbol{\beta}+ \boldsymbol{\epsilon}) \big) \nonumber \\
    &amp;= \text{Var}\big( (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\boldsymbol{\epsilon}) \big) \nonumber \\
    &amp;= (\mathbf{X}^\text{T}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{X}(\mathbf{X}^\text{T}\mathbf{X})^{-1} \mathbf{I}\sigma^2 \nonumber \\
    &amp;= (\mathbf{X}^\text{T}\mathbf{X})^{-1}\sigma^2
\end{align}
\]</span>
All of the above mentioned results are already implemented in R through the <code>lm()</code> function to fit a linear regression.</p>
</div>
<div id="using-the-lm-function" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Using the <code>lm()</code> Function<a href="linear-regression-and-model-selection.html#using-the-lm-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s consider a simple regression that uses <code>age</code> and <code>distance</code> to model <code>price</code>. We will save the fitted object as <code>lm.fit</code></p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="linear-regression-and-model-selection.html#cb48-1" tabindex="-1"></a>    lm.fit <span class="ot">=</span> <span class="fu">lm</span>(price <span class="sc">~</span> age <span class="sc">+</span> distance, <span class="at">data =</span> realestate)</span></code></pre></div>
<p>This syntax contains three components:</p>
<ul>
<li><code>data =</code> specifies the dataset</li>
<li>The outcome variable should be on the left hand side of <code>~</code></li>
<li>The covariates should be on the right hand side of <code>~</code></li>
</ul>
<p>To look at the detailed model fitting results, use the <code>summary()</code> function</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="linear-regression-and-model-selection.html#cb49-1" tabindex="-1"></a>    lm.summary <span class="ot">=</span> <span class="fu">summary</span>(lm.fit)</span>
<span id="cb49-2"><a href="linear-regression-and-model-selection.html#cb49-2" tabindex="-1"></a>    lm.summary</span>
<span id="cb49-3"><a href="linear-regression-and-model-selection.html#cb49-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-4"><a href="linear-regression-and-model-selection.html#cb49-4" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb49-5"><a href="linear-regression-and-model-selection.html#cb49-5" tabindex="-1"></a><span class="do">## lm(formula = price ~ age + distance, data = realestate)</span></span>
<span id="cb49-6"><a href="linear-regression-and-model-selection.html#cb49-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-7"><a href="linear-regression-and-model-selection.html#cb49-7" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb49-8"><a href="linear-regression-and-model-selection.html#cb49-8" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb49-9"><a href="linear-regression-and-model-selection.html#cb49-9" tabindex="-1"></a><span class="do">## -36.032  -4.742  -1.037   4.533  71.930 </span></span>
<span id="cb49-10"><a href="linear-regression-and-model-selection.html#cb49-10" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-11"><a href="linear-regression-and-model-selection.html#cb49-11" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb49-12"><a href="linear-regression-and-model-selection.html#cb49-12" tabindex="-1"></a><span class="do">##               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb49-13"><a href="linear-regression-and-model-selection.html#cb49-13" tabindex="-1"></a><span class="do">## (Intercept) 49.8855858  0.9677644  51.547  &lt; 2e-16 ***</span></span>
<span id="cb49-14"><a href="linear-regression-and-model-selection.html#cb49-14" tabindex="-1"></a><span class="do">## age         -0.2310266  0.0420383  -5.496 6.84e-08 ***</span></span>
<span id="cb49-15"><a href="linear-regression-and-model-selection.html#cb49-15" tabindex="-1"></a><span class="do">## distance    -0.0072086  0.0003795 -18.997  &lt; 2e-16 ***</span></span>
<span id="cb49-16"><a href="linear-regression-and-model-selection.html#cb49-16" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb49-17"><a href="linear-regression-and-model-selection.html#cb49-17" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb49-18"><a href="linear-regression-and-model-selection.html#cb49-18" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb49-19"><a href="linear-regression-and-model-selection.html#cb49-19" tabindex="-1"></a><span class="do">## Residual standard error: 9.73 on 411 degrees of freedom</span></span>
<span id="cb49-20"><a href="linear-regression-and-model-selection.html#cb49-20" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.4911, Adjusted R-squared:  0.4887 </span></span>
<span id="cb49-21"><a href="linear-regression-and-model-selection.html#cb49-21" tabindex="-1"></a><span class="do">## F-statistic: 198.3 on 2 and 411 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>This shows that both <code>age</code> and <code>distance</code> are highly significant for predicting the price. In fact, this fitted object (<code>lm.fit</code>) and the summary object (<code>lm.summary</code>) are both saved as a list. This is pretty common to handle an output object with many things involved. We may peek into this object to see what are provided using a <code>$</code> after the object.</p>
<center>
<img src="images/reactive.png" style="width:40.0%" />
</center>
<p>The <code>str()</code> function can also display all the items in a list.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="linear-regression-and-model-selection.html#cb50-1" tabindex="-1"></a>    <span class="fu">str</span>(lm.summary)</span></code></pre></div>
<p>Usually, printing out the summary is sufficient. However, further details can be useful for other purposes. For example, if we interested in the residual vs. fits plot, we may use</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="linear-regression-and-model-selection.html#cb51-1" tabindex="-1"></a>    <span class="fu">plot</span>(lm.fit<span class="sc">$</span>fitted.values, lm.fit<span class="sc">$</span>residuals, </span>
<span id="cb51-2"><a href="linear-regression-and-model-selection.html#cb51-2" tabindex="-1"></a>         <span class="at">xlab =</span> <span class="st">&quot;Fitted Values&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Residuals&quot;</span>,</span>
<span id="cb51-3"><a href="linear-regression-and-model-selection.html#cb51-3" tabindex="-1"></a>         <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-71-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>It seems that the error variance is not constant (as a function of the fitted values), hence additional techniques may be required to handle this issue. However, that is beyond the scope of this book.</p>
<div id="adding-covariates" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Adding Covariates<a href="linear-regression-and-model-selection.html#adding-covariates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is pretty simple if we want to include additional variables. This is usually done by connecting them with the <code>+</code> sign on the right hand side of <code>~</code>. R also provide convenient ways to include interactions and higher order terms. The following code with the interaction term between <code>age</code> and <code>distance</code>, and a squared term of <code>distance</code> should be self-explanatory.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="linear-regression-and-model-selection.html#cb52-1" tabindex="-1"></a>    lm.fit2 <span class="ot">=</span> <span class="fu">lm</span>(price <span class="sc">~</span> age <span class="sc">+</span> distance <span class="sc">+</span> age<span class="sc">*</span>distance <span class="sc">+</span> <span class="fu">I</span>(distance<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> realestate)</span>
<span id="cb52-2"><a href="linear-regression-and-model-selection.html#cb52-2" tabindex="-1"></a>    <span class="fu">summary</span>(lm.fit2)</span>
<span id="cb52-3"><a href="linear-regression-and-model-selection.html#cb52-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-4"><a href="linear-regression-and-model-selection.html#cb52-4" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb52-5"><a href="linear-regression-and-model-selection.html#cb52-5" tabindex="-1"></a><span class="do">## lm(formula = price ~ age + distance + age * distance + I(distance^2), </span></span>
<span id="cb52-6"><a href="linear-regression-and-model-selection.html#cb52-6" tabindex="-1"></a><span class="do">##     data = realestate)</span></span>
<span id="cb52-7"><a href="linear-regression-and-model-selection.html#cb52-7" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-8"><a href="linear-regression-and-model-selection.html#cb52-8" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb52-9"><a href="linear-regression-and-model-selection.html#cb52-9" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb52-10"><a href="linear-regression-and-model-selection.html#cb52-10" tabindex="-1"></a><span class="do">## -37.117  -4.160  -0.594   3.548  69.683 </span></span>
<span id="cb52-11"><a href="linear-regression-and-model-selection.html#cb52-11" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-12"><a href="linear-regression-and-model-selection.html#cb52-12" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb52-13"><a href="linear-regression-and-model-selection.html#cb52-13" tabindex="-1"></a><span class="do">##                 Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb52-14"><a href="linear-regression-and-model-selection.html#cb52-14" tabindex="-1"></a><span class="do">## (Intercept)    5.454e+01  1.099e+00  49.612  &lt; 2e-16 ***</span></span>
<span id="cb52-15"><a href="linear-regression-and-model-selection.html#cb52-15" tabindex="-1"></a><span class="do">## age           -2.615e-01  4.931e-02  -5.302 1.87e-07 ***</span></span>
<span id="cb52-16"><a href="linear-regression-and-model-selection.html#cb52-16" tabindex="-1"></a><span class="do">## distance      -1.603e-02  1.133e-03 -14.152  &lt; 2e-16 ***</span></span>
<span id="cb52-17"><a href="linear-regression-and-model-selection.html#cb52-17" tabindex="-1"></a><span class="do">## I(distance^2)  1.907e-06  2.416e-07   7.892 2.75e-14 ***</span></span>
<span id="cb52-18"><a href="linear-regression-and-model-selection.html#cb52-18" tabindex="-1"></a><span class="do">## age:distance   8.727e-06  4.615e-05   0.189     0.85    </span></span>
<span id="cb52-19"><a href="linear-regression-and-model-selection.html#cb52-19" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb52-20"><a href="linear-regression-and-model-selection.html#cb52-20" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb52-21"><a href="linear-regression-and-model-selection.html#cb52-21" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb52-22"><a href="linear-regression-and-model-selection.html#cb52-22" tabindex="-1"></a><span class="do">## Residual standard error: 8.939 on 409 degrees of freedom</span></span>
<span id="cb52-23"><a href="linear-regression-and-model-selection.html#cb52-23" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.5726, Adjusted R-squared:  0.5684 </span></span>
<span id="cb52-24"><a href="linear-regression-and-model-selection.html#cb52-24" tabindex="-1"></a><span class="do">## F-statistic:   137 on 4 and 409 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>If you choose to include all covariates presented in the data, then simply use <code>.</code> on the right hand side of <code>~</code>. However, you should always be careful when doing this because some dataset would contain meaningless variables such as subject ID.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="linear-regression-and-model-selection.html#cb53-1" tabindex="-1"></a>    lm.fit3 <span class="ot">=</span> <span class="fu">lm</span>(price <span class="sc">~</span> ., <span class="at">data =</span> realestate)</span></code></pre></div>
</div>
<div id="categorical-variables" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Categorical Variables<a href="linear-regression-and-model-selection.html#categorical-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>store</code> variable has several different values. We can see that it has 11 different values. One strategy is to model this as a continuous variable. However, we may also consider to discretize it. For example, we may create a new variable, say <code>store.cat</code>, defined as follows</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="linear-regression-and-model-selection.html#cb54-1" tabindex="-1"></a>  <span class="fu">table</span>(realestate<span class="sc">$</span>stores)</span>
<span id="cb54-2"><a href="linear-regression-and-model-selection.html#cb54-2" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb54-3"><a href="linear-regression-and-model-selection.html#cb54-3" tabindex="-1"></a><span class="do">##  0  1  2  3  4  5  6  7  8  9 10 </span></span>
<span id="cb54-4"><a href="linear-regression-and-model-selection.html#cb54-4" tabindex="-1"></a><span class="do">## 67 46 24 46 31 67 37 31 30 25 10</span></span>
<span id="cb54-5"><a href="linear-regression-and-model-selection.html#cb54-5" tabindex="-1"></a></span>
<span id="cb54-6"><a href="linear-regression-and-model-selection.html#cb54-6" tabindex="-1"></a>  <span class="co"># define a new factor variable</span></span>
<span id="cb54-7"><a href="linear-regression-and-model-selection.html#cb54-7" tabindex="-1"></a>  realestate<span class="sc">$</span>store.cat <span class="ot">=</span> <span class="fu">as.factor</span>((realestate<span class="sc">$</span>stores <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="sc">+</span> (realestate<span class="sc">$</span>stores <span class="sc">&gt;</span> <span class="dv">4</span>))</span>
<span id="cb54-8"><a href="linear-regression-and-model-selection.html#cb54-8" tabindex="-1"></a>  <span class="fu">table</span>(realestate<span class="sc">$</span>store.cat)</span>
<span id="cb54-9"><a href="linear-regression-and-model-selection.html#cb54-9" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb54-10"><a href="linear-regression-and-model-selection.html#cb54-10" tabindex="-1"></a><span class="do">##   0   1   2 </span></span>
<span id="cb54-11"><a href="linear-regression-and-model-selection.html#cb54-11" tabindex="-1"></a><span class="do">##  67 147 200</span></span>
<span id="cb54-12"><a href="linear-regression-and-model-selection.html#cb54-12" tabindex="-1"></a>  <span class="fu">levels</span>(realestate<span class="sc">$</span>store.cat) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;None&quot;</span>, <span class="st">&quot;Several&quot;</span>, <span class="st">&quot;Many&quot;</span>)</span>
<span id="cb54-13"><a href="linear-regression-and-model-selection.html#cb54-13" tabindex="-1"></a>  <span class="fu">head</span>(realestate<span class="sc">$</span>store.cat)</span>
<span id="cb54-14"><a href="linear-regression-and-model-selection.html#cb54-14" tabindex="-1"></a><span class="do">## [1] Many    Many    Many    Many    Many    Several</span></span>
<span id="cb54-15"><a href="linear-regression-and-model-selection.html#cb54-15" tabindex="-1"></a><span class="do">## Levels: None Several Many</span></span></code></pre></div>
<p>This variable is defined as a factor, which is often used for categorical variables. Since this variable has three different categories, if we include it in the linear regression, it will introduce two additional variables (using the third as the reference):</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="linear-regression-and-model-selection.html#cb55-1" tabindex="-1"></a>    lm.fit3 <span class="ot">=</span> <span class="fu">lm</span>(price <span class="sc">~</span> age <span class="sc">+</span> distance <span class="sc">+</span> store.cat, <span class="at">data =</span> realestate)</span>
<span id="cb55-2"><a href="linear-regression-and-model-selection.html#cb55-2" tabindex="-1"></a>    <span class="fu">summary</span>(lm.fit3)</span>
<span id="cb55-3"><a href="linear-regression-and-model-selection.html#cb55-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb55-4"><a href="linear-regression-and-model-selection.html#cb55-4" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb55-5"><a href="linear-regression-and-model-selection.html#cb55-5" tabindex="-1"></a><span class="do">## lm(formula = price ~ age + distance + store.cat, data = realestate)</span></span>
<span id="cb55-6"><a href="linear-regression-and-model-selection.html#cb55-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb55-7"><a href="linear-regression-and-model-selection.html#cb55-7" tabindex="-1"></a><span class="do">## Residuals:</span></span>
<span id="cb55-8"><a href="linear-regression-and-model-selection.html#cb55-8" tabindex="-1"></a><span class="do">##     Min      1Q  Median      3Q     Max </span></span>
<span id="cb55-9"><a href="linear-regression-and-model-selection.html#cb55-9" tabindex="-1"></a><span class="do">## -38.656  -5.360  -0.868   3.913  76.797 </span></span>
<span id="cb55-10"><a href="linear-regression-and-model-selection.html#cb55-10" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb55-11"><a href="linear-regression-and-model-selection.html#cb55-11" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb55-12"><a href="linear-regression-and-model-selection.html#cb55-12" tabindex="-1"></a><span class="do">##                   Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb55-13"><a href="linear-regression-and-model-selection.html#cb55-13" tabindex="-1"></a><span class="do">## (Intercept)      43.712887   1.751608  24.956  &lt; 2e-16 ***</span></span>
<span id="cb55-14"><a href="linear-regression-and-model-selection.html#cb55-14" tabindex="-1"></a><span class="do">## age              -0.229882   0.040095  -5.733 1.91e-08 ***</span></span>
<span id="cb55-15"><a href="linear-regression-and-model-selection.html#cb55-15" tabindex="-1"></a><span class="do">## distance         -0.005404   0.000470 -11.497  &lt; 2e-16 ***</span></span>
<span id="cb55-16"><a href="linear-regression-and-model-selection.html#cb55-16" tabindex="-1"></a><span class="do">## store.catSeveral  0.838228   1.435773   0.584     0.56    </span></span>
<span id="cb55-17"><a href="linear-regression-and-model-selection.html#cb55-17" tabindex="-1"></a><span class="do">## store.catMany     8.070551   1.646731   4.901 1.38e-06 ***</span></span>
<span id="cb55-18"><a href="linear-regression-and-model-selection.html#cb55-18" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb55-19"><a href="linear-regression-and-model-selection.html#cb55-19" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb55-20"><a href="linear-regression-and-model-selection.html#cb55-20" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb55-21"><a href="linear-regression-and-model-selection.html#cb55-21" tabindex="-1"></a><span class="do">## Residual standard error: 9.279 on 409 degrees of freedom</span></span>
<span id="cb55-22"><a href="linear-regression-and-model-selection.html#cb55-22" tabindex="-1"></a><span class="do">## Multiple R-squared:  0.5395, Adjusted R-squared:  0.535 </span></span>
<span id="cb55-23"><a href="linear-regression-and-model-selection.html#cb55-23" tabindex="-1"></a><span class="do">## F-statistic: 119.8 on 4 and 409 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>There are usually two types of categorical variables:</p>
<ul>
<li>Ordinal: the numbers representing each category is ordered, e.g., how many stores in the neighborhood. Oftentimes nominal data can be treated as a continuous variable.</li>
<li>Nominal: they are not ordered and can be represented using either numbers or letters, e.g., ethnic group.</li>
</ul>
<p>The above example is treating <code>store.cat</code> as a nominal variable, and the <code>lm()</code> function is using dummy variables for each category. This should be our default approach to handle nominal variables.</p>
</div>
</div>
<div id="model-selection-criteria" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Model Selection Criteria<a href="linear-regression-and-model-selection.html#model-selection-criteria" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will use the <code>diabetes</code> dataset from the <code>lars</code> package as a demonstration of model selection. Ten baseline variables include age, sex, body mass index, average blood pressure, and six blood serum measurements. These measurements were obtained for each of n = 442 diabetes patients, as well as the outcome of interest, a quantitative measure of disease progression one year after baseline. More details are available in <span class="citation">Efron et al. (<a href="#ref-efron2004least">2004</a>)</span>. Our goal is to select a linear model, preferably with a small number of variables, that can predict the outcome. To select the best model, commonly used strategies include Marrow’s <span class="math inline">\(C_p\)</span>, AIC (Akaike information criterion) and BIC (Bayesian information criterion). Further derivations will be provide at a later section.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="linear-regression-and-model-selection.html#cb56-1" tabindex="-1"></a>    <span class="co"># load the diabetes data</span></span>
<span id="cb56-2"><a href="linear-regression-and-model-selection.html#cb56-2" tabindex="-1"></a>    <span class="fu">library</span>(lars)</span>
<span id="cb56-3"><a href="linear-regression-and-model-selection.html#cb56-3" tabindex="-1"></a><span class="do">## Loaded lars 1.3</span></span>
<span id="cb56-4"><a href="linear-regression-and-model-selection.html#cb56-4" tabindex="-1"></a>    <span class="fu">data</span>(diabetes)</span>
<span id="cb56-5"><a href="linear-regression-and-model-selection.html#cb56-5" tabindex="-1"></a>    diab <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(diabetes<span class="sc">$</span>x, <span class="st">&quot;Y&quot;</span> <span class="ot">=</span> diabetes<span class="sc">$</span>y))</span>
<span id="cb56-6"><a href="linear-regression-and-model-selection.html#cb56-6" tabindex="-1"></a></span>
<span id="cb56-7"><a href="linear-regression-and-model-selection.html#cb56-7" tabindex="-1"></a>    <span class="co"># fit linear regression with all covariates</span></span>
<span id="cb56-8"><a href="linear-regression-and-model-selection.html#cb56-8" tabindex="-1"></a>    lm.fit <span class="ot">=</span> <span class="fu">lm</span>(Y<span class="sc">~</span>., <span class="at">data=</span>diab)</span></code></pre></div>
<p>The idea of model selection is to apply some penalty on the number of parameters used in the model. In general, they are usually in the form of</p>
<p><span class="math display">\[\text{Goodness-of-Fit} + \text{Complexity Penality}\]</span></p>
<div id="using-marrows-c_p" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Using Marrows’ <span class="math inline">\(C_p\)</span><a href="linear-regression-and-model-selection.html#using-marrows-c_p" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For example, the Marrows’ <span class="math inline">\(C_p\)</span> criterion minimize the following quantity (a derivation is provided at Section <a href="linear-regression-and-model-selection.html#marrows-cp">6.6</a>):</p>
<p><span class="math display">\[\text{RSS} + 2 p \widehat\sigma_{\text{full}}^2\]</span>
Note that the <span class="math inline">\(\sigma_{\text{full}}^2\)</span> refers to the residual variance estimation based on the full model, i.e., will all variables. Hence, this formula cannot be used when <span class="math inline">\(p &gt; n\)</span> because you would not be able to obtain a valid estimation of <span class="math inline">\(\sigma_{\text{full}}^2\)</span>. Nonetheless, we can calculate this quantity with the diabetes dataset</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="linear-regression-and-model-selection.html#cb57-1" tabindex="-1"></a>    <span class="co"># number of variables (including intercept)</span></span>
<span id="cb57-2"><a href="linear-regression-and-model-selection.html#cb57-2" tabindex="-1"></a>    p <span class="ot">=</span> <span class="dv">11</span></span>
<span id="cb57-3"><a href="linear-regression-and-model-selection.html#cb57-3" tabindex="-1"></a>    n <span class="ot">=</span> <span class="fu">nrow</span>(diab)</span>
<span id="cb57-4"><a href="linear-regression-and-model-selection.html#cb57-4" tabindex="-1"></a>      </span>
<span id="cb57-5"><a href="linear-regression-and-model-selection.html#cb57-5" tabindex="-1"></a>    <span class="co"># obtain residual sum of squares</span></span>
<span id="cb57-6"><a href="linear-regression-and-model-selection.html#cb57-6" tabindex="-1"></a>    RSS <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(lm.fit)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb57-7"><a href="linear-regression-and-model-selection.html#cb57-7" tabindex="-1"></a>    </span>
<span id="cb57-8"><a href="linear-regression-and-model-selection.html#cb57-8" tabindex="-1"></a>    <span class="co"># use the formula directly to calculate the Cp criterion </span></span>
<span id="cb57-9"><a href="linear-regression-and-model-selection.html#cb57-9" tabindex="-1"></a>    Cp <span class="ot">=</span> RSS <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>p<span class="sc">*</span><span class="fu">summary</span>(lm.fit)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb57-10"><a href="linear-regression-and-model-selection.html#cb57-10" tabindex="-1"></a>    Cp</span>
<span id="cb57-11"><a href="linear-regression-and-model-selection.html#cb57-11" tabindex="-1"></a><span class="do">## [1] 1328502</span></span></code></pre></div>
<p>We can compare this with another sub-model, say, with just <code>age</code> and <code>glu</code>:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="linear-regression-and-model-selection.html#cb58-1" tabindex="-1"></a>    lm.fit_sub <span class="ot">=</span> <span class="fu">lm</span>(Y<span class="sc">~</span> age <span class="sc">+</span> glu, <span class="at">data=</span>diab)</span>
<span id="cb58-2"><a href="linear-regression-and-model-selection.html#cb58-2" tabindex="-1"></a>  </span>
<span id="cb58-3"><a href="linear-regression-and-model-selection.html#cb58-3" tabindex="-1"></a>    <span class="co"># obtain residual sum of squares</span></span>
<span id="cb58-4"><a href="linear-regression-and-model-selection.html#cb58-4" tabindex="-1"></a>    RSS_sub <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(lm.fit_sub)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb58-5"><a href="linear-regression-and-model-selection.html#cb58-5" tabindex="-1"></a>    </span>
<span id="cb58-6"><a href="linear-regression-and-model-selection.html#cb58-6" tabindex="-1"></a>    <span class="co"># use the formula directly to calculate the Cp criterion </span></span>
<span id="cb58-7"><a href="linear-regression-and-model-selection.html#cb58-7" tabindex="-1"></a>    Cp_sub <span class="ot">=</span> RSS_sub <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span><span class="dv">3</span><span class="sc">*</span><span class="fu">summary</span>(lm.fit)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb58-8"><a href="linear-regression-and-model-selection.html#cb58-8" tabindex="-1"></a>    Cp_sub</span>
<span id="cb58-9"><a href="linear-regression-and-model-selection.html#cb58-9" tabindex="-1"></a><span class="do">## [1] 2240019</span></span></code></pre></div>
<p>Comparing this with the previous one, the full model is better.</p>
</div>
<div id="using-aic-and-bic" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Using AIC and BIC<a href="linear-regression-and-model-selection.html#using-aic-and-bic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Calculating the AIC and BIC criteria in <code>R</code> is a lot simpler, with the existing functions. The AIC score is given by</p>
<p><span class="math display">\[-2 \text{Log-likelihood} + 2 p,\]</span>
while the BIC score is given by</p>
<p><span class="math display">\[-2 \text{Log-likelihood} + \log(n) p,\]</span></p>
<p>Interestingly, when assuming that the error distribution is Gaussian, the log-likelihood part is just a function of the RSS. In general, AIC performs similarly to <span class="math inline">\(C_p\)</span>, while BIC tend to select a much smaller set due to the larger penalty. Theoretically, both AIC and <span class="math inline">\(C_p\)</span> are interested in the prediction error, regardless of whether the model is specified correctly, while BIC is interested in selecting the true set of variables, while assuming that the true model is being considered.</p>
<p>The AIC score can be done using the <code>AIC()</code> function. We can match this result by writing out the normal density function and plug in the estimated parameters. Note that this requires one additional parameter, which is the variance. Hence the total number of parameters is 12. We can calculate this with our own code:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="linear-regression-and-model-selection.html#cb59-1" tabindex="-1"></a>    <span class="co"># ?AIC</span></span>
<span id="cb59-2"><a href="linear-regression-and-model-selection.html#cb59-2" tabindex="-1"></a>    <span class="co"># a build-in function for calculating AIC using -2log likelihood</span></span>
<span id="cb59-3"><a href="linear-regression-and-model-selection.html#cb59-3" tabindex="-1"></a>    <span class="fu">AIC</span>(lm.fit) </span>
<span id="cb59-4"><a href="linear-regression-and-model-selection.html#cb59-4" tabindex="-1"></a><span class="do">## [1] 4795.985</span></span>
<span id="cb59-5"><a href="linear-regression-and-model-selection.html#cb59-5" tabindex="-1"></a></span>
<span id="cb59-6"><a href="linear-regression-and-model-selection.html#cb59-6" tabindex="-1"></a>    <span class="co"># Match the result</span></span>
<span id="cb59-7"><a href="linear-regression-and-model-selection.html#cb59-7" tabindex="-1"></a>    n<span class="sc">*</span><span class="fu">log</span>(RSS<span class="sc">/</span>n) <span class="sc">+</span> n <span class="sc">+</span> n<span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>p</span>
<span id="cb59-8"><a href="linear-regression-and-model-selection.html#cb59-8" tabindex="-1"></a><span class="do">## [1] 4795.985</span></span></code></pre></div>
<p>Alternatively, the <code>extractAIC()</code> function can calculate both AIC and BIC. However, note that the <code>n + n*log(2*pi) + 2</code> part in the above code does not change regardless of how many parameters we use. Hence, this quantify does not affect the comparison between different models. Then we can safely remove this part and only focus on the essential ones.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="linear-regression-and-model-selection.html#cb60-1" tabindex="-1"></a>    <span class="co"># ?extractAIC</span></span>
<span id="cb60-2"><a href="linear-regression-and-model-selection.html#cb60-2" tabindex="-1"></a>    <span class="co"># AIC for the full model</span></span>
<span id="cb60-3"><a href="linear-regression-and-model-selection.html#cb60-3" tabindex="-1"></a>    <span class="fu">extractAIC</span>(lm.fit)</span>
<span id="cb60-4"><a href="linear-regression-and-model-selection.html#cb60-4" tabindex="-1"></a><span class="do">## [1]   11.000 3539.643</span></span>
<span id="cb60-5"><a href="linear-regression-and-model-selection.html#cb60-5" tabindex="-1"></a>    n<span class="sc">*</span><span class="fu">log</span>(RSS<span class="sc">/</span>n) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>p</span>
<span id="cb60-6"><a href="linear-regression-and-model-selection.html#cb60-6" tabindex="-1"></a><span class="do">## [1] 3539.643</span></span>
<span id="cb60-7"><a href="linear-regression-and-model-selection.html#cb60-7" tabindex="-1"></a></span>
<span id="cb60-8"><a href="linear-regression-and-model-selection.html#cb60-8" tabindex="-1"></a>    <span class="co"># BIC for the full model</span></span>
<span id="cb60-9"><a href="linear-regression-and-model-selection.html#cb60-9" tabindex="-1"></a>    <span class="fu">extractAIC</span>(lm.fit, <span class="at">k =</span> <span class="fu">log</span>(n))</span>
<span id="cb60-10"><a href="linear-regression-and-model-selection.html#cb60-10" tabindex="-1"></a><span class="do">## [1]   11.000 3584.648</span></span>
<span id="cb60-11"><a href="linear-regression-and-model-selection.html#cb60-11" tabindex="-1"></a>    n<span class="sc">*</span><span class="fu">log</span>(RSS<span class="sc">/</span>n) <span class="sc">+</span> <span class="fu">log</span>(n)<span class="sc">*</span>p</span>
<span id="cb60-12"><a href="linear-regression-and-model-selection.html#cb60-12" tabindex="-1"></a><span class="do">## [1] 3584.648</span></span></code></pre></div>
<p>Now, we can compare AIC or BIC using of two different models and select whichever one that gives a smaller value. For example the AIC of the previous sub-model is</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="linear-regression-and-model-selection.html#cb61-1" tabindex="-1"></a>    <span class="co"># AIC for the sub-model</span></span>
<span id="cb61-2"><a href="linear-regression-and-model-selection.html#cb61-2" tabindex="-1"></a>    <span class="fu">extractAIC</span>(lm.fit_sub)</span>
<span id="cb61-3"><a href="linear-regression-and-model-selection.html#cb61-3" tabindex="-1"></a><span class="do">## [1]    3.000 3773.077</span></span></code></pre></div>
</div>
</div>
<div id="model-selection-algorithms" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Model Selection Algorithms<a href="linear-regression-and-model-selection.html#model-selection-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous examples, we have to manually fit two models and calculate their respective selection criteria and compare them. This is a rather tedious process if we have many variables and a huge number of combinations to consider. To automatically compare different models and select the best one, there are two common computational approaches: best subset regression and step-wise regression. As their name suggest, the best subset selection will exhaust all possible combination of variables, while the step-wise regression would adjust the model by adding or subtracting one variable at a time to reach the best model.</p>
<div id="best-subset-selection-with-leaps" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> Best Subset Selection with <code>leaps</code><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since the penalty is only affected by the number of variables, we may first choose the best model with the smallest RSS for each model size, and then compare across these models by attaching the penalty terms of their corresponding sizes. The <code>leaps</code> package can be used to calculate the best model of each model size. It essentially performs an exhaustive search, however, still utilizing some tricks to skip some really bad models. Note that the <code>leaps</code> package uses the data matrix directly, instead of specifying a formula.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="linear-regression-and-model-selection.html#cb62-1" tabindex="-1"></a>    <span class="fu">library</span>(leaps)</span>
<span id="cb62-2"><a href="linear-regression-and-model-selection.html#cb62-2" tabindex="-1"></a>    </span>
<span id="cb62-3"><a href="linear-regression-and-model-selection.html#cb62-3" tabindex="-1"></a>    <span class="co"># The package specifies the X matrix and outcome y vector</span></span>
<span id="cb62-4"><a href="linear-regression-and-model-selection.html#cb62-4" tabindex="-1"></a>    RSSleaps <span class="ot">=</span> <span class="fu">regsubsets</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(diab[, <span class="sc">-</span><span class="dv">11</span>]), <span class="at">y =</span> diab[, <span class="dv">11</span>])</span>
<span id="cb62-5"><a href="linear-regression-and-model-selection.html#cb62-5" tabindex="-1"></a>    <span class="fu">summary</span>(RSSleaps, <span class="at">matrix=</span>T)</span>
<span id="cb62-6"><a href="linear-regression-and-model-selection.html#cb62-6" tabindex="-1"></a><span class="do">## Subset selection object</span></span>
<span id="cb62-7"><a href="linear-regression-and-model-selection.html#cb62-7" tabindex="-1"></a><span class="do">## 10 Variables  (and intercept)</span></span>
<span id="cb62-8"><a href="linear-regression-and-model-selection.html#cb62-8" tabindex="-1"></a><span class="do">##     Forced in Forced out</span></span>
<span id="cb62-9"><a href="linear-regression-and-model-selection.html#cb62-9" tabindex="-1"></a><span class="do">## age     FALSE      FALSE</span></span>
<span id="cb62-10"><a href="linear-regression-and-model-selection.html#cb62-10" tabindex="-1"></a><span class="do">## sex     FALSE      FALSE</span></span>
<span id="cb62-11"><a href="linear-regression-and-model-selection.html#cb62-11" tabindex="-1"></a><span class="do">## bmi     FALSE      FALSE</span></span>
<span id="cb62-12"><a href="linear-regression-and-model-selection.html#cb62-12" tabindex="-1"></a><span class="do">## map     FALSE      FALSE</span></span>
<span id="cb62-13"><a href="linear-regression-and-model-selection.html#cb62-13" tabindex="-1"></a><span class="do">## tc      FALSE      FALSE</span></span>
<span id="cb62-14"><a href="linear-regression-and-model-selection.html#cb62-14" tabindex="-1"></a><span class="do">## ldl     FALSE      FALSE</span></span>
<span id="cb62-15"><a href="linear-regression-and-model-selection.html#cb62-15" tabindex="-1"></a><span class="do">## hdl     FALSE      FALSE</span></span>
<span id="cb62-16"><a href="linear-regression-and-model-selection.html#cb62-16" tabindex="-1"></a><span class="do">## tch     FALSE      FALSE</span></span>
<span id="cb62-17"><a href="linear-regression-and-model-selection.html#cb62-17" tabindex="-1"></a><span class="do">## ltg     FALSE      FALSE</span></span>
<span id="cb62-18"><a href="linear-regression-and-model-selection.html#cb62-18" tabindex="-1"></a><span class="do">## glu     FALSE      FALSE</span></span>
<span id="cb62-19"><a href="linear-regression-and-model-selection.html#cb62-19" tabindex="-1"></a><span class="do">## 1 subsets of each size up to 8</span></span>
<span id="cb62-20"><a href="linear-regression-and-model-selection.html#cb62-20" tabindex="-1"></a><span class="do">## Selection Algorithm: exhaustive</span></span>
<span id="cb62-21"><a href="linear-regression-and-model-selection.html#cb62-21" tabindex="-1"></a><span class="do">##          age sex bmi map tc  ldl hdl tch ltg glu</span></span>
<span id="cb62-22"><a href="linear-regression-and-model-selection.html#cb62-22" tabindex="-1"></a><span class="do">## 1  ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;</span></span>
<span id="cb62-23"><a href="linear-regression-and-model-selection.html#cb62-23" tabindex="-1"></a><span class="do">## 2  ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-24"><a href="linear-regression-and-model-selection.html#cb62-24" tabindex="-1"></a><span class="do">## 3  ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-25"><a href="linear-regression-and-model-selection.html#cb62-25" tabindex="-1"></a><span class="do">## 4  ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-26"><a href="linear-regression-and-model-selection.html#cb62-26" tabindex="-1"></a><span class="do">## 5  ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-27"><a href="linear-regression-and-model-selection.html#cb62-27" tabindex="-1"></a><span class="do">## 6  ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-28"><a href="linear-regression-and-model-selection.html#cb62-28" tabindex="-1"></a><span class="do">## 7  ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb62-29"><a href="linear-regression-and-model-selection.html#cb62-29" tabindex="-1"></a><span class="do">## 8  ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</span></span></code></pre></div>
<p>The results is summarized in a matrix, with each row representing a model size. The <code>"*"</code> sign indicates that the variable is include in the model for the corresponding size. Hence, there should be only one of such in the first row, two in the second row, etc.</p>
<p>By default, the algorithm would only consider models up to size 8. This is controlled by the argument <code>nvmax</code>. If we want to consider larger model sizes, then set this to a larger number. However, be careful that this many drastically increase the computational cost.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="linear-regression-and-model-selection.html#cb63-1" tabindex="-1"></a>    <span class="co"># Consider maximum of 10 variables</span></span>
<span id="cb63-2"><a href="linear-regression-and-model-selection.html#cb63-2" tabindex="-1"></a>    RSSleaps <span class="ot">=</span> <span class="fu">regsubsets</span>(<span class="at">x =</span> <span class="fu">as.matrix</span>(diab[, <span class="sc">-</span><span class="dv">11</span>]), <span class="at">y =</span> diab[, <span class="dv">11</span>], <span class="at">nvmax =</span> <span class="dv">10</span>)</span>
<span id="cb63-3"><a href="linear-regression-and-model-selection.html#cb63-3" tabindex="-1"></a>    <span class="fu">summary</span>(RSSleaps,<span class="at">matrix=</span>T)</span>
<span id="cb63-4"><a href="linear-regression-and-model-selection.html#cb63-4" tabindex="-1"></a><span class="do">## Subset selection object</span></span>
<span id="cb63-5"><a href="linear-regression-and-model-selection.html#cb63-5" tabindex="-1"></a><span class="do">## 10 Variables  (and intercept)</span></span>
<span id="cb63-6"><a href="linear-regression-and-model-selection.html#cb63-6" tabindex="-1"></a><span class="do">##     Forced in Forced out</span></span>
<span id="cb63-7"><a href="linear-regression-and-model-selection.html#cb63-7" tabindex="-1"></a><span class="do">## age     FALSE      FALSE</span></span>
<span id="cb63-8"><a href="linear-regression-and-model-selection.html#cb63-8" tabindex="-1"></a><span class="do">## sex     FALSE      FALSE</span></span>
<span id="cb63-9"><a href="linear-regression-and-model-selection.html#cb63-9" tabindex="-1"></a><span class="do">## bmi     FALSE      FALSE</span></span>
<span id="cb63-10"><a href="linear-regression-and-model-selection.html#cb63-10" tabindex="-1"></a><span class="do">## map     FALSE      FALSE</span></span>
<span id="cb63-11"><a href="linear-regression-and-model-selection.html#cb63-11" tabindex="-1"></a><span class="do">## tc      FALSE      FALSE</span></span>
<span id="cb63-12"><a href="linear-regression-and-model-selection.html#cb63-12" tabindex="-1"></a><span class="do">## ldl     FALSE      FALSE</span></span>
<span id="cb63-13"><a href="linear-regression-and-model-selection.html#cb63-13" tabindex="-1"></a><span class="do">## hdl     FALSE      FALSE</span></span>
<span id="cb63-14"><a href="linear-regression-and-model-selection.html#cb63-14" tabindex="-1"></a><span class="do">## tch     FALSE      FALSE</span></span>
<span id="cb63-15"><a href="linear-regression-and-model-selection.html#cb63-15" tabindex="-1"></a><span class="do">## ltg     FALSE      FALSE</span></span>
<span id="cb63-16"><a href="linear-regression-and-model-selection.html#cb63-16" tabindex="-1"></a><span class="do">## glu     FALSE      FALSE</span></span>
<span id="cb63-17"><a href="linear-regression-and-model-selection.html#cb63-17" tabindex="-1"></a><span class="do">## 1 subsets of each size up to 10</span></span>
<span id="cb63-18"><a href="linear-regression-and-model-selection.html#cb63-18" tabindex="-1"></a><span class="do">## Selection Algorithm: exhaustive</span></span>
<span id="cb63-19"><a href="linear-regression-and-model-selection.html#cb63-19" tabindex="-1"></a><span class="do">##           age sex bmi map tc  ldl hdl tch ltg glu</span></span>
<span id="cb63-20"><a href="linear-regression-and-model-selection.html#cb63-20" tabindex="-1"></a><span class="do">## 1  ( 1 )  &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;</span></span>
<span id="cb63-21"><a href="linear-regression-and-model-selection.html#cb63-21" tabindex="-1"></a><span class="do">## 2  ( 1 )  &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-22"><a href="linear-regression-and-model-selection.html#cb63-22" tabindex="-1"></a><span class="do">## 3  ( 1 )  &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-23"><a href="linear-regression-and-model-selection.html#cb63-23" tabindex="-1"></a><span class="do">## 4  ( 1 )  &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-24"><a href="linear-regression-and-model-selection.html#cb63-24" tabindex="-1"></a><span class="do">## 5  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-25"><a href="linear-regression-and-model-selection.html#cb63-25" tabindex="-1"></a><span class="do">## 6  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-26"><a href="linear-regression-and-model-selection.html#cb63-26" tabindex="-1"></a><span class="do">## 7  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot;</span></span>
<span id="cb63-27"><a href="linear-regression-and-model-selection.html#cb63-27" tabindex="-1"></a><span class="do">## 8  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</span></span>
<span id="cb63-28"><a href="linear-regression-and-model-selection.html#cb63-28" tabindex="-1"></a><span class="do">## 9  ( 1 )  &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</span></span>
<span id="cb63-29"><a href="linear-regression-and-model-selection.html#cb63-29" tabindex="-1"></a><span class="do">## 10  ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</span></span>
<span id="cb63-30"><a href="linear-regression-and-model-selection.html#cb63-30" tabindex="-1"></a>    </span>
<span id="cb63-31"><a href="linear-regression-and-model-selection.html#cb63-31" tabindex="-1"></a>    <span class="co"># Obtain the matrix that indicates the variables</span></span>
<span id="cb63-32"><a href="linear-regression-and-model-selection.html#cb63-32" tabindex="-1"></a>    sumleaps <span class="ot">=</span> <span class="fu">summary</span>(RSSleaps, <span class="at">matrix =</span> T)</span>
<span id="cb63-33"><a href="linear-regression-and-model-selection.html#cb63-33" tabindex="-1"></a>    </span>
<span id="cb63-34"><a href="linear-regression-and-model-selection.html#cb63-34" tabindex="-1"></a>    <span class="co"># This object includes the RSS results, which is needed to calculate the scores</span></span>
<span id="cb63-35"><a href="linear-regression-and-model-selection.html#cb63-35" tabindex="-1"></a>    sumleaps<span class="sc">$</span>rss</span>
<span id="cb63-36"><a href="linear-regression-and-model-selection.html#cb63-36" tabindex="-1"></a><span class="do">##  [1] 1719582 1416694 1362708 1331430 1287879 1271491 1267805 1264712 1264066 1263983</span></span>
<span id="cb63-37"><a href="linear-regression-and-model-selection.html#cb63-37" tabindex="-1"></a>    </span>
<span id="cb63-38"><a href="linear-regression-and-model-selection.html#cb63-38" tabindex="-1"></a>    <span class="co"># This matrix indicates whether a variable is in the best model(s)</span></span>
<span id="cb63-39"><a href="linear-regression-and-model-selection.html#cb63-39" tabindex="-1"></a>    sumleaps<span class="sc">$</span>which</span>
<span id="cb63-40"><a href="linear-regression-and-model-selection.html#cb63-40" tabindex="-1"></a><span class="do">##    (Intercept)   age   sex  bmi   map    tc   ldl   hdl   tch   ltg   glu</span></span>
<span id="cb63-41"><a href="linear-regression-and-model-selection.html#cb63-41" tabindex="-1"></a><span class="do">## 1         TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span id="cb63-42"><a href="linear-regression-and-model-selection.html#cb63-42" tabindex="-1"></a><span class="do">## 2         TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE</span></span>
<span id="cb63-43"><a href="linear-regression-and-model-selection.html#cb63-43" tabindex="-1"></a><span class="do">## 3         TRUE FALSE FALSE TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE</span></span>
<span id="cb63-44"><a href="linear-regression-and-model-selection.html#cb63-44" tabindex="-1"></a><span class="do">## 4         TRUE FALSE FALSE TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE</span></span>
<span id="cb63-45"><a href="linear-regression-and-model-selection.html#cb63-45" tabindex="-1"></a><span class="do">## 5         TRUE FALSE  TRUE TRUE  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE</span></span>
<span id="cb63-46"><a href="linear-regression-and-model-selection.html#cb63-46" tabindex="-1"></a><span class="do">## 6         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE</span></span>
<span id="cb63-47"><a href="linear-regression-and-model-selection.html#cb63-47" tabindex="-1"></a><span class="do">## 7         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE</span></span>
<span id="cb63-48"><a href="linear-regression-and-model-selection.html#cb63-48" tabindex="-1"></a><span class="do">## 8         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE</span></span>
<span id="cb63-49"><a href="linear-regression-and-model-selection.html#cb63-49" tabindex="-1"></a><span class="do">## 9         TRUE FALSE  TRUE TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE</span></span>
<span id="cb63-50"><a href="linear-regression-and-model-selection.html#cb63-50" tabindex="-1"></a><span class="do">## 10        TRUE  TRUE  TRUE TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE</span></span>
<span id="cb63-51"><a href="linear-regression-and-model-selection.html#cb63-51" tabindex="-1"></a>    </span>
<span id="cb63-52"><a href="linear-regression-and-model-selection.html#cb63-52" tabindex="-1"></a>    <span class="co"># The package automatically produces the Cp statistic</span></span>
<span id="cb63-53"><a href="linear-regression-and-model-selection.html#cb63-53" tabindex="-1"></a>    sumleaps<span class="sc">$</span>cp</span>
<span id="cb63-54"><a href="linear-regression-and-model-selection.html#cb63-54" tabindex="-1"></a><span class="do">##  [1] 148.352561  47.072229  30.663634  21.998461   9.148045   5.560162   6.303221   7.248522</span></span>
<span id="cb63-55"><a href="linear-regression-and-model-selection.html#cb63-55" tabindex="-1"></a><span class="do">##  [9]   9.028080  11.000000</span></span></code></pre></div>
<p>We can calculate different model selection criteria with the best models of each size. The model fitting result already produces the <span class="math inline">\(C_p\)</span> and BIC results. However, please note that both quantities are modified slightly. For the <span class="math inline">\(C_p\)</span> statistics, the quantity is divided by the estimated error variance, and also adjust for the sample size. For the BIC, the difference is a constant regardless of the model size. Hence these difference do will not affect the model selection result because the modification is the same regardless of the number of variables.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="linear-regression-and-model-selection.html#cb64-1" tabindex="-1"></a>    modelsize<span class="ot">=</span><span class="fu">apply</span>(sumleaps<span class="sc">$</span>which,<span class="dv">1</span>,sum)</span>
<span id="cb64-2"><a href="linear-regression-and-model-selection.html#cb64-2" tabindex="-1"></a>    </span>
<span id="cb64-3"><a href="linear-regression-and-model-selection.html#cb64-3" tabindex="-1"></a>    Cp <span class="ot">=</span> sumleaps<span class="sc">$</span>rss<span class="sc">/</span>(<span class="fu">summary</span>(lm.fit)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>modelsize <span class="sc">-</span> n;</span>
<span id="cb64-4"><a href="linear-regression-and-model-selection.html#cb64-4" tabindex="-1"></a>    AIC <span class="ot">=</span> n<span class="sc">*</span><span class="fu">log</span>(sumleaps<span class="sc">$</span>rss<span class="sc">/</span>n) <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>modelsize;</span>
<span id="cb64-5"><a href="linear-regression-and-model-selection.html#cb64-5" tabindex="-1"></a>    BIC <span class="ot">=</span> n<span class="sc">*</span><span class="fu">log</span>(sumleaps<span class="sc">$</span>rss<span class="sc">/</span>n) <span class="sc">+</span> modelsize<span class="sc">*</span><span class="fu">log</span>(n);</span>
<span id="cb64-6"><a href="linear-regression-and-model-selection.html#cb64-6" tabindex="-1"></a>    </span>
<span id="cb64-7"><a href="linear-regression-and-model-selection.html#cb64-7" tabindex="-1"></a>    <span class="co"># Comparing the Cp scores </span></span>
<span id="cb64-8"><a href="linear-regression-and-model-selection.html#cb64-8" tabindex="-1"></a>    <span class="fu">cbind</span>(<span class="st">&quot;Our Cp&quot;</span> <span class="ot">=</span> Cp, <span class="st">&quot;leaps Cp&quot;</span> <span class="ot">=</span> sumleaps<span class="sc">$</span>cp) </span>
<span id="cb64-9"><a href="linear-regression-and-model-selection.html#cb64-9" tabindex="-1"></a><span class="do">##        Our Cp   leaps Cp</span></span>
<span id="cb64-10"><a href="linear-regression-and-model-selection.html#cb64-10" tabindex="-1"></a><span class="do">## 1  148.352561 148.352561</span></span>
<span id="cb64-11"><a href="linear-regression-and-model-selection.html#cb64-11" tabindex="-1"></a><span class="do">## 2   47.072229  47.072229</span></span>
<span id="cb64-12"><a href="linear-regression-and-model-selection.html#cb64-12" tabindex="-1"></a><span class="do">## 3   30.663634  30.663634</span></span>
<span id="cb64-13"><a href="linear-regression-and-model-selection.html#cb64-13" tabindex="-1"></a><span class="do">## 4   21.998461  21.998461</span></span>
<span id="cb64-14"><a href="linear-regression-and-model-selection.html#cb64-14" tabindex="-1"></a><span class="do">## 5    9.148045   9.148045</span></span>
<span id="cb64-15"><a href="linear-regression-and-model-selection.html#cb64-15" tabindex="-1"></a><span class="do">## 6    5.560162   5.560162</span></span>
<span id="cb64-16"><a href="linear-regression-and-model-selection.html#cb64-16" tabindex="-1"></a><span class="do">## 7    6.303221   6.303221</span></span>
<span id="cb64-17"><a href="linear-regression-and-model-selection.html#cb64-17" tabindex="-1"></a><span class="do">## 8    7.248522   7.248522</span></span>
<span id="cb64-18"><a href="linear-regression-and-model-selection.html#cb64-18" tabindex="-1"></a><span class="do">## 9    9.028080   9.028080</span></span>
<span id="cb64-19"><a href="linear-regression-and-model-selection.html#cb64-19" tabindex="-1"></a><span class="do">## 10  11.000000  11.000000</span></span>
<span id="cb64-20"><a href="linear-regression-and-model-selection.html#cb64-20" tabindex="-1"></a>    </span>
<span id="cb64-21"><a href="linear-regression-and-model-selection.html#cb64-21" tabindex="-1"></a>    <span class="co"># Comparing the BIC results. The difference is a constant, </span></span>
<span id="cb64-22"><a href="linear-regression-and-model-selection.html#cb64-22" tabindex="-1"></a>    <span class="co"># which is the score of an intercept model</span></span>
<span id="cb64-23"><a href="linear-regression-and-model-selection.html#cb64-23" tabindex="-1"></a>    <span class="fu">cbind</span>(<span class="st">&quot;Our BIC&quot;</span> <span class="ot">=</span> BIC, <span class="st">&quot;leaps BIC&quot;</span> <span class="ot">=</span> sumleaps<span class="sc">$</span>bic, </span>
<span id="cb64-24"><a href="linear-regression-and-model-selection.html#cb64-24" tabindex="-1"></a>          <span class="st">&quot;Difference&quot;</span> <span class="ot">=</span> BIC<span class="sc">-</span>sumleaps<span class="sc">$</span>bic, </span>
<span id="cb64-25"><a href="linear-regression-and-model-selection.html#cb64-25" tabindex="-1"></a>          <span class="st">&quot;Intercept Score&quot;</span> <span class="ot">=</span> n<span class="sc">*</span><span class="fu">log</span>(<span class="fu">sum</span>((diab[,<span class="dv">11</span>] <span class="sc">-</span> <span class="fu">mean</span>(diab[,<span class="dv">11</span>]))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>n)))</span>
<span id="cb64-26"><a href="linear-regression-and-model-selection.html#cb64-26" tabindex="-1"></a><span class="do">##     Our BIC leaps BIC Difference Intercept Score</span></span>
<span id="cb64-27"><a href="linear-regression-and-model-selection.html#cb64-27" tabindex="-1"></a><span class="do">## 1  3665.879 -174.1108    3839.99         3839.99</span></span>
<span id="cb64-28"><a href="linear-regression-and-model-selection.html#cb64-28" tabindex="-1"></a><span class="do">## 2  3586.331 -253.6592    3839.99         3839.99</span></span>
<span id="cb64-29"><a href="linear-regression-and-model-selection.html#cb64-29" tabindex="-1"></a><span class="do">## 3  3575.249 -264.7407    3839.99         3839.99</span></span>
<span id="cb64-30"><a href="linear-regression-and-model-selection.html#cb64-30" tabindex="-1"></a><span class="do">## 4  3571.077 -268.9126    3839.99         3839.99</span></span>
<span id="cb64-31"><a href="linear-regression-and-model-selection.html#cb64-31" tabindex="-1"></a><span class="do">## 5  3562.469 -277.5210    3839.99         3839.99</span></span>
<span id="cb64-32"><a href="linear-regression-and-model-selection.html#cb64-32" tabindex="-1"></a><span class="do">## 6  3562.900 -277.0899    3839.99         3839.99</span></span>
<span id="cb64-33"><a href="linear-regression-and-model-selection.html#cb64-33" tabindex="-1"></a><span class="do">## 7  3567.708 -272.2819    3839.99         3839.99</span></span>
<span id="cb64-34"><a href="linear-regression-and-model-selection.html#cb64-34" tabindex="-1"></a><span class="do">## 8  3572.720 -267.2702    3839.99         3839.99</span></span>
<span id="cb64-35"><a href="linear-regression-and-model-selection.html#cb64-35" tabindex="-1"></a><span class="do">## 9  3578.585 -261.4049    3839.99         3839.99</span></span>
<span id="cb64-36"><a href="linear-regression-and-model-selection.html#cb64-36" tabindex="-1"></a><span class="do">## 10 3584.648 -255.3424    3839.99         3839.99</span></span></code></pre></div>
<p>Finally, we may select the best model, using any of the criteria. The following code would produced a plot to visualize it. We can see that BIC selects 6 variables, while both AIC and <span class="math inline">\(C_p\)</span> selects 7.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="linear-regression-and-model-selection.html#cb65-1" tabindex="-1"></a>    <span class="co"># Rescale Cp, AIC and BIC to (0,1).</span></span>
<span id="cb65-2"><a href="linear-regression-and-model-selection.html#cb65-2" tabindex="-1"></a>    inrange <span class="ot">&lt;-</span> <span class="cf">function</span>(x) { (x <span class="sc">-</span> <span class="fu">min</span>(x)) <span class="sc">/</span> (<span class="fu">max</span>(x) <span class="sc">-</span> <span class="fu">min</span>(x)) }</span>
<span id="cb65-3"><a href="linear-regression-and-model-selection.html#cb65-3" tabindex="-1"></a>    </span>
<span id="cb65-4"><a href="linear-regression-and-model-selection.html#cb65-4" tabindex="-1"></a>    Cp <span class="ot">=</span> <span class="fu">inrange</span>(Cp)</span>
<span id="cb65-5"><a href="linear-regression-and-model-selection.html#cb65-5" tabindex="-1"></a>    BIC <span class="ot">=</span> <span class="fu">inrange</span>(BIC)</span>
<span id="cb65-6"><a href="linear-regression-and-model-selection.html#cb65-6" tabindex="-1"></a>    AIC <span class="ot">=</span> <span class="fu">inrange</span>(AIC)</span>
<span id="cb65-7"><a href="linear-regression-and-model-selection.html#cb65-7" tabindex="-1"></a></span>
<span id="cb65-8"><a href="linear-regression-and-model-selection.html#cb65-8" tabindex="-1"></a>    <span class="fu">plot</span>(<span class="fu">range</span>(modelsize), <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>), <span class="at">type=</span><span class="st">&quot;n&quot;</span>, </span>
<span id="cb65-9"><a href="linear-regression-and-model-selection.html#cb65-9" tabindex="-1"></a>         <span class="at">xlab=</span><span class="st">&quot;Model Size (with Intercept)&quot;</span>, </span>
<span id="cb65-10"><a href="linear-regression-and-model-selection.html#cb65-10" tabindex="-1"></a>         <span class="at">ylab=</span><span class="st">&quot;Model Selection Criteria&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">1.5</span>)</span>
<span id="cb65-11"><a href="linear-regression-and-model-selection.html#cb65-11" tabindex="-1"></a></span>
<span id="cb65-12"><a href="linear-regression-and-model-selection.html#cb65-12" tabindex="-1"></a>    <span class="fu">points</span>(modelsize, Cp, <span class="at">col =</span> <span class="st">&quot;green4&quot;</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb65-13"><a href="linear-regression-and-model-selection.html#cb65-13" tabindex="-1"></a>    <span class="fu">points</span>(modelsize, AIC, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb65-14"><a href="linear-regression-and-model-selection.html#cb65-14" tabindex="-1"></a>    <span class="fu">points</span>(modelsize, BIC, <span class="at">col =</span> <span class="st">&quot;purple&quot;</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb65-15"><a href="linear-regression-and-model-selection.html#cb65-15" tabindex="-1"></a>    <span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;Cp&quot;</span>, <span class="st">&quot;AIC&quot;</span>, <span class="st">&quot;BIC&quot;</span>),</span>
<span id="cb65-16"><a href="linear-regression-and-model-selection.html#cb65-16" tabindex="-1"></a>           <span class="at">col=</span><span class="fu">c</span>(<span class="st">&quot;green4&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>), </span>
<span id="cb65-17"><a href="linear-regression-and-model-selection.html#cb65-17" tabindex="-1"></a>           <span class="at">lty =</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">3</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.7</span>)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-85-1.png" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="step-wise-regression-using-step" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Step-wise regression using <code>step()</code><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea of step-wise regression is very simple: we start with a certain model (e.g. the intercept or the full mode), and add or subtract one variable at a time by making the best decision to improve the model selection score. The <code>step()</code> function implements this procedure. The following example starts with the full model and uses AIC as the selection criteria (default of the function). After removing several variables, the model ends up with six predictors.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="linear-regression-and-model-selection.html#cb66-1" tabindex="-1"></a>    <span class="co"># k = 2 (AIC) is default; </span></span>
<span id="cb66-2"><a href="linear-regression-and-model-selection.html#cb66-2" tabindex="-1"></a>    <span class="fu">step</span>(lm.fit, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb66-3"><a href="linear-regression-and-model-selection.html#cb66-3" tabindex="-1"></a><span class="do">## Start:  AIC=3539.64</span></span>
<span id="cb66-4"><a href="linear-regression-and-model-selection.html#cb66-4" tabindex="-1"></a><span class="do">## Y ~ age + sex + bmi + map + tc + ldl + hdl + tch + ltg + glu</span></span>
<span id="cb66-5"><a href="linear-regression-and-model-selection.html#cb66-5" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-6"><a href="linear-regression-and-model-selection.html#cb66-6" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-7"><a href="linear-regression-and-model-selection.html#cb66-7" tabindex="-1"></a><span class="do">## - age   1        82 1264066 3537.7</span></span>
<span id="cb66-8"><a href="linear-regression-and-model-selection.html#cb66-8" tabindex="-1"></a><span class="do">## - hdl   1       663 1264646 3537.9</span></span>
<span id="cb66-9"><a href="linear-regression-and-model-selection.html#cb66-9" tabindex="-1"></a><span class="do">## - glu   1      3080 1267064 3538.7</span></span>
<span id="cb66-10"><a href="linear-regression-and-model-selection.html#cb66-10" tabindex="-1"></a><span class="do">## - tch   1      3526 1267509 3538.9</span></span>
<span id="cb66-11"><a href="linear-regression-and-model-selection.html#cb66-11" tabindex="-1"></a><span class="do">## &lt;none&gt;              1263983 3539.6</span></span>
<span id="cb66-12"><a href="linear-regression-and-model-selection.html#cb66-12" tabindex="-1"></a><span class="do">## - ldl   1      5799 1269782 3539.7</span></span>
<span id="cb66-13"><a href="linear-regression-and-model-selection.html#cb66-13" tabindex="-1"></a><span class="do">## - tc    1     10600 1274583 3541.3</span></span>
<span id="cb66-14"><a href="linear-regression-and-model-selection.html#cb66-14" tabindex="-1"></a><span class="do">## - sex   1     45000 1308983 3553.1</span></span>
<span id="cb66-15"><a href="linear-regression-and-model-selection.html#cb66-15" tabindex="-1"></a><span class="do">## - ltg   1     56015 1319998 3556.8</span></span>
<span id="cb66-16"><a href="linear-regression-and-model-selection.html#cb66-16" tabindex="-1"></a><span class="do">## - map   1     72103 1336086 3562.2</span></span>
<span id="cb66-17"><a href="linear-regression-and-model-selection.html#cb66-17" tabindex="-1"></a><span class="do">## - bmi   1    179028 1443011 3596.2</span></span>
<span id="cb66-18"><a href="linear-regression-and-model-selection.html#cb66-18" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-19"><a href="linear-regression-and-model-selection.html#cb66-19" tabindex="-1"></a><span class="do">## Step:  AIC=3537.67</span></span>
<span id="cb66-20"><a href="linear-regression-and-model-selection.html#cb66-20" tabindex="-1"></a><span class="do">## Y ~ sex + bmi + map + tc + ldl + hdl + tch + ltg + glu</span></span>
<span id="cb66-21"><a href="linear-regression-and-model-selection.html#cb66-21" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-22"><a href="linear-regression-and-model-selection.html#cb66-22" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-23"><a href="linear-regression-and-model-selection.html#cb66-23" tabindex="-1"></a><span class="do">## - hdl   1       646 1264712 3535.9</span></span>
<span id="cb66-24"><a href="linear-regression-and-model-selection.html#cb66-24" tabindex="-1"></a><span class="do">## - glu   1      3001 1267067 3536.7</span></span>
<span id="cb66-25"><a href="linear-regression-and-model-selection.html#cb66-25" tabindex="-1"></a><span class="do">## - tch   1      3543 1267608 3536.9</span></span>
<span id="cb66-26"><a href="linear-regression-and-model-selection.html#cb66-26" tabindex="-1"></a><span class="do">## &lt;none&gt;              1264066 3537.7</span></span>
<span id="cb66-27"><a href="linear-regression-and-model-selection.html#cb66-27" tabindex="-1"></a><span class="do">## - ldl   1      5751 1269817 3537.7</span></span>
<span id="cb66-28"><a href="linear-regression-and-model-selection.html#cb66-28" tabindex="-1"></a><span class="do">## - tc    1     10569 1274635 3539.4</span></span>
<span id="cb66-29"><a href="linear-regression-and-model-selection.html#cb66-29" tabindex="-1"></a><span class="do">## + age   1        82 1263983 3539.6</span></span>
<span id="cb66-30"><a href="linear-regression-and-model-selection.html#cb66-30" tabindex="-1"></a><span class="do">## - sex   1     45831 1309896 3551.4</span></span>
<span id="cb66-31"><a href="linear-regression-and-model-selection.html#cb66-31" tabindex="-1"></a><span class="do">## - ltg   1     55963 1320029 3554.8</span></span>
<span id="cb66-32"><a href="linear-regression-and-model-selection.html#cb66-32" tabindex="-1"></a><span class="do">## - map   1     73850 1337915 3560.8</span></span>
<span id="cb66-33"><a href="linear-regression-and-model-selection.html#cb66-33" tabindex="-1"></a><span class="do">## - bmi   1    179079 1443144 3594.2</span></span>
<span id="cb66-34"><a href="linear-regression-and-model-selection.html#cb66-34" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-35"><a href="linear-regression-and-model-selection.html#cb66-35" tabindex="-1"></a><span class="do">## Step:  AIC=3535.9</span></span>
<span id="cb66-36"><a href="linear-regression-and-model-selection.html#cb66-36" tabindex="-1"></a><span class="do">## Y ~ sex + bmi + map + tc + ldl + tch + ltg + glu</span></span>
<span id="cb66-37"><a href="linear-regression-and-model-selection.html#cb66-37" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-38"><a href="linear-regression-and-model-selection.html#cb66-38" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-39"><a href="linear-regression-and-model-selection.html#cb66-39" tabindex="-1"></a><span class="do">## - glu   1      3093 1267805 3535.0</span></span>
<span id="cb66-40"><a href="linear-regression-and-model-selection.html#cb66-40" tabindex="-1"></a><span class="do">## - tch   1      3247 1267959 3535.0</span></span>
<span id="cb66-41"><a href="linear-regression-and-model-selection.html#cb66-41" tabindex="-1"></a><span class="do">## &lt;none&gt;              1264712 3535.9</span></span>
<span id="cb66-42"><a href="linear-regression-and-model-selection.html#cb66-42" tabindex="-1"></a><span class="do">## - ldl   1      7505 1272217 3536.5</span></span>
<span id="cb66-43"><a href="linear-regression-and-model-selection.html#cb66-43" tabindex="-1"></a><span class="do">## + hdl   1       646 1264066 3537.7</span></span>
<span id="cb66-44"><a href="linear-regression-and-model-selection.html#cb66-44" tabindex="-1"></a><span class="do">## + age   1        66 1264646 3537.9</span></span>
<span id="cb66-45"><a href="linear-regression-and-model-selection.html#cb66-45" tabindex="-1"></a><span class="do">## - tc    1     26840 1291552 3543.2</span></span>
<span id="cb66-46"><a href="linear-regression-and-model-selection.html#cb66-46" tabindex="-1"></a><span class="do">## - sex   1     46382 1311094 3549.8</span></span>
<span id="cb66-47"><a href="linear-regression-and-model-selection.html#cb66-47" tabindex="-1"></a><span class="do">## - map   1     73536 1338248 3558.9</span></span>
<span id="cb66-48"><a href="linear-regression-and-model-selection.html#cb66-48" tabindex="-1"></a><span class="do">## - ltg   1     97509 1362221 3566.7</span></span>
<span id="cb66-49"><a href="linear-regression-and-model-selection.html#cb66-49" tabindex="-1"></a><span class="do">## - bmi   1    178537 1443249 3592.3</span></span>
<span id="cb66-50"><a href="linear-regression-and-model-selection.html#cb66-50" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-51"><a href="linear-regression-and-model-selection.html#cb66-51" tabindex="-1"></a><span class="do">## Step:  AIC=3534.98</span></span>
<span id="cb66-52"><a href="linear-regression-and-model-selection.html#cb66-52" tabindex="-1"></a><span class="do">## Y ~ sex + bmi + map + tc + ldl + tch + ltg</span></span>
<span id="cb66-53"><a href="linear-regression-and-model-selection.html#cb66-53" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-54"><a href="linear-regression-and-model-selection.html#cb66-54" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-55"><a href="linear-regression-and-model-selection.html#cb66-55" tabindex="-1"></a><span class="do">## - tch   1      3686 1271491 3534.3</span></span>
<span id="cb66-56"><a href="linear-regression-and-model-selection.html#cb66-56" tabindex="-1"></a><span class="do">## &lt;none&gt;              1267805 3535.0</span></span>
<span id="cb66-57"><a href="linear-regression-and-model-selection.html#cb66-57" tabindex="-1"></a><span class="do">## - ldl   1      7472 1275277 3535.6</span></span>
<span id="cb66-58"><a href="linear-regression-and-model-selection.html#cb66-58" tabindex="-1"></a><span class="do">## + glu   1      3093 1264712 3535.9</span></span>
<span id="cb66-59"><a href="linear-regression-and-model-selection.html#cb66-59" tabindex="-1"></a><span class="do">## + hdl   1       738 1267067 3536.7</span></span>
<span id="cb66-60"><a href="linear-regression-and-model-selection.html#cb66-60" tabindex="-1"></a><span class="do">## + age   1         0 1267805 3537.0</span></span>
<span id="cb66-61"><a href="linear-regression-and-model-selection.html#cb66-61" tabindex="-1"></a><span class="do">## - tc    1     26378 1294183 3542.1</span></span>
<span id="cb66-62"><a href="linear-regression-and-model-selection.html#cb66-62" tabindex="-1"></a><span class="do">## - sex   1     44686 1312491 3548.3</span></span>
<span id="cb66-63"><a href="linear-regression-and-model-selection.html#cb66-63" tabindex="-1"></a><span class="do">## - map   1     82154 1349959 3560.7</span></span>
<span id="cb66-64"><a href="linear-regression-and-model-selection.html#cb66-64" tabindex="-1"></a><span class="do">## - ltg   1    102520 1370325 3567.3</span></span>
<span id="cb66-65"><a href="linear-regression-and-model-selection.html#cb66-65" tabindex="-1"></a><span class="do">## - bmi   1    189970 1457775 3594.7</span></span>
<span id="cb66-66"><a href="linear-regression-and-model-selection.html#cb66-66" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-67"><a href="linear-regression-and-model-selection.html#cb66-67" tabindex="-1"></a><span class="do">## Step:  AIC=3534.26</span></span>
<span id="cb66-68"><a href="linear-regression-and-model-selection.html#cb66-68" tabindex="-1"></a><span class="do">## Y ~ sex + bmi + map + tc + ldl + ltg</span></span>
<span id="cb66-69"><a href="linear-regression-and-model-selection.html#cb66-69" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-70"><a href="linear-regression-and-model-selection.html#cb66-70" tabindex="-1"></a><span class="do">##        Df Sum of Sq     RSS    AIC</span></span>
<span id="cb66-71"><a href="linear-regression-and-model-selection.html#cb66-71" tabindex="-1"></a><span class="do">## &lt;none&gt;              1271491 3534.3</span></span>
<span id="cb66-72"><a href="linear-regression-and-model-selection.html#cb66-72" tabindex="-1"></a><span class="do">## + tch   1      3686 1267805 3535.0</span></span>
<span id="cb66-73"><a href="linear-regression-and-model-selection.html#cb66-73" tabindex="-1"></a><span class="do">## + glu   1      3532 1267959 3535.0</span></span>
<span id="cb66-74"><a href="linear-regression-and-model-selection.html#cb66-74" tabindex="-1"></a><span class="do">## + hdl   1       395 1271097 3536.1</span></span>
<span id="cb66-75"><a href="linear-regression-and-model-selection.html#cb66-75" tabindex="-1"></a><span class="do">## + age   1        11 1271480 3536.3</span></span>
<span id="cb66-76"><a href="linear-regression-and-model-selection.html#cb66-76" tabindex="-1"></a><span class="do">## - ldl   1     39378 1310869 3545.7</span></span>
<span id="cb66-77"><a href="linear-regression-and-model-selection.html#cb66-77" tabindex="-1"></a><span class="do">## - sex   1     41858 1313349 3546.6</span></span>
<span id="cb66-78"><a href="linear-regression-and-model-selection.html#cb66-78" tabindex="-1"></a><span class="do">## - tc    1     65237 1336728 3554.4</span></span>
<span id="cb66-79"><a href="linear-regression-and-model-selection.html#cb66-79" tabindex="-1"></a><span class="do">## - map   1     79627 1351119 3559.1</span></span>
<span id="cb66-80"><a href="linear-regression-and-model-selection.html#cb66-80" tabindex="-1"></a><span class="do">## - bmi   1    190586 1462077 3594.0</span></span>
<span id="cb66-81"><a href="linear-regression-and-model-selection.html#cb66-81" tabindex="-1"></a><span class="do">## - ltg   1    294094 1565585 3624.2</span></span>
<span id="cb66-82"><a href="linear-regression-and-model-selection.html#cb66-82" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-83"><a href="linear-regression-and-model-selection.html#cb66-83" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb66-84"><a href="linear-regression-and-model-selection.html#cb66-84" tabindex="-1"></a><span class="do">## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab)</span></span>
<span id="cb66-85"><a href="linear-regression-and-model-selection.html#cb66-85" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb66-86"><a href="linear-regression-and-model-selection.html#cb66-86" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb66-87"><a href="linear-regression-and-model-selection.html#cb66-87" tabindex="-1"></a><span class="do">## (Intercept)          sex          bmi          map           tc          ldl          ltg  </span></span>
<span id="cb66-88"><a href="linear-regression-and-model-selection.html#cb66-88" tabindex="-1"></a><span class="do">##       152.1       -226.5        529.9        327.2       -757.9        538.6        804.2</span></span></code></pre></div>
<p>We can also use different settings, such as which model to start with, which is the minimum/maximum model, and do we allow to adding/subtracting.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="linear-regression-and-model-selection.html#cb67-1" tabindex="-1"></a>    <span class="co"># use BIC (k = log(n))instead of AIC</span></span>
<span id="cb67-2"><a href="linear-regression-and-model-selection.html#cb67-2" tabindex="-1"></a>    <span class="co"># trace = 0 will suppress the output of intermediate steps </span></span>
<span id="cb67-3"><a href="linear-regression-and-model-selection.html#cb67-3" tabindex="-1"></a>    <span class="fu">step</span>(lm.fit, <span class="at">direction=</span><span class="st">&quot;both&quot;</span>, <span class="at">k =</span> <span class="fu">log</span>(n), <span class="at">trace=</span><span class="dv">0</span>)</span>
<span id="cb67-4"><a href="linear-regression-and-model-selection.html#cb67-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb67-5"><a href="linear-regression-and-model-selection.html#cb67-5" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb67-6"><a href="linear-regression-and-model-selection.html#cb67-6" tabindex="-1"></a><span class="do">## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab)</span></span>
<span id="cb67-7"><a href="linear-regression-and-model-selection.html#cb67-7" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb67-8"><a href="linear-regression-and-model-selection.html#cb67-8" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb67-9"><a href="linear-regression-and-model-selection.html#cb67-9" tabindex="-1"></a><span class="do">## (Intercept)          sex          bmi          map           tc          ldl          ltg  </span></span>
<span id="cb67-10"><a href="linear-regression-and-model-selection.html#cb67-10" tabindex="-1"></a><span class="do">##       152.1       -226.5        529.9        327.2       -757.9        538.6        804.2</span></span>
<span id="cb67-11"><a href="linear-regression-and-model-selection.html#cb67-11" tabindex="-1"></a></span>
<span id="cb67-12"><a href="linear-regression-and-model-selection.html#cb67-12" tabindex="-1"></a>    <span class="co"># Start with an intercept model, and use forward selection (adding only)</span></span>
<span id="cb67-13"><a href="linear-regression-and-model-selection.html#cb67-13" tabindex="-1"></a>    <span class="fu">step</span>(<span class="fu">lm</span>(Y<span class="sc">~</span><span class="dv">1</span>, <span class="at">data=</span>diab), <span class="at">scope=</span><span class="fu">list</span>(<span class="at">upper=</span>lm.fit, <span class="at">lower=</span><span class="sc">~</span><span class="dv">1</span>), </span>
<span id="cb67-14"><a href="linear-regression-and-model-selection.html#cb67-14" tabindex="-1"></a>         <span class="at">direction=</span><span class="st">&quot;forward&quot;</span>, <span class="at">trace=</span><span class="dv">0</span>)</span>
<span id="cb67-15"><a href="linear-regression-and-model-selection.html#cb67-15" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb67-16"><a href="linear-regression-and-model-selection.html#cb67-16" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb67-17"><a href="linear-regression-and-model-selection.html#cb67-17" tabindex="-1"></a><span class="do">## lm(formula = Y ~ bmi + ltg + map + tc + sex + ldl, data = diab)</span></span>
<span id="cb67-18"><a href="linear-regression-and-model-selection.html#cb67-18" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb67-19"><a href="linear-regression-and-model-selection.html#cb67-19" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb67-20"><a href="linear-regression-and-model-selection.html#cb67-20" tabindex="-1"></a><span class="do">## (Intercept)          bmi          ltg          map           tc          sex          ldl  </span></span>
<span id="cb67-21"><a href="linear-regression-and-model-selection.html#cb67-21" tabindex="-1"></a><span class="do">##       152.1        529.9        804.2        327.2       -757.9       -226.5        538.6</span></span></code></pre></div>
<p>We can see that these results are slightly different from the best subset selection. So which is better? Of course the best subset selection is better because it considers all possible candidates, which step-wise regression may stuck at a sub-optimal model, while adding and subtracting any variable do not benefit further. Hence, the results of step-wise regression may be unstable. On the other hand, best subset selection not really feasible for high-dimensional problems because of the computational cost.</p>
</div>
</div>
<div id="marrows-cp" class="section level2 hasAnchor" number="6.6">
<h2><span class="header-section-number">6.6</span> Derivation of Marrows’ <span class="math inline">\(C_p\)</span><a href="linear-regression-and-model-selection.html#marrows-cp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we have a set of training data <span class="math inline">\({\cal D}_n = \{x_i, \color{DodgerBlue}{y_i}\}_{i=1}^n\)</span> and a set of testing data, with the same covariates <span class="math inline">\({\cal D}_n^\ast = \{x_i, \color{OrangeRed}{y_i^\ast}\}_{i=1}^n\)</span>. Hence, this is an <strong>in-sample prediction</strong> problem. However, the <span class="math inline">\(\color{OrangeRed}{y_i^\ast}\)</span>s are newly observed. Assuming that the data are generated from a linear model, i.e., in vector form,</p>
<p><span class="math display">\[\color{DodgerBlue}{\mathbf{y}}= \boldsymbol{\mu}+ \color{DodgerBlue}{\mathbf{e}}= \mathbf{X}\boldsymbol{\beta}+ \color{DodgerBlue}{\mathbf{e}},\]</span>
and
<span class="math display">\[\color{OrangeRed}{\mathbf{y}^\ast}= \boldsymbol{\mu}+ \color{OrangeRed}{\mathbf{e}^\ast}= \mathbf{X}\boldsymbol{\beta}+ \color{OrangeRed}{\mathbf{e}^\ast},\]</span>
where the error terms are i.i.d with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>. We want to know what is the best model that predicts <span class="math inline">\(\color{OrangeRed}{\mathbf{y}^\ast}\)</span>. Let’s look at the testing error first:</p>
<p><span class="math display">\[\begin{align}
\text{E}[\color{OrangeRed}{\text{Testing Error}}] =&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol{\beta}}}\rVert^2 \\
=&amp; ~\text{E}\lVert (\color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\boldsymbol{\beta}) + (\mathbf{X}\boldsymbol{\beta}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol{\beta}}}) \rVert^2 \\
=&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{e}^\ast}\rVert^2 + \text{E}\lVert \mathbf{X}(\color{DodgerBlue}{\widehat{\boldsymbol{\beta}}}- \boldsymbol{\beta}) \rVert^2 \\
=&amp; ~\color{OrangeRed}{n \sigma^2} + \text{E}\big[ \text{Trace}\big( (\color{DodgerBlue}{\widehat{\boldsymbol{\beta}}}- \boldsymbol{\beta})^\text{T}\mathbf{X}^\text{T}\mathbf{X}(\color{DodgerBlue}{\widehat{\boldsymbol{\beta}}}- \boldsymbol{\beta}) \big) \big] \\
=&amp; ~\color{OrangeRed}{n \sigma^2} + \text{Trace}\big(\mathbf{X}^\text{T}\mathbf{X}\text{Cov}(\color{DodgerBlue}{\widehat{\boldsymbol{\beta}}})\big) \\
=&amp; ~\color{OrangeRed}{n \sigma^2} + \color{DodgerBlue}{p \sigma^2}.
\end{align}\]</span></p>
<p>In the above, we used properties</p>
<ul>
<li><span class="math inline">\(\text{Trace}(ABC) = \text{Trace}(CAB)\)</span></li>
<li><span class="math inline">\(\text{E}[\text{Trace}(A)] = \text{Trace}(\text{E}[A])\)</span></li>
</ul>
<p>On the other hand, the training error is</p>
<p><span class="math display">\[\begin{align}
\text{E}[\color{DodgerBlue}{\text{Training Error}}] =&amp; ~\text{E}\lVert \mathbf{r}\rVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H}) \color{DodgerBlue}{\mathbf{y}}\lVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H})(\mathbf{X}\boldsymbol{\beta}+ \color{DodgerBlue}{\mathbf{e}}) \rVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H})\color{DodgerBlue}{\mathbf{e}}\rVert^2 \\
=&amp; ~\text{E}[\text{Trace}(\color{DodgerBlue}{\mathbf{e}}^\text{T}(\mathbf{I}- \mathbf{H})^\text{T}(\mathbf{I}- \mathbf{H}) \color{DodgerBlue}{\mathbf{e}})]\\
=&amp; ~\text{Trace}((\mathbf{I}- \mathbf{H})^\text{T}(\mathbf{I}- \mathbf{H}) \text{Cov}(\color{DodgerBlue}{\mathbf{e}})]\\
=&amp; ~\color{DodgerBlue}{(n - p) \sigma^2}.
\end{align}\]</span></p>
<p>In the above, we further used properties</p>
<ul>
<li><span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\(\mathbf{I}- \mathbf{H}\)</span> are projection matrices</li>
<li><span class="math inline">\(\mathbf{H}\mathbf{X}= \mathbf{X}\)</span></li>
</ul>
<p>If we contrast the two results above, the difference between the training and testing errors is <span class="math inline">\(2 p \sigma^2\)</span>. Hence, if we can obtain a valid estimation of <span class="math inline">\(\sigma^2\)</span>, then the training error plus <span class="math inline">\(2 p \widehat{\sigma}^2\)</span> is a good approximation of the testing error, which we want to minimize. And that is exactly what Marrows’ <span class="math inline">\(C_p\)</span> does.</p>
<p>We can also generalize this result to the case when the underlying model is not a linear model. Assume that</p>
<p><span class="math display">\[\color{DodgerBlue}{\mathbf{y}}= f(\mathbf{X}) + \color{DodgerBlue}{\mathbf{e}}= \boldsymbol{\mu}+ \color{DodgerBlue}{\mathbf{e}},\]</span>
and
<span class="math display">\[\color{OrangeRed}{\mathbf{y}^\ast}= f(\mathbf{X}) + \color{OrangeRed}{\mathbf{e}^\ast}= \boldsymbol{\mu}+ \color{OrangeRed}{\mathbf{e}^\ast}.\]</span>
In this case, a linear model would not estimate <span class="math inline">\(\boldsymbol{\mu}\)</span>. Instead, it is only capable to produce the best linear approximation of <span class="math inline">\(\boldsymbol{\mu}\)</span> using the columns in <span class="math inline">\(\mathbf{X}\)</span>, which is <span class="math inline">\(\mathbf{H}\boldsymbol{\mu}\)</span>, the projection of <span class="math inline">\(\boldsymbol{\mu}\)</span> on the column space of <span class="math inline">\(\mathbf{X}\)</span>. In general, <span class="math inline">\(\mathbf{H}\boldsymbol{\mu}\neq \boldsymbol{\mu}\)</span>, and the remaining part <span class="math inline">\(\boldsymbol{\mu}- \mathbf{H}\boldsymbol{\mu}\)</span> is called <strong>bias</strong>. This is a new concept that will appear frequently in this book. Selection variables will essentially trade between bias and variance of a model. The following derivation shows this phenomenon:</p>
<p><span class="math display">\[\begin{align}
\text{E}[\color{OrangeRed}{\text{Testing Error}}] =&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol{\beta}}}\rVert^2 \\
=&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=&amp; ~\text{E}\lVert (\color{OrangeRed}{\mathbf{y}^\ast}- \boldsymbol{\mu}) + (\boldsymbol{\mu}- \mathbf{H}\boldsymbol{\mu}) + (\mathbf{H}\boldsymbol{\mu}- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}) \rVert^2 \\
=&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{y}^\ast}- \boldsymbol{\mu}\rVert^2 + \text{E}\lVert \boldsymbol{\mu}- \mathbf{H}\boldsymbol{\mu}\rVert^2 + \text{E}\lVert \mathbf{H}\boldsymbol{\mu}- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=&amp; ~\text{E}\lVert \color{OrangeRed}{\mathbf{e}^\ast}\rVert^2 + \text{E}\lVert \boldsymbol{\mu}- \mathbf{H}\boldsymbol{\mu}\rVert^2 + \text{E}\lVert \mathbf{H}\color{DodgerBlue}{\mathbf{e}}\rVert^2 \\
=&amp; ~\color{OrangeRed}{n \sigma^2} + \text{Bias}^2 + \color{DodgerBlue}{p \sigma^2},
\end{align}\]</span></p>
<p>while the training error is</p>
<p><span class="math display">\[\begin{align}
\text{E}[\color{DodgerBlue}{\text{Training Error}}] =&amp; ~\text{E}\lVert \color{DodgerBlue}{\mathbf{y}}- \mathbf{X}\color{DodgerBlue}{\widehat{\boldsymbol{\beta}}}\rVert^2 \\
=&amp; ~\text{E}\lVert \color{DodgerBlue}{\mathbf{y}}- \mathbf{H}\color{DodgerBlue}{\mathbf{y}}\rVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H})(\boldsymbol{\mu}+ \color{DodgerBlue}{\mathbf{e}}) \rVert^2 \\
=&amp; ~\text{E}\lVert (\mathbf{I}- \mathbf{H})\boldsymbol{\mu}\rVert^2 + \text{E}\lVert (\mathbf{I}- \mathbf{H})\color{DodgerBlue}{\mathbf{e}}\rVert^2\\
=&amp; ~\text{Bias}^2 + \color{DodgerBlue}{(n - p) \sigma^2}.
\end{align}\]</span></p>
<p>We can notice again that the difference is <span class="math inline">\(2p\sigma^2\)</span>. Note that this is regardless of whether the linear model is correct or not.</p>

<div style="display:none;">
<!-- Conflict \def\bf{\mathbf{f}} -->
</div>
</div>
</div>
<h3> Reference<a href="reference.html#reference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-efron2004least" class="csl-entry">
Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. <span>“Least Angle Regression.”</span> <em>The Annals of Statistics</em> 32 (2): 407–99.
</div>
<div id="ref-yeh2018building" class="csl-entry">
Yeh, I-Cheng, and Tzu-Kuang Hsu. 2018. <span>“Building Real Estate Valuation Models with Comparative Approach Through Case-Based Reasoning.”</span> <em>Applied Soft Computing</em> 65: 260–71.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimization-basics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ridge-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "sepia",
    "family": "serif",
    "size": 1
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
