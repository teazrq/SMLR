[["index.html", "Statistical Machine Learning with R Preface Target Audience What’s Covered? Acknowledgements License", " Statistical Machine Learning with R Ruoqing Zhu, PhD 2025-09-01 Preface Welcome to Statistical Machine Learning with R! I started this project during the summer of 2018 when I was preparing for the Stat 432 course (Basics of Statistical Learning). At that time, our faculty member Dr. David Dalpiaz, had decided to move to The Ohio State University (although he moved back to UIUC later on). David introduced to me this awesome way of publishing website on GitHub, which is a very efficient approach for developing course materials. Since I have also taught Stat 542 (Statistical Learning) for several years, I figured it could be beneficial to integrate what I have to this existing book by David and use it as the R material for both courses. For Stat 542, the main focus is to learn the numerical optimization behind these learning algorithms, and also be familiar with the theoretical background. As you can tell, I am not being very creative on the name, so SMLR it is. You can find the source file of this book on my GitHub. Recently, I started a new course Stat 546 (Machine Learning in Data Science), which focuses more on RKHS, random forests and reinforcement learning, and I am also add more materials to this book. Target Audience This book can be suitable for students ranging from advanced undergraduate to first/second year Ph.D students who have prior knowledge in statistics. Although a student at the masters level will likely benefit most from the material. Previous experience with both basic mathematics (mainly linear algebra), statistical modeling (such as linear regressions) and R are assumed. What’s Covered? This book currently covers the following topics: Basic Knowledge R, R Studio and R Markdown Linear regression and linear algebra Numerical optimization basics Model Selection and Regularization in Linear Models Ridge regression Lasso Spline Classification models Logistic regression Discriminant analysis Nonparametric Models with Local Smoothing K-nearest neighbor Kernel smoothing Kernel Methods and RKHS Support vector machine RKHS Kernel ridge regression Tree and Ensemble Models Tree models Random forests Boosting Unsupervised Learning K-means Hierarchical clustering PCA self-organizing map Spectral clustering UMAP The goal of this book is to introduce not only how to run some of the popular statistical learning models in R, know the algorithms and programming techniques for solving these models and also understand some of the fundamental statistical theory behind them. For example, for graduate students, these topics will be discuss in more detail: Optimization Lagrangian Primal vs. dual EM and MM algorithm Bias-variance trade-off in Linear regression KNN Kernel density estimation Kernel Trick and RKHS Representer Theorem SVM Spline This book is under active development. Hence, you may encounter errors ranging from typos to broken code, to poorly explained topics. If you do, please let me know! Simply send an email to rqzhu@illinois.edu and I will make the changes as soon as possible. Or, if you know R Markdown and are familiar with GitHub, make a pull request and fix an issue yourself! These contributions will be acknowledged. Acknowledgements The initial contents are derived from Dr. David Dalpiaz’s book. My STAT 542 course materials are also inspired by Dr. Feng Liang and Dr. John Marden who developed earlier versions of this course. And I also incorporated many online resources, which I cannot put into a comprehensive list. If you think I missed some references, please let me know. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["r-and-rstudio.html", "Chapter 1 R and RStudio 1.1 Installing R and RStudio 1.2 Resources and Guides 1.3 Basic Mathematical Operations 1.4 Data Objects 1.5 Readin and save data 1.6 Using and defining functions 1.7 Distribution and random numbers 1.8 Using packages and other resources 1.9 Practice questions", " Chapter 1 R and RStudio 1.1 Installing R and RStudio The first step is to download and install R and RStudio. Most steps should be self-explanatory. You can also find many online guides for step-by-step instruction, such as this YouTube video. However, be aware that some details may have been changed over the years. After installing both, open your RStudio, you should see four panes, which can be seen below: Source pane on top-left where you write code in to files Console on bottom-left where the code is inputted into R Environment (and other tabs) on top-right where you can see current variables and objects you defined File (and other tabs) on bottom-right which is essentially a file borrower We will mainly use the left two panes. You can either directly input code into the console to run for results, or edit your code in a file and run them in chunks or as a whole. 1.2 Resources and Guides There are many online resources for how to use R, RStudio. For example, David Dalpiaz’s other online book Applied Statistics with R contains an introduction to using them. There are also other online documentation such as Install R and RStudio R tutorial Data in R Play-list (video) R and RStudio Play-list (video) It is worth to mention that once you become an advanced user, and possibly a developer of R packages using C/C++ (add-on of R for performing specific tasks), and you also happen to use Windows like I do, you will have to install Rtools that contains the gcc compilers. This is also needed if you want to install any R package from a “source” (.tar.gz) file instead of using the so-called “binaries” (.zip files). 1.3 Basic Mathematical Operations Basic R calculations and operations should be self-explanatory. Try to type-in the following commands into your R console and start to explore yourself. Lines with a # in the front are comments, which will not be executed. Lines with ## in the front are outputs you should expect. # Basic mathematical operations 1 + 3 ## [1] 4 1 - 3 ## [1] -2 1 * 3 ## [1] 3 1 / 3 ## [1] 0.3333333 3^5 ## [1] 243 4^(-1/2) ## [1] 0.5 pi ## [1] 3.141593 # some math functions sqrt(4) ## [1] 2 exp(1) ## [1] 2.718282 log(3) ## [1] 1.098612 log2(16) ## [1] 4 log(15, base = 3) ## [1] 2.464974 factorial(5) ## [1] 120 sin(pi) ## [1] 1.224606e-16 If you want to see more information about a particular function or operator in R, the easiest way is to get the reference document. Put a question mark in front of a function name: # In a default R console window, this will open up a web browser. # In RStudio, this will be displayed at the ‘Help’ window at the bottom-right penal (Help tab). ?log10 ?cos 1.4 Data Objects Data objects can be a complicated topic for people who never used R before. The most common data objects are vector, matrix, list, and data.frame. They are defined using a specific syntax. To define a vector, we use c followed by (), where the elements within the parenthesis are separated using comma. You can save the vector and name as something else. For example # creating a vector c(1,2,3,4) ## [1] 1 2 3 4 c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; # define a new vector object, called `x` x = c(1,1,1,0,0,0) After defining this object x, it should also appear on your top-right environment pane. To access elements in an object, we use the [] operator, like a C programming reference style. # getting the second element in x x[2] ## [1] 1 # getting the second to the fourth element in x x[2:4] ## [1] 1 1 0 Similarly, we can create and access elements in a matrix: # create a matrix by providing all of its elements # the elements are filled to the matrix by column matrix(c(1,2,3,4), 2, 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 # create a matrix by column-bind vectors y = c(1,0,1,0,1,0) cbind(x, y) ## x y ## [1,] 1 1 ## [2,] 1 0 ## [3,] 1 1 ## [4,] 0 0 ## [5,] 0 1 ## [6,] 0 0 # access elements in a matrix # Note that in R, upper and lower cases are treated as two different objects X = matrix(c(1:16), 4, 4) X ## [,1] [,2] [,3] [,4] ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 ## [3,] 3 7 11 15 ## [4,] 4 8 12 16 X[2, 3] ## [1] 10 X[1, ] ## [1] 1 5 9 13 # getting a sub-matrix of X X[1:2, 3:4] ## [,1] [,2] ## [1,] 9 13 ## [2,] 10 14 Mathematical operations on vectors and matrices are, by default, element-wise. For matrix multiplications, you should use %*%. # adding two vectors (x + y)^2 ## [1] 4 1 4 0 1 0 # getting the length of a vector length(x) ## [1] 6 # matrix multiplication X %*% X ## [,1] [,2] [,3] [,4] ## [1,] 90 202 314 426 ## [2,] 100 228 356 484 ## [3,] 110 254 398 542 ## [4,] 120 280 440 600 # getting the dimension of a matrix dim(X) ## [1] 4 4 # A warning will be issued when R detects something wrong # Results may still be produced however y + c(1,2,3,4) ## Warning in y + c(1, 2, 3, 4): longer object length is not a multiple of shorter object length ## [1] 2 2 4 4 2 2 list() creates a list of objects (of any type). However, some operators cannot be directly applied to a list in a similar way as to vectors or matrices. Model fitting results in R are usually stored as a list. For example, the lm() function, which will be introduced later. # creating a list x = list(c(1,2), &quot;hello&quot;, matrix(c(1,2,3,4), 2, 2)) # accessing its elements using double brackets `[[]]` x[[1]] ## [1] 1 2 data.frame() creates a list of vectors of equal length, and display them as a matrix-like object, in which each vector is a column of the matrix. It is mainly used for storing data. This will be our most frequently used data object for analysis. For example, in the famous iris data, the first four columns are numerical variables, while the last column is a categorical variable with three levels: setosa, versicolor, and virginica: # The iris data is included with base R, so we can use them directly # This will create a copy of the data into your environment data(iris) # the head function peeks the first several rows of the dataset head(iris, n = 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa # each column usually contains a column (variable) name colnames(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; # data frame can be called by each individual column, which will be a vector # iris$Species iris$Species[2:4] ## [1] setosa setosa setosa ## Levels: setosa versicolor virginica # the summary function can be used to view summary statistics of all variables summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 factor is a special type of vector. It is frequently used to store a categorical variable with more than two categories. The last column of the iris data is a factor. You need to be a little bit careful when dealing with factor variables when during modeling since some functions do not take care of them automatically or they do it in a different way than you thought. For example, changing a factor variable into numerical ones will ignore any potential relationship among different categories. levels(iris$Species) ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; as.numeric(iris$Species) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [48] 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [95] 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [142] 3 3 3 3 3 3 3 3 3 1.5 Readin and save data Data can be imported from a variety of sources. More commonly, a dataset can be stored in .txt and .csv files. Such data reading methods require specific structures in the source file: the first row should contain column names, and there should be equal number of elements in each row. Hence you should always check your file before reading them in. # read-in data birthrate = read.csv(&quot;data/birthrate.csv&quot;) head(birthrate) ## Year Birthrate ## 1 1917 183.1 ## 2 1918 183.9 ## 3 1919 163.1 ## 4 1920 179.5 ## 5 1921 181.4 ## 6 1922 173.4 # to see how many observations (rows) and variables (columns) in a dataset dim(birthrate) ## [1] 87 2 R data can also be saved into other formats. The more efficient way, assuming that you are going to load these file back to R in the future, is to save them as .RData file. Usually, for a larger dataset, this reduces the time spend on reading the data. # saving a object to .RData file save(birthrate, file = &quot;mydata.RData&quot;) # you can specify multiple objects to be saved into the same file save(birthrate, iris, file = &quot;mydata.RData&quot;) # load the data again back to your environment load(&quot;mydata.RData&quot;) # alternatively, you can also save data to a .csv file write.csv(birthrate, file = &quot;mydata.csv&quot;) # you can notice that this .csv file contains an extra column of &quot;ID number&quot;, without a column name # Hence, when you read this file back into R, you should specify `row.names = 1` to indicate that. # Otherwise this will produce an error read.csv(file = &quot;mydata.csv&quot;, row.names = 1) 1.6 Using and defining functions We have already used many functions. You can also define your own functions, and even build them into packages (more on this later) for other people to use. This is the main advantage of R. For example, let’s consider writing a function that returns the minimum and maximum of a vector. Suppose we already know the min() and max() functions. myrange &lt;- function(x) # x is the argument that your function takes in { return(c(min(x), max(x))) # return a vector that contains two elements } x = 1:10 myrange(x) ## [1] 1 10 # R already has this function range(x) ## [1] 1 10 1.7 Distribution and random numbers Three distributions that are most frequently used in this course are Bernoulli, Gaussian (normal), and \\(t\\) distributions. Bernoulli distributions can be used to describe binary variables, while Gaussian distribution is often used to describe continuous ones. The following code generates some random variables # read the documentation of rbinom() using ?rbinom x = rbinom(100, 1, 0.4) table(x) ## x ## 0 1 ## 62 38 However, this result cannot be replicated by others, since the next time we run this code, the random numbers will be different. Hence it is important to set and keep the random seed when a random algorithm is involved. The following code will always generate the same result set.seed(1) x = rbinom(100, 1, 0.4) y = rnorm(100) # by default, this is mean 0 and variance 1 table(x) ## x ## 0 1 ## 57 43 hist(y) boxplot(y ~ x) 1.8 Using packages and other resources Packages are written and contributed to R by individuals. They provide additional features (functions or data) that serve particular needs. For example, the ggplot2 package is developed by the RStudio team that provides nice features to plot data. We will have more examples of this later on, but first, let’s install and load the package so that we can use these features. More details will be provided in the data visualization section. # to install a package install.packages(&quot;ggplot2&quot;) # to load the package library(ggplot2) # use the ggplot() function to produce a plot # Sepal.Length is the horizontal axis # Sepal.Width is the vertical axis # Species labels are used as color ggplot(iris, aes(Sepal.Length, Sepal.Width, colour = Species)) + geom_point() You may also noticed that in our previous examples, all tables only displayed the first several rows. One may be interested in looking at the entire dataset, however, it would take too much space to display the whole table. Here is a package that would allow you to display it in a compact window. It also provides searching and sorting tools. You can integrate this into your R Markdown reports. library(DT) datatable(iris, filter = &quot;top&quot;, rownames = FALSE, options = list(pageLength = 5)) Often times, you may want to perform a new task and you don’t know what function can be used to achieve that. Google Search or Stack Overflow are probably your best friends. I used to encounter this problem: I have a list of objects, and each of them is a vector. I then need to extract the first element of all these vectors. However, doing this using a for-loop can be slow, and I am also interested in a cleaner code. So I found this post, which provided a simple answer: # create the list a = list(c(1,1,1), c(2,2,2), c(3,3,3)) # extract the first element in each vector of the list sapply(a, &quot;[[&quot;, 1) ## [1] 1 2 3 1.9 Practice questions Attach a new numerical column to the iris data, as the product of Petal.Length and Petal.Width and name the column as Petal.Prod. iris = cbind(iris, &quot;Petal.Prod&quot; = iris$Petal.Length*iris$Petal.Width) head(iris) Attach a new numerical column to the iris data, with value 1 if the observation is setosa, 2 for versicolor and 3 for virginica, and name the column as Species.Num. iris = cbind(iris, &quot;Species.Num&quot; = as.numeric(iris$Species)) head(iris) Change Species.Num to a factor variable such that it takes value “Type1” if the observation is setosa and “NA” otherwise. iris$Species.Num = as.factor(ifelse(iris$Species == &quot;setosa&quot;, &quot;Type1&quot;, &quot;NA&quot;)) head(iris) Define a function that takes in a numerical vector, and output the mean of that vector. Do this without using the mean() and sum() function. mymean &lt;- function(x) { sum = 0 for (i in 1:length(x)) sum = sum + x[i] return(sum = sum / length(x)) } x = 1:10 mymean(x) mean(x) "],["rmarkdown.html", "Chapter 2 RMarkdown 2.1 Basics and Resources 2.2 Formatting Text 2.3 Adding R Code 2.4 Importing Data 2.5 Working Directory 2.6 Plotting 2.7 Chunk Options 2.8 Adding Math with LaTeX 2.9 Output Options 2.10 Try It!", " Chapter 2 RMarkdown 2.1 Basics and Resources R Markdown is a built-in feature of RStudio. It integrates plain text with chunks of R code in to a single file, which is extremely useful when constructing class notes or building a website. A .rmd file can be compiled into nice-looking .html, .pdf, and .docx file. For example, this entire guide is created using R Markdown. With RStudio, you can install R Markdown from R console using the following code. Note that this should be automatically done the first time you create and compile a .rmd file in RStudio. # Install R Markdown from CRAN install.packages(&quot;rmarkdown&quot;) Again there are many online guides for R Markdown, and these may not be the best ones. R Markdown: The Definitive Guide R Markdown Cheat Sheet R Markdown Play-list (video) To get started, create an R Markdown template file by clicking File -&gt; New File -&gt; R Markdown... You can then Knit the template file and start to explore its features. Please note that this guide is provided in the .html format. However, your homework report should be in .pdf format. This can be done by selecting the Knit to PDF option from the Knit button. Again there are many online guides, and these may not be the best ones. R Markdown Cheat Sheet R Markdown Play-list (video) 2.2 Formatting Text Formatting text is easy. Bold can be done using ** or __ before and after the text. Italics can be done using * or _ before and after the text. For example, This is bold. This is italics. and this is bold italics. This text appears as monospaced. Unordered list element 1. Unordered list element 2. Unordered list element 3. Ordered list element 1. Ordered list element 2. Ordered list element 3. We could mix lists and links. Note that a link can be constructed in the format [display text](http link). If colors are desired, we can customize it using, for example, [\\textcolor{blue}{display text}](http link). But this only works in .pdf format. For .html, use &lt;span style=\"color: red;\"&gt;text&lt;/span&gt;. A default link: RMarkdown Documentation colored link 1: (Not shown because it only works in PDF) colored link 2: Table Generator (only works in HTML) Tables are sometimes tricky using Markdown. See the above link for a helpful Markdown table generator. Note that you can also adjust the alignment by using a : sign. A B C 1 2 3 Middle Left Right 2.3 Adding R Code So far we have only used Markdown to create the text part. This is useful by itself, but the real power of RMarkdown comes when we add R. There are two ways we can do this. We can use R code chunks, or run R inline. 2.3.1 R Chunks The following is an example of an R code chunk. Start the chunk with ```{r} and end with ```: ```{r} \\(\\quad\\) set.seed(123) \\(\\quad\\) rnorm(5) ``` This generates five random observations from the standard normal distribution. We also set the seed so that the results can be later on replicated. The result looks like the following set.seed(123) rnorm(5) ## [1] -0.56047565 -0.23017749 1.55870831 0.07050839 0.12928774 # define function get_sd = function(x, biased = FALSE) { n = length(x) - 1 * !biased sqrt((1 / n) * sum((x - mean(x)) ^ 2)) } # generate random sample data set.seed(42) (test_sample = rnorm(n = 10, mean = 2, sd = 5)) ## [1] 8.8547922 -0.8234909 3.8156421 5.1643130 4.0213416 1.4693774 9.5576100 1.5267048 ## [9] 12.0921186 1.6864295 # run function on generated data get_sd(test_sample) ## [1] 4.177244 There is a lot going on here. In the .Rmd file, notice the syntax that creates and ends the chunk. Also note that example_chunk is the chunk name. Everything between the start and end syntax must be valid R code. Chunk names are not necessary, but can become useful as your documents grow in size. In this example, we define a function, generate some random data in a reproducible manner, displayed the data, then ran our function. 2.3.2 Inline R R can also be run in the middle of the exposition. For example, the mean of the data we generated is 4.7364838. 2.4 Importing Data When using RMarkdown, any time you knit your document to its final form, say .html, a number of programs run in the background. Your current R environment seen in RStudio will be reset. Any objects you created while working interactively inside RStudio will be ignored. Essentially a new R session will be spawned in the background and the code in your document is run there from start to finish. For this reason, things such as importing data must be explicitly coded into your document. library(readr) example_data = read_table(&quot;data/skincancer.txt&quot;) The above loads the online file. In many cases, you will load a file that is locally stored in your own computer. In that case, you can either specify the full file path, or simply use, for example read_csv(\"filename.csv\") if that file is stored at your working directory. The working directory will usually be the directory that contains your .Rmd file. You are recommended to reference data in this manner. Note that we use the newer read_csv() from the readr package instead of the default read.csv(). 2.5 Working Directory Whenever R code is run, there is always a current working directory. This allows for relative references to external files, in addition to absolute references. Since the working directory when knitting a file is always the directory that contains the .Rmd file, it can be helpful to set the working directory inside RStudio to match while working interactively. To do so, select Session &gt; Set Working Directory &gt; To Source File Location while editing a .Rmd file. This will set the working directory to the path that contains the .Rmd. You can also use getwd() and setwd() to manipulate your working directory programmatically. These should only be used interactively. Using them inside an RMarkdown document would likely result in lessened reproducibility. As of recent RStudio updates, this practice is not always necessary when working interactively. If lines of code are being “Output Inline,” then the working directory is automatically the directory which contains the .Rmd file. 2.6 Plotting The following generates a simple plot, which displays the skin cancer mortality. By default, the figure is aligned on the left, with size 3 by 5 inches. plot(Mort ~ Lat, data = example_data) In our R introduction, we used ggplot2 to create a more interesting plot. You may also polish a plot with basic functions. Notice it is huge in the resulting document, since we have modified some chunk options (fig.height = 6, fig.width = 8) in the RMarkdown file to manipulate its size. plot(Mort ~ Lat, data = example_data, xlab = &quot;Latitude&quot;, ylab = &quot;Skin Cancer Mortality Rate&quot;, main = &quot;Skin Cancer Mortality vs. State Latitude&quot;, pch = 19, cex = 1.5, col = &quot;deepskyblue&quot;) But you can also notice that the labels and the plots becomes disproportional when the figure size is set too small. This can be resolved using a scaling option such as out.width = '60%, but enlarge the original figure size. We also align the figure at the center using fig.align = 'center' 2.7 Chunk Options We have already seen chunk options fig.height, fig.width, and out.width which modified the size of plots from a particular chunk. There are many chunk options, but we will discuss some others which are frequently used including; eval, echo, message, and warning. If you noticed, the plot above was displayed without showing the code. install.packages(&quot;rmarkdown&quot;) ?log View(mpg) Using eval = FALSE the above chunk displays the code, but it is not run. We’ve already discussed not wanting install code to run. The ? code pulls up documentation of a function. This will spawn a browser window when knitting, or potentially crash during knitting. Similarly, using View() is an issue with RMarkdown. Inside RStudio, this would pull up a window which displays the data. However, when knitting, R runs in the background and RStudio is not modifying the View() function. This, on OSX especially, usually causes knitting to fail. ## [1] &quot;Hello World!&quot; Above, we see output, but no code! This is done using echo = FALSE, which is often useful. x = 1:10 y = 1:10 summary(lm(y ~ x)) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.661e-16 -1.157e-16 4.273e-17 2.153e-16 4.167e-16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.123e-15 2.458e-16 4.571e+00 0.00182 ** ## x 1.000e+00 3.961e-17 2.525e+16 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.598e-16 on 8 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 6.374e+32 on 1 and 8 DF, p-value: &lt; 2.2e-16 The above code produces a warning, for reasons we will discuss later. Sometimes, in final reports, it is nice to hide these, which we have done here. message = FALSE and warning = FALSE can be used to do so. Messages are often created when loading packages to give the user information about the effects of loading the package. These should be suppressed in final reports. Be careful about suppressing these messages and warnings too early in an analysis as you could potentially miss important information! 2.8 Adding Math with LaTeX Another benefit of RMarkdown is the ability to add Latex for mathematics typesetting. Like R code, there are two ways we can include Latex; displaystyle and inline. Note that use of LaTeX is somewhat dependent on the resulting file format. For example, it cannot be used at all with .docx. To use it with .pdf you must have LaTeX installed on your machine. With .html the LaTeX is not actually rendered during knitting, but actually rendered in your browser using MathJax. 2.8.1 Displaystyle LaTeX Displaystyle is used for larger equations which appear centered on their own line. This is done by putting $$ before and after the mathematical equation. \\[ \\widehat \\sigma = \\sqrt{\\frac{1}{n - 1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\] 2.8.2 Inline LaTex We could mix LaTeX commands in the middle of exposition, for example: \\(t = 2\\). We could actually mix R with Latex as well! For example: \\(\\bar{x} = 4.7364838\\). 2.9 Output Options At the beginning of the document, there is a code which describes some metadata and settings of the document. The default code looks like title: &quot;R Notebook&quot; output: html_notebook You can easily add your name and date to it, and add a Table of Contents, using toc: yes. Note that the following code would specify the theme of an html file. title: &quot;My RMarkdown Template&quot; author: &quot;Your Name&quot; date: &quot;Aug 26, 2021&quot; output: html_document: toc: yes You can edit this yourself, or click the settings button at the top of the document and select Output Options.... Here you can explore other themes and syntax highlighting options, as well as many additional options. Using this method will automatically modify this information in the document. 2.10 Try It! Be sure to play with this document! Change it. Break it. Fix it. The best way to learn RMarkdown (or really almost anything) is to try, fail, then find out what you did wrong. RStudio has provided a number of beginner tutorials which have been greatly improved recently and detail many of the specifics potentially not covered in this document. RMarkdown is continually improving, and this document covers only the very basics. "],["visual-studio-code.html", "Chapter 3 Visual Studio Code 3.1 Basics and Resources", " Chapter 3 Visual Studio Code 3.1 Basics and Resources Visual Studio Code (VS Code) is another popular IDE for programming in pretty much all languages. With the recent development of GitHub Copilot (X), it makes VS Code a very attractive platform. Here is a place to get things started. Please note that in Windows, running the applications as Administrator is probably needed to install related components. Official Overview To get things working, you only need to Install R Install VS Code Install R Extension in VS Code Install the languageserver package install.packages(\"languageserver\") To install radian (for interactive R terminal), you need to Install Python and pip (if you have Anaconda, that should be included already) Use pip3 install -U radian to install radian Go to settings in VS Code, and find r.path, add your radian.exe file path to it "],["linear-algebra-basics.html", "Chapter 4 Linear Algebra Basics 4.1 Definition 4.2 Linear Regression 4.3 Matrix Inversion", " Chapter 4 Linear Algebra Basics You should already be familiar with some basic linear algebra concepts such as matrix and vector multiplications. Here we review some basic concepts and properties that will be used in this course. For the most part, they are used in deriving linear regression results. 4.1 Definition We usually use \\(\\mathbf{X}\\) to denote an \\(n \\times p\\) dimensional design matrix, where \\(n\\) is the number of observations and \\(p\\) is the number of variables. The columns of \\(\\mathbf{x}\\) are denoted as \\(\\mathbf{x}_1, \\ldots, \\mathbf{x}_p\\): \\[ \\mathbf{X}= \\begin{pmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{m1} &amp; x_{n2} &amp; \\cdots &amp; x_{np}\\\\ \\end{pmatrix} = \\begin{pmatrix} \\mathbf{x}_1 &amp; \\mathbf{x}_2 &amp; \\cdots &amp; \\mathbf{x}_p\\\\ \\end{pmatrix} \\] The column space, \\(\\cal{C}(\\mathbf{X})\\) of \\(\\mathbf{X}\\) is the set of all linear combinations of \\(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p\\), i.e., \\[c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 + \\cdots c_p \\mathbf{x}_p.\\] This is also called the span of these vectors, \\(\\text{span}(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_p)\\). Its orthogonal space is \\[\\{\\mathbf{v}: \\mathbf{X}^\\text{T}\\mathbf{v}= 0\\}.\\] 4.2 Linear Regression In many cases, we will be using linear regression as an example. This concerns regressing a vector of outcome \\(\\mathbf{y}\\) onto the column space of \\(\\mathbf{X}\\). Or in other words, finding a set of coefficients \\((c_1, \\ldots, c_p)\\) such that the Euclidean distance between \\(c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 + \\cdots c_p \\mathbf{x}_p\\) and \\(\\mathbf{y}\\) is minimized. One way to view this problem is to project the vector \\(\\mathbf{y}\\) onto this space \\(\\cal{C}(\\mathbf{X})\\). This can be done through a linear operator, called the projection matrix \\(\\mathbf{P}\\), defined as \\[ \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}, \\] provided that \\(\\mathbf{X}\\) has full column rank. Hence, the projection can be obtained as \\[ \\mathbf{P}\\mathbf{y}= \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}/ \\] Noticing that this is a linear combination of the columns of \\(\\mathbf{X}\\) since \\((\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}\\) is a \\(p-\\)vector, which is the vector \\((c_1, \\ldots, c_p)^\\text{T}\\). And if you are familiar with the solution from a linear regression, this is \\((\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}\\) is the solution, while \\(\\mathbf{P}\\mathbf{y}\\) is the fitted value. On the other hand, \\(\\mathbf{y}- \\mathbf{P}\\mathbf{y}\\) is the residual vector. There are some useful properties of a projection matrix \\(\\mathbf{P}\\) Idempotent: A projection matrix is idempotent, which means that multiplying it by itself leaves it unchanged: \\(\\mathbf{P}^2 = \\mathbf{P}\\) Symmetry (for Orthogonal Projections): If the projection is orthogonal, the matrix is symmetric: \\(\\mathbf{P}= \\mathbf{P}^T\\) Spectrum: The eigenvalues of a projection matrix are either 0 or 1. The number of eigenvalues equal to 1 is the rank of the matrix. Rank and Nullity: If \\(\\mathbf{P}\\) is a projection onto a subspace of dimension \\(k\\), then the rank of \\(\\mathbf{P}\\) is \\(k\\), and the nullity (dimension of the null space) is \\(n - k\\), where \\(n\\) is the dimension of the space onto which \\(\\mathbf{P}\\) projects. Orthogonal Complement: If \\(\\mathbf{P}\\) is the projection onto a subspace \\(U\\), then \\((\\mathbf{I}- \\mathbf{P})\\) is the projection onto the orthogonal complement of \\(U\\). These properties make projection matrices useful in various applications, including least squares regression, signal processing, and many areas of machine learning and statistics. 4.3 Matrix Inversion 4.3.1 Rank-one Update The Sherman-Morrison formula is a helpful result in linear algebra, especially in the context of statistical computations. It provides an expression for the inverse of a rank-one perturbation of a given invertible matrix. Here’s the formula in LaTeX form: \\[ (\\mathbf{A}+ \\mathbf{u}\\mathbf{v}^T)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{u}\\mathbf{v}^T \\mathbf{A}^{-1}}{1 + \\mathbf{v}^T \\mathbf{A}^{-1}\\mathbf{u}} \\] Here, \\(\\mathbf{A}\\) is an \\(n \\times n\\) invertible matrix, and \\(\\mathbf{u}\\) and $ $ are \\(n \\times 1\\) vectors. The denominator in the expression ensures that the resulting matrix is well-defined, and it has many applications in statistical computations, optimization, and other areas. 4.3.2 Rank-\\(k\\) Update The Woodbury Matrix Identity is another powerful result used in statistical computations and relates to the inversion of a rank-k correction of a matrix: \\[ (\\mathbf{A}+ \\mathbf{U}\\mathbf{C}\\mathbf{V}^T)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{C}^{-1} + \\mathbf{V}^T\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}^T\\mathbf{A}^{-1} \\] This can be seen as an extension of the Sherman-Morrison formula and can be particularly useful when dealing with high-dimensional data or large-scale computations. 4.3.3 2 \\(\\times\\) 2 Block Matrix Inversion A general 2x2 block matrix can be represented as: \\[ \\mathbf{M}= \\begin{bmatrix} \\mathbf{A}&amp; \\mathbf{B}\\\\ \\mathbf{C}&amp; \\mathbf{D}\\end{bmatrix} \\] If the matrix is invertible, its inverse can be expressed as: \\[ \\mathbf{M}^{-1} = \\begin{bmatrix} \\mathbf{A}^{-1} + \\mathbf{A}^{-1}\\mathbf{B}(\\mathbf{D}- \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B})^{-1}\\mathbf{C}\\mathbf{A}^{-1} &amp; -\\mathbf{A}^{-1}\\mathbf{B}(\\mathbf{D}- \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B})^{-1} \\\\ -(\\mathbf{D}- \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B})^{-1}\\mathbf{C}\\mathbf{A}^{-1} &amp; (\\mathbf{D}- \\mathbf{C}\\mathbf{A}^{-1}\\mathbf{B})^{-1} \\end{bmatrix} \\] This formula assumes that the required inverses exist. "],["optimization-basics.html", "Chapter 5 Optimization Basics 5.1 Basic Concept 5.2 Global vs. Local Optima 5.3 Example: Linear Regression using optim() 5.4 First and Second Order Properties 5.5 Algorithm 5.6 Second-order Methods 5.7 First-order Methods 5.8 Coordinate Descent 5.9 Stocastic Gradient Descent 5.10 Lagrangian Multiplier for Constrained Problems", " Chapter 5 Optimization Basics Optimization is heavily involved in statistics and machine learning. Almost all methods introduced in this book can be viewed as some form of optimization. It would be good to have some prior knowledge of it so that later chapters can use these concepts without difficulties. Especially, one should be familiar with concepts such as constrains, gradient methods, and be able to implement them using existing R functions. Since optimization is such a broad topic, we refer readers to Boyd and Vandenberghe (2004) and Nocedal and Wright (2006) for more further reading. We will use a slightly different set of notations in this Chapter so that we are consistent with the literature. This means that for the most part, we will use \\(x\\) as our parameter of interest and optimize a function \\(f(x)\\). This is in contrast to optimizing \\(\\theta\\) in a statistical model \\(f_\\theta(x)\\) where \\(x\\) is the observed data. However, in the example of linear regression, we may again switch back to the regular notation of \\(x^\\text{T} \\boldsymbol{\\beta}\\). These transitions will only happen under clear context and should not create ambiguity. 5.1 Basic Concept We usually consider a convex optimization problem (non-convex problems are a bit too involving although we will also see some examples of that), meaning that we optimize (minimize) a convex function in a convex domain. A convex function \\(f(\\mathbf{x})\\) maps some subset \\(C \\in \\mathbb{R}^p\\) into \\(\\mathbb{R}^p\\), but enjoys the property that \\[ f(t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_2) \\leq t f(\\mathbf{x}_1) + ( 1- t) f(\\mathbf{x}_2), \\] for all \\(t \\in [0, 1]\\) and any two points \\(\\mathbf{x}_1\\), \\(\\mathbf{x}_2\\) in the domain of \\(f\\). An example of convex function, from wikipedia Note that if you have a concave function (the bowl faces downwards) then \\(-f(\\mathbf{x})\\) would be convex. Examples of convex functions: Univariate functions: \\(x^2\\), \\(\\exp(x)\\), \\(-log(x)\\) Affine map: \\(a^\\text{T}\\mathbf{x}+ b\\) is both convex and concave A quadratic function \\(\\frac{1}{2}\\mathbf{x}^\\text{T}\\mathbf{A}\\mathbf{x}+ b^\\text{T}\\mathbf{x}+ c\\), if \\(\\mathbf{A}\\) is positive semidefinite All \\(p\\) norms are convex, following the Triangle inequality and properties of a norm. A sin function is neither convex or concave On the other hand, a convex set \\(C\\) means that if we have two points \\(x_1\\) and \\(x_2\\) in \\(C\\), the line segment joining these two points has to lie within \\(C\\), i.e., \\[\\mathbf{x}_1, \\mathbf{x}_2 \\in C \\quad \\Longrightarrow \\quad t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_2 \\in C,\\] for all \\(t \\in [0, 1]\\). An example of convex set Examples of convex set include Real line: \\(\\mathbb{R}\\) Norm ball: \\(\\{ \\mathbf{x}: \\lVert \\mathbf{x}\\rVert \\leq r \\}\\) Hyperplane: \\(\\{ \\mathbf{x}: a^\\text{T}\\mathbf{x}= b \\}\\) Consider a simple optimization problem: \\[ \\text{minimize} \\quad f(x_1, x_2) = x_1^2 + x_2^2\\] Clearly, \\(f(x_1, x_2)\\) is a convex function, and we know that the solution of this problem is \\(x_1 = x_2 = 0\\). However, the problem might be a bit more complicated if we restrict that in a certain (convex) region, for example, \\[\\begin{align} &amp;\\underset{x_1, x_2}{\\text{minimize}} &amp; \\quad f(x_1, x_2) &amp;= x_1^2 + x_2^2 \\\\ &amp;\\text{subject to} &amp; x_1 + x_2 &amp;\\leq -1 \\\\ &amp; &amp; x_1 + x_2 &amp;&gt; -2 \\end{align}\\] Here the convex set \\(C = \\{x_1, x_2 \\in \\mathbb{R}: x_1 + x_2 \\leq -1 \\,\\, \\text{and} \\,\\, x_1 + x_2 &gt; -2\\}\\). And our problem looks like the following, which attains it minimum at \\((-0.5, -0.5)\\). In general, we will be dealing with a problem in the form of \\[\\begin{align} &amp;\\underset{\\mathbf{x}}{\\text{minimize}} &amp; \\quad f(\\mathbf{x}) \\\\ &amp;\\text{subject to} &amp; g_i(\\mathbf{x}) &amp; \\leq 0, \\, i = 1,\\ldots, m \\\\ &amp; &amp; h_j(\\mathbf{x}) &amp;= 0, \\, j = 1,\\ldots, k \\end{align}\\] where \\(g_i(\\mathbf{x})\\)s are a set of inequality constrains, and \\(h_j(\\mathbf{x})\\)s are equality constrains. There are established result showing what type of constrains would lead to a convex set, but let’s assuming for now that we will be dealing a well behaved problem. We shall see in later chapters that many models such as, Lasso, Ridge and support vector machines can all be formulated into this form. 5.2 Global vs. Local Optima Although we would like to deal with convex optimization problems, non-convex problems appears more and more frequently. For example, deep learning models are almost always non-convex except overly simplified ones. However, for convex optimization problems, a local minimum is also a global minimum, i.e., a \\(x^\\ast\\) such that for any \\(x\\) in the feasible set, \\(f(x^\\ast) \\leq f(x)\\). This can be achieved by a variety of descent algorithms, to be introduced. However, for non-convex problems, we may still be interested in a local minimum, which satisfies that for any \\(x\\) in a neighboring set of \\(x^\\ast\\), \\(f(x^\\ast) \\leq f(x)\\). The comparison of these two cases can be demonstrated in the following plots. Again, a descent algorithm can help us find a local minimum, except for some very special cases, such as a saddle point. However, we will not discuss these issues in this book. 5.3 Example: Linear Regression using optim() Although completely not necessary, we may also view linear regression as an optimization problem. This is of course an unconstrained problem, meaning that \\(C \\in \\mathbb{R}^p\\). Such problems can be solved using the optim() function. Also, let’s temporarily switch back to the \\(\\boldsymbol{\\beta}\\) notation of parameters. Hence, if we observe a set of observations \\(\\{\\mathbf{x}_i, y_i\\}_{i = 1}^n\\), our optimization problem is to minimize the objection function, i.e., residual sum of squares (RSS): \\[\\begin{align} \\underset{\\boldsymbol{\\beta}}{\\text{minimize}} \\quad f(\\boldsymbol{\\beta}) = \\frac{1}{n} \\sum_i (y_i - \\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta})^2 \\\\ \\end{align}\\] We generate 200 random observations, and also write a function to calculate the RSS for any given \\(\\boldsymbol{\\beta}\\) values. The objective function looks like the following: # generate data from a simple linear model set.seed(20) n = 200 x &lt;- cbind(1, rnorm(n)) y &lt;- x %*% c(0.5, 1) + rnorm(n) # calculate the residual sum of squares for a grid of beta values rss &lt;- function(b, trainx, trainy) sum((trainy - trainx %*% b)^2) Now the question is how to solve this problem. The optim() function uses the following syntax: # The solution can be solved by any optimization algorithm lm.optim &lt;- optim(par = c(2, 2), fn = rss, trainx = x, trainy = y) The par argument specifies an initial value, in this case, \\(\\beta_0 = \\beta_1 = 2\\) The fn argument specifies the name of an R function that can calculate the objective function. Please note that the first argument in this function has be the parameter being optimized, i.e, \\(\\boldsymbol{\\beta}\\). Also, it must be a vector, not a matrix or other types. The arguments trainx = x, trainy = y specifies any additional arguments that the objective function fn, i.e., rss needs. It behaves the same as if you are supplying this to rss. lm.optim ## $par ## [1] 0.4532072 0.9236502 ## ## $value ## [1] 203.5623 ## ## $counts ## function gradient ## 63 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The result shows that the estimated parameters ($par) are 0.453 and 0.924, with a functional value 203.562. The convergence code is 0, meaning that the algorithm converged. The parameter estimates are almost the same as lm(), with small numerical errors. # The solution form lm() summary(lm(y ~ x - 1))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## x1 0.4530498 0.07177854 6.311772 1.773276e-09 ## x2 0.9236934 0.07226742 12.781602 1.136397e-27 What we will be introducing in the following are some basic approaches to solve such a numerical problem. We will start with unconstrained problems, then introduce constrained problems. 5.4 First and Second Order Properties These properties are usually applied to unconstrained optimization problems. They are essentially just describing the landscape around a point \\(\\mathbf{x}^\\ast\\) such that it becomes the local optimizer. Since we generally concerns a convex problem, a local solution is also the global solution. However, these properties are still generally applied when solving a non-convex problem. Note that these statements are multi-dimensional. First-Order Necessary Conditions: If \\(f\\) is continuously differentiable in an open neighborhood of local minimum \\(\\mathbf{x}^\\ast\\), then \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\). When we have a point \\(\\mathbf{x}^\\ast\\) with \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\), we call \\(\\mathbf{x}^\\ast\\) a stationary point. This is only a necessary condition, but not sufficient. Since example, \\(f(x) = x^3\\) has zero derivative at \\(x = 0\\), but this is not an optimizer. The figure in @ref(global_local) also contains such a point. TO further strengthen this, we have Second-order Necessary Conditions: If \\(f\\) is twice continuously differentiable in an open neighborhood of local minimum \\(\\mathbf{x}^\\ast\\), then \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) and \\(\\nabla^2 f(\\mathbf{x}^\\ast)\\) is positive semi-definite. This does rule out some cases, with a higher cost (\\(f\\) needs to be twice continuously differentiable). But requiring positive semi-definite would not ensure everything. The same example \\(f(x) = x^3\\) still satisfies this, but its not a local minimum. A positive definite \\(\\nabla^2 f(\\mathbf{x}^\\ast)\\) would be sufficient: Second-order Sufficient Conditions: \\(f\\) is twice continuously differentiable in an open neighborhood of \\(\\mathbf{x}^\\ast\\). If \\(\\nabla f(\\mathbf{x}^\\ast) = \\mathbf{0}\\) and \\(\\nabla^2 f(\\mathbf{x}^\\ast)\\) is positive definite, i.e., \\[ \\nabla^2 f(\\mathbf{x}) = \\left(\\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j}\\right) = \\mathbf{H}(\\mathbf{x}) \\succ 0 \\] then \\(\\mathbf{x}^\\ast\\) is a strict local minimizer of \\(f\\). Here \\(\\mathbf{H}(\\mathbf{x})\\) is called the Hessian matrix, which will be frequently used in second-order methods. 5.5 Algorithm Most optimization algorithms follow the same idea: starting from a point \\(\\mathbf{x}^{(0)}\\) (which is usually specified by the user) and move to a new point \\(\\mathbf{x}^{(1)}\\) that improves the objective function value. Repeatedly performing this to get a sequence of points \\(\\mathbf{x}^{(0)}, \\mathbf{x}^{(1)}, \\ldots\\) until the certain stopping criterion is reached. A stopping criterion could be Using the gradient of the objective function: \\(\\lVert \\nabla f(\\mathbf{x}^{(k)}) \\rVert &lt; \\epsilon\\) Using the (relative) change of distance: \\(\\lVert \\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)} \\rVert / \\lVert \\mathbf{x}^{(k-1)}\\rVert&lt; \\epsilon\\) or \\(\\lVert \\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)} \\rVert &lt; \\epsilon\\) Using the (relative) change of functional value: \\(| f(\\mathbf{x}^{(k)}) - f(\\mathbf{x}^{(k-1)})| &lt; \\epsilon\\) or \\(| f(\\mathbf{x}^{(k)}) - f(\\mathbf{x}^{(k-1)})| / |f(\\mathbf{x}^{(k)})| &lt; \\epsilon\\) Stop at a pre-specified number of iterations. Most algorithms differ in terms of how to move from the current point \\(\\mathbf{x}^{(k)}\\) to the next, better target point \\(\\mathbf{x}^{(k+1)}\\). This may depend on the smoothness or structure of \\(f\\), constrains on the domain, computational complexity, memory limitation, and many others. 5.6 Second-order Methods 5.6.1 Newton’s Method Now, let’s discuss several specific methods. One of the oldest one is Newton’s method. This is motivated form a quadratic approximation (essentially Taylor expansion) at a current point \\(\\mathbf{x}\\), \\[f(\\mathbf{x}^\\ast) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\text{T}(\\mathbf{x}^\\ast - \\mathbf{x}) + \\frac{1}{2} (\\mathbf{x}^\\ast - \\mathbf{x})^\\text{T}\\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x})\\] Our goal is to find a new stationary point \\(\\mathbf{x}^\\ast\\) such that \\(\\nabla f(\\mathbf{x}^\\ast) = 0\\). By taking derivative of the above equation on both sides, with respect to \\(\\mathbf{x}^\\ast\\), we need \\[0 = \\nabla f(\\mathbf{x}^\\ast) = 0 + \\nabla f(\\mathbf{x}) + \\mathbf{H}(\\mathbf{x}) (\\mathbf{x}^\\ast - \\mathbf{x}) \\] which leads to \\[\\mathbf{x}^\\ast = \\mathbf{x}- \\mathbf{H}(\\mathbf{x})^{-1} \\nabla f(\\mathbf{x}).\\] Hence, if we are currently at a point \\(\\mathbf{x}^{(k)}\\), we need to calculate the gradient \\(\\nabla f(\\mathbf{x}^{(k)})\\) and Hessian \\(\\mathbf{H}(\\mathbf{x})\\) at this point, then move to the new point using \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\mathbf{H}(\\mathbf{x}^{(k)})^{-1} \\nabla f(\\mathbf{x}^{(k)})\\). Some properties and things to concern regarding Newton’s method: Newton’s method is scale invariant, meaning that you do not need to worry about the step size. It is automatically taken care of by the Hessian matrix. However, in practice, the local approximation may not be accurate, which makes the new point \\(\\mathbf{x}^{(k+1)}\\) behaves differently than what we expect. Hence, it might still be safe to introduce a smaller step size \\(\\delta \\in (0, 1)\\) and move with \\[\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\delta \\, \\mathbf{H}(\\mathbf{x}^{(k)})^{-1} \\nabla f(\\mathbf{x}^{(k)})\\] There are also alternatives to take care of the step size. For example, line search is frequently used, which will try to find the optimal \\(\\delta\\) that minimizes the function \\[f(\\mathbf{x}^{(k)} + \\delta \\mathbf{v})\\] where the direction \\(\\mathbf{v}\\) in this case is \\(\\mathbf{v}= \\mathbf{H}(\\mathbf{x}^{(k)})^{-1} \\nabla f(\\mathbf{x}^{(k)})\\). It is also popular to use backtracking line search, which reduces the computational cost. The idea is to start with a large \\(\\delta\\) and gradually reduces it by a certain proportion if the new point doesn’t significantly improves, i.e., \\[f(\\mathbf{x}^{(k)} + \\delta \\mathbf{v}) &gt; f(\\mathbf{x}^{(k)}) - \\frac{1}{2} \\delta \\nabla f(\\mathbf{x}^{(k)})^\\text{T}\\mathbf{v}\\] Note that when the direction \\(\\mathbf{v}\\) is \\(\\mathbf{H}(\\mathbf{x}^{(k)})^{-1} \\nabla f(\\mathbf{x}^{(k)})\\), \\(\\nabla f(\\mathbf{x}^{(k)})^\\text{T}\\mathbf{v}\\) is essentially the norm defined by the Hessian matrix. When you do not have the explicit formula of Hessian and even the gradient, you may numerically approximate the derivative using the definition. For example, we could use \\[ \\frac{f(\\mathbf{x}^{(k)} + \\epsilon) - f(\\mathbf{x}^{(k)})}{\\epsilon} \\] with \\(\\epsilon\\) small enough, e.g., \\(10^{-5}\\). However, this is very costly for the Hessian matrix if the number of variables is large. 5.6.2 Quasi-Newton Methods Since the idea of Newton’s method is to solve a vector \\(\\mathbf{v}\\) such that \\[\\mathbf{H}(\\mathbf{x}^{(k)}) \\mathbf{v}= - \\nabla f(\\mathbf{x}^{(k)}), \\] If \\(\\mathbf{H}\\) is difficult to compute, we may use some matrix to substitute it. For example, if we simplify use the identity matrix \\(\\mathbf{I}\\), then this reduces to a first-order method to be introduced later. However, if we can get a good approximation, we can still solve this linear system and get to a better point. Then the question is how to obtain such matrix in a computationally inexpensive way. The Broyden–Fletcher–Goldfarb–Shanno (BFSG) algorithm is such an approach by iteratively updating its (inverse) estimation. The algorithm proceed as follows: Start with \\(x^{(0)}\\) and a positive definite matrix, e.g., \\(\\mathbf{B}^{(0)} = \\mathbf{I}\\) For \\(k = 0, 1, 2, \\ldots\\), Search a updating direction by solving the linear system \\(\\mathbf{B}^{(k)} \\mathbf{p}_k = - \\nabla f(\\mathbf{x}^{(k)})\\) Perform line search in the direction of \\(\\mathbf{v}_k\\) and obtain the next point \\(\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\delta \\mathbf{p}_k\\) Update the approximation by \\[ \\mathbf{B}^{(k+1)} = \\mathbf{B}^{(k)} + \\frac{\\mathbf{y}_k^\\text{T}\\mathbf{y}_{k}}{ \\mathbf{y}_{k}^\\text{T}\\mathbf{s}_{k} } - \\frac{\\mathbf{B}^{(k)}\\mathbf{s}_{k}\\mathbf{s}_{k}^\\text{T}{\\mathbf{B}^{(k)}}^\\text{T}}{\\mathbf{s}_{k}^\\text{T}\\mathbf{B}^{(k)} \\mathbf{s}_{k} }, \\] where \\(\\mathbf{y}_k = \\nabla f(\\mathbf{x}^{(k+1)}) - \\nabla f(\\mathbf{x}^{(k)})\\) and \\(\\mathbf{s}_{k} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}\\). The BFGS is performing a rank-two update by assuming that \\[ \\mathbf{B}^{(k+1)} = \\mathbf{B}^{(k)} + a \\mathbf{u}\\mathbf{u}^\\text{T}+ b \\mathbf{v}\\mathbf{v}^\\text{T},\\] Alternatives of such type of methods include the symmetric rank-one and Davidon-Fletcher-Powell (DFP) updates. 5.7 First-order Methods 5.7.1 Gradient Descent When simply using \\(\\mathbf{H}= \\mathbf{I}\\), we update \\[\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\delta \\nabla f(\\mathbf{x}^{(k)}).\\] However, it is then crucial to figure out the step size \\(\\delta\\). A step size too large may not even converge at all, however, a step size too small will take too many iterations to converge. Alternatively, line search could be used. 5.7.2 Gradient Descent Example: Linear Regression We use linear regression as an example. The objective function for linear regression is: \\[ \\ell(\\boldsymbol \\beta) = \\frac{1}{2n}||\\mathbf{y} - \\mathbf{X} \\boldsymbol \\beta ||^2 \\] with solution is \\[\\widehat{\\boldsymbol \\beta} = \\left(\\mathbf{X}^\\text{T}\\mathbf{X}\\right)^{-1} \\mathbf{X}^\\text{T} \\mathbf{y} \\] par(mfrow=c(1,1)) library(MASS) set.seed(3) n = 200 # create some data with linear model X = mvrnorm(n, c(0, 0), matrix(c(1,0.7, 0.7, 1), 2,2)) y = rnorm(n, mean = 2*X[,1] + X[,2]) beta1 &lt;- seq(-1, 4, 0.005) beta2 &lt;- seq(-1, 4, 0.005) allbeta &lt;- data.matrix(expand.grid(beta1, beta2)) rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), length(beta1), length(beta2)) # quantile levels for drawing contour quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75) # plot the contour contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) box() # the truth b = solve(t(X) %*% X) %*% t(X) %*% y points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 2) We use an optimization approach to solve this problem. By taking the derivative with respect to \\(\\boldsymbol \\beta\\), we have the gradient \\[ \\begin{align} \\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = -\\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^\\text{T} \\boldsymbol \\beta) x_i. \\end{align} \\] To perform the optimization, we will first set an initial beta value, say \\(\\boldsymbol \\beta = \\mathbf{0}\\) for all entries, then proceed with the update \\[ \\boldsymbol \\beta^\\text{new} = \\boldsymbol \\beta^\\text{old} - \\frac{\\partial \\ell(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} \\times \\delta.\\] Let’s set \\(\\delta = 0.2\\) for now. The following function performs gradient descent. # gradient descent function, which also record the path mylm_g &lt;- function(x, y, b0 = rep(0, ncol(x)), # initial value delta = 0.2, # step size epsilon = 1e-6, #stopping rule maxitr = 5000) # maximum iterations { if (!is.matrix(x)) stop(&quot;x must be a matrix&quot;) if (!is.vector(y)) stop(&quot;y must be a vector&quot;) if (nrow(x) != length(y)) stop(&quot;number of observations different&quot;) # initialize beta values allb = matrix(b0, 1, length(b0)) # iterative update for (k in 1:maxitr) { # the new beta value b1 = b0 + t(x) %*% (y - x %*% b0) * delta / length(y) # record the new beta allb = rbind(allb, as.vector(b1)) # stopping rule if (max(abs(b0 - b1)) &lt; epsilon) break; # reset beta0 b0 = b1 } if (k == maxitr) cat(&quot;maximum iteration reached\\n&quot;) return(list(&quot;allb&quot; = allb, &quot;beta&quot; = b1)) } # fit the model mybeta = mylm_g(X, y, b0 = c(0, 1)) par(bg=&quot;transparent&quot;) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() The descent path is very smooth because we choose a very small step size. However, if the step size is too large, we may observe unstable results or even unable to converge. For example, if We set \\(\\delta = 1\\) or \\(\\delta = 1.5\\). par(mfrow=c(1,2)) par(mar=c(2,2,2,2)) # fit the model with a larger step size mybeta = mylm_g(X, y, b0 = c(0, 1), delta = 1) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() # and even larger mybeta = mylm_g(X, y, b0 = c(0, 1), delta = 1.5, maxitr = 6) ## maximum iteration reached contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() 5.8 Coordinate Descent Instead of updating all parameters at a time, we can also update one parameter each time. The Gauss-Seidel style coordinate descent algorithm at the \\(k\\)th iteration will sequentially update all \\(p\\) parameters: \\[\\begin{align} x_1^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_1}}{\\arg\\min} \\quad f(\\color{OrangeRed}{x_1}, x_2^{(k)}, \\ldots, x_p^{(k)}) \\nonumber \\\\ x_2^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_2}}{\\arg\\min} \\quad f(x_1^{\\color{DodgerBlue}{(k+1)}}, \\color{OrangeRed}{\\mathbf{x}_2}, \\ldots, x_p^{(k)}) \\nonumber \\\\ \\cdots &amp;\\nonumber \\\\ x_p^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_p}}{\\arg\\min} \\quad f(x_1^{\\color{DodgerBlue}{(k+1)}}, x_2^{\\color{DodgerBlue}{(k+1)}}, \\ldots, \\color{OrangeRed}{x_p}) \\nonumber \\\\ \\end{align}\\] Note that after updating one coordinate, the new parameter value is used for updating the next coordinate. After we complete this loop, all \\(j\\) are updated to their new values, and we proceed to the next step. Another type of update is the Jacobi style, which can be performed in parallel at the \\(k\\)th iteration: \\[\\begin{align} x_1^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_1}}{\\arg\\min} \\quad f(\\color{OrangeRed}{x_1}, x_2^{(k)}, \\ldots, x_p^{(k)}) \\nonumber \\\\ x_2^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_2}}{\\arg\\min} \\quad f(x_1^{(k+1)}, \\color{OrangeRed}{\\mathbf{x}_2}, \\ldots, x_p^{(k)}) \\nonumber \\\\ \\cdots &amp;\\nonumber \\\\ x_p^{(k+1)} &amp;= \\underset{\\color{OrangeRed}{x_p}}{\\arg\\min} \\quad f(x_1^{(k+1)}, x_2^{(k+1)}, \\ldots, \\color{OrangeRed}{x_p}) \\nonumber \\\\ \\end{align}\\] For differentiable convex functions \\(f\\), we can ensure that if all parameters are optimized then the entire problem is also optimized. If \\(f\\) is not differentiable, we may have trouble (see the example on wiki). However, there are also cases where coordinate descent would still guarantee a convergence, e.g., a sperable case: \\[f(\\mathbf{x}) = g(\\mathbf{x}) + \\sum_{j=1}^p h_j(x_j)\\] This is the Lasso formulation which will be discussed in later section. 5.8.1 Coordinate Descent Example: Linear Regression Coordinate descent for linear regression is not really necessary. However, we will still use this as an example. Note that the update for a single parameter is \\[ \\underset{\\boldsymbol \\beta_j}{\\text{argmin}} \\frac{1}{2n} ||\\mathbf{y}- X_j \\beta_j - \\mathbf{X}_{(-j)} \\boldsymbol{\\beta}_{(-j)} ||^2 \\] where \\(\\mathbf{X}_{(-j)}\\) is the data matrix without the \\(j\\)th column. Note that when updating \\(\\beta_j\\) coordinate-wise, we can first calculate the residual defined as \\(\\mathbf{r} = \\mathbf{y} - \\mathbf{X}_{(-j)} \\boldsymbol \\beta_{(-j)}\\) which does not depend on \\(\\beta_j\\), and optimize the rest of the formula for \\(\\beta_j\\). This is essentially the same as performing a one-dimensional regression by regressing \\(\\mathbf{r}\\) on \\(X_j\\) and obtain the update. \\[ \\beta_j = \\frac{X_j^T \\mathbf{r}}{X_j^T X_j} \\] The coordinate descent usually does not involve choosing a step size. Note that the following function is NOT efficient because there are a lot of wasted calculations. It is only for demonstration purpose. Here we use the Gauss-Seidel style update. # gradient descent function, which also record the path mylm_c &lt;- function(x, y, b0 = rep(0, ncol(x)), epsilon = 1e-6, maxitr = 5000) { if (!is.matrix(x)) stop(&quot;x must be a matrix&quot;) if (!is.vector(y)) stop(&quot;y must be a vector&quot;) if (nrow(x) != length(y)) stop(&quot;number of observations different&quot;) # initialize beta values allb = matrix(b0, 1, length(b0)) # iterative update for (k in 1:maxitr) { # initiate a vector for new beta b1 = b0 for (j in 1:ncol(x)) { # calculate the residual r = y - x[, -j, drop = FALSE] %*% b1[-j] # update jth coordinate b1[j] = t(r) %*% x[,j] / (t(x[,j, drop = FALSE]) %*% x[,j]) # record the update allb = rbind(allb, as.vector(b1)) } if (max(abs(b0 - b1)) &lt; epsilon) break; # reset beta0 b0 = b1 } if (k == maxitr) cat(&quot;maximum iteration reached\\n&quot;) return(list(&quot;allb&quot; = allb, &quot;beta&quot; = b1)) } # fit the model mybeta = mylm_c(X, y, b0 = c(0, 3)) par(mfrow=c(1,1)) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() 5.9 Stocastic Gradient Descent The main advantage of using Stochastic Gradient Descent (SGD) is its computational speed. Calculating the gradient using all observations can be costly. Instead, we consider update the parameter based on a single observation. Hence, the gradient is defined as \\[ \\frac{\\partial \\ell_i(\\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta} = - (y_i - x_i^\\text{T} \\boldsymbol \\beta) x_i. \\] Compared with using all observations, this is \\(1/n\\) of the cost. However, because this is rater not accurate for each iteration, but can still converge in the long run. There is a decay rate involved in SGD step size. If the step size does not decreases to 0, the algorithm cannot converge. However, it also has to sum up to infinite to allow us to go as far as we can. For example, a choice could be \\(\\delta_k = 1/k\\), hence \\(\\sum \\delta_k = \\infty\\) and \\(\\sum \\delta_k^2 &lt; \\infty\\). # gradient descent function, which also record the path mylm_sgd &lt;- function(x, y, b0 = rep(0, ncol(x)), delta = 0.05, maxitr = 10) { if (!is.matrix(x)) stop(&quot;x must be a matrix&quot;) if (!is.vector(y)) stop(&quot;y must be a vector&quot;) if (nrow(x) != length(y)) stop(&quot;number of observations different&quot;) # initialize beta values allb = matrix(b0, 1, length(b0)) # iterative update for (k in 1:maxitr) { # going through all samples for (i in sample(1:nrow(x))) { # update based on the gradient of a single subject b0 = b0 + (y[i] - sum(x[i, ] * b0)) * x[i, ] * delta # record the update allb = rbind(allb, as.vector(b0)) # learning rate decay delta = delta * 1/k } } return(list(&quot;allb&quot; = allb, &quot;beta&quot; = b0)) } # fit the model mybeta = mylm_sgd(X, y, b0 = c(0, 1), maxitr = 3) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() 5.9.1 Mini-batch Stocastic Gradient Descent Instead of using just one observation, we could also consider splitting the data into several small “batches” and use one batch of sample to calculate the gradient at each iteration. # gradient descent function, which also record the path mylm_sgd_mb &lt;- function(x, y, b0 = rep(0, ncol(x)), delta = 0.3, maxitr = 20) { if (!is.matrix(x)) stop(&quot;x must be a matrix&quot;) if (!is.vector(y)) stop(&quot;y must be a vector&quot;) if (nrow(x) != length(y)) stop(&quot;number of observations different&quot;) # initiate batches with 10 observations each batch = sample(rep(1:floor(nrow(x)/10), length.out = nrow(x))) # initialize beta values allb = matrix(b0, 1, length(b0)) # iterative update for (k in 1:maxitr) { for (i in 1:max(batch)) # loop through batches { # update based on the gradient of a single subject b0 = b0 + t(x[batch==i, ]) %*% (y[batch==i] - x[batch==i, ] %*% b0) * delta / sum(batch==i) # record the update allb = rbind(allb, as.vector(b0)) # learning rate decay delta = delta * 1/k } } return(list(&quot;allb&quot; = allb, &quot;beta&quot; = b0)) } # fit the model mybeta = mylm_sgd_mb(X, y, b0 = c(0, 1), maxitr = 3) contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(mybeta$allb[,1], mybeta$allb[,2], type = &quot;b&quot;, col = &quot;red&quot;, pch = 19) points(b[1], b[2], pch = 19, col = &quot;blue&quot;, cex = 1.5) box() You may further play around with these tuning parameters to see how sensitive the optimization is to them. A stopping rule can be difficult to determine, hence in practice, early stop is also used. 5.10 Lagrangian Multiplier for Constrained Problems Constrained optimization problems appear very frequently. Both Lasso and Ridge regressions can be viewed as constrained problems, while support vector machines (SVM) is another example, which will be introduced later on. Let’s investigate this using a toy example. Suppose we have an optimization problem \\[\\text{minimize} \\quad f(x, y) = x^2 + y^2\\] \\[\\text{subj. to} \\quad g(x, y) = xy - 4 = 0\\] x &lt;- seq(-5, 5, 0.05) y &lt;- seq(-5, 5, 0.05) mygrid &lt;- data.matrix(expand.grid(x, y)) f &lt;- matrix(mygrid[,1]^2 + mygrid[,2]^2, length(x), length(y)) f2 &lt;- matrix(mygrid[,1]*mygrid[,2], length(x), length(y)) # plot the contour par(mar=c(2,2,2,2)) contour(x, y, f, levels = c(0.2, 1, 2, 4, 8, 16)) contour(x, y, f2, levels = 4, add = TRUE, col = &quot;blue&quot;, lwd = 2) box() lines(seq(1, 3, 0.01), 4- seq(1, 3, 0.01), type = &quot;l&quot;, col = &quot;darkorange&quot;, lwd = 3) points(2, 2, col = &quot;red&quot;, pch = 19, cex = 2) points(-2, -2, col = &quot;red&quot;, pch = 19, cex = 2) The problem itself is very simple. We know that the optimizer is the red dot. But an interesting point of view is to look at the level curves of the objective function. As it is growing (expanding), there is one point (the red dot) at which level curve barely touches the constrain curve (blue line). This should be the optimizer. But this also implies that the tangent line (orange line) of this leveling curve must coincide with the tangent line of the constraint. Noticing that the tangent line can be obtained by taking the derivative of the function, this observation implies that gradients of the two functions (the objective function and the constraint function) must be a multiple of the other. Hence, \\[ \\begin{align} &amp; \\bigtriangledown f = \\lambda \\bigtriangledown g \\\\ \\\\ \\Longrightarrow \\qquad &amp; \\begin{cases} 2x = \\lambda y &amp; \\text{by taking derivative w.r.t.} \\,\\, x\\\\ 2y = \\lambda x &amp; \\text{by taking derivative w.r.t.} \\,\\, y\\\\ xy - 4 = 0 &amp; \\text{the constraint itself} \\end{cases} \\end{align} \\] The three equations put together is very easy to solve. We have \\(x = y = 0\\) or \\(\\lambda = \\pm 2\\) based on the first two equations. The first one is not feasible based on the constraint. The second solution leads to two feasible solutions: \\(x = y = 2\\) or \\(x = y = -2\\). Hence, we now know that there are two solutions. Now, looking back at the equation \\(\\bigtriangledown f = \\lambda \\bigtriangledown g\\), this is simply the derivative of the Lagrangian function defined as \\[{\\cal L}(x, y, \\lambda) = f(x, y) - \\lambda g(x, y),\\] while solving for the solution of the constrained problem becomes finding the stationary point of the Lagrangian. Be aware that in some cases, the solution you found can be maximizers instead of minimizers. Hence, its necessary to compare all of them and see which one is smaller. Reference Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge university press. Nocedal, Jorge, and Stephen Wright. 2006. Numerical Optimization. Springer Science &amp; Business Media. "],["linear-regression-and-model-selection.html", "Chapter 6 Linear Regression and Model Selection 6.1 Example: real estate data 6.2 Notation and Basic Properties 6.3 Using the lm() Function 6.4 Model Selection Criteria 6.5 Model Selection Algorithms 6.6 Derivation of Marrows’ \\(C_p\\)", " Chapter 6 Linear Regression and Model Selection This chapter severs several purposes. First, we will review some basic knowledge of linear regression. This includes the concept of vector space, projection, which leads to estimating parameters of a linear regression. Most of these knowledge are covered in the prerequisite so you shouldn’t find these concepts too difficult to understand. Secondly, we will mainly use the lm() function as an example to demonstrate some features of R. This includes extracting results, visualizations, handling categorical variables, prediction and model selection. These concepts will be useful for other models. Finally, we will introduce several model selection criteria and algorithms to perform model selection. 6.1 Example: real estate data This Real Estate data (Yeh and Hsu 2018) is provided on the UCI machine learning repository. The goal of this dataset is to predict the unit house price based on six different covariates: date: The transaction date (for example, 2013.250=2013 March, 2013.500=2013 June, etc.) age: The house age (unit: year) distance: The distance to the nearest MRT station (unit: meter) stores: The number of convenience stores in the living circle on foot (integer) latitude: Latitude (unit: degree) longitude: Longitude (unit: degree) price: House price of unit area realestate = read.csv(&quot;data/realestate.csv&quot;, row.names = 1) library(DT) datatable(realestate, filter = &quot;top&quot;, rownames = FALSE, options = list(pageLength = 8)) dim(realestate) ## [1] 414 7 6.2 Notation and Basic Properties We usually denote the observed covariates data as the design matrix \\(\\mathbf{X}\\), with dimension \\(n \\times p\\). Hence in this case, the dimension of \\(\\mathbf{X}\\) is \\(414 \\times 7\\). The \\(j\\)th variable is simply the \\(j\\)th column of this matrix, which is denoted as \\(\\mathbf{x}_j\\). The outcome \\(\\mathbf{y}\\) (price) is a vector of length \\(414\\). Please note that we usually use a “bold” symbol to represent a vector, while for a single element (scalar), such as the \\(j\\)th variable of subject \\(i\\), we use \\(x_{ij}\\). A linear regression concerns modeling the relationship (in matrix form) \\[\\mathbf{y}_{n \\times 1} = \\mathbf{X}_{n \\times p} \\boldsymbol{\\beta}_{p \\times 1} + \\boldsymbol{\\epsilon}_{n \\times 1}\\] And we know that the solution is obtained by minimizing the residual sum of squares (RSS): \\[ \\begin{align} \\widehat{\\boldsymbol{\\beta}} &amp;= \\underset{\\boldsymbol{\\beta}}{\\arg\\min} \\sum_{i=1}^n \\left(y_i - x_i^\\text{T}\\boldsymbol{\\beta}\\right)^2 \\\\ &amp;= \\underset{\\boldsymbol{\\beta}}{\\arg\\min} \\big( \\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\big)^\\text{T}\\big( \\mathbf y - \\mathbf{X} \\boldsymbol \\beta \\big) \\end{align} \\] Classic solution can be obtained by taking the derivative of RSS w.r.t \\(\\boldsymbol{\\beta}\\) and set it to zero. This leads to the well known normal equation: \\[ \\begin{align} \\frac{\\partial \\text{RSS}}{\\partial \\boldsymbol{\\beta}} &amp;= -2 \\mathbf{X}^\\text{T}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) \\doteq 0 \\\\ \\Longrightarrow \\quad \\mathbf{X}^\\text{T}\\mathbf{y}&amp;= \\mathbf{X}^\\text{T}\\mathbf{X}\\boldsymbol{\\beta} \\end{align} \\] Assuming that \\(\\mathbf{X}\\) is full rank, then \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is invertible. Then, we have \\[ \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\mathbf{y} \\] Some additional concepts are frequently used. The fitted values \\(\\widehat{\\mathbf{y}}\\) are essentially the prediction of the original \\(n\\) training data points: \\[ \\begin{align} \\widehat{\\mathbf{y}} =&amp; \\mathbf{X}\\boldsymbol{\\beta}\\\\ =&amp; \\underbrace{\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}}_{\\mathbf{H}} \\mathbf{y}\\\\ \\doteq&amp; \\mathbf{H}_{n \\times n} \\mathbf{y} \\end{align} \\] where \\(\\mathbf{H}\\) is called the “hat” matrix. It is a projection matrix that projects any vector (\\(\\mathbf{y}\\) in our case) onto the column space of \\(\\mathbf{X}\\). A project matrix enjoys two properties Symmetric: \\(\\mathbf{H}^\\text{T}= \\mathbf{H}\\) Idempotent \\(\\mathbf{H}\\mathbf{H}= \\mathbf{H}\\) The residuals \\(\\mathbf{r}\\) can also be obtained using the hat matrix: \\[ \\mathbf{r}= \\mathbf{y}- \\widehat{\\mathbf{y}} = (\\mathbf{I}- \\mathbf{H}) \\mathbf{y}\\] From the properties of a projection matrix, we also know that \\(\\mathbf{r}\\) should be orthogonal to any vector from the column space of \\(\\mathbf{X}\\). Hence, \\[\\mathbf{X}^\\text{T}\\mathbf{r}= \\mathbf{0}_{p \\times 1}\\] The residuals is also used to estimate the error variance: \\[\\widehat\\sigma^2 = \\frac{1}{n-p} \\sum_{i=1}^n r_i^2 = \\frac{\\text{RSS}}{n-p}\\] When the data are indeed generated from a linear model, and with suitable conditions on the design matrix and random errors \\(\\boldsymbol{\\epsilon}\\), we can conclude that \\(\\widehat{\\boldsymbol{\\beta}}\\) is an unbiased estimator of \\(\\boldsymbol{\\beta}\\). Its variance-covariance matrix satisfies \\[ \\begin{align} \\text{Var}(\\widehat{\\boldsymbol{\\beta}}) &amp;= \\text{Var}\\big( (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\mathbf{y}\\big) \\nonumber \\\\ &amp;= \\text{Var}\\big( (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}(\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\epsilon}) \\big) \\nonumber \\\\ &amp;= \\text{Var}\\big( (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\boldsymbol{\\epsilon}) \\big) \\nonumber \\\\ &amp;= (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{I}\\sigma^2 \\nonumber \\\\ &amp;= (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\sigma^2 \\end{align} \\] All of the above mentioned results are already implemented in R through the lm() function to fit a linear regression. 6.3 Using the lm() Function Let’s consider a simple regression that uses age and distance to model price. We will save the fitted object as lm.fit lm.fit = lm(price ~ age + distance, data = realestate) This syntax contains three components: data = specifies the dataset The outcome variable should be on the left hand side of ~ The covariates should be on the right hand side of ~ To look at the detailed model fitting results, use the summary() function lm.summary = summary(lm.fit) lm.summary ## ## Call: ## lm(formula = price ~ age + distance, data = realestate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.032 -4.742 -1.037 4.533 71.930 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.8855858 0.9677644 51.547 &lt; 2e-16 *** ## age -0.2310266 0.0420383 -5.496 6.84e-08 *** ## distance -0.0072086 0.0003795 -18.997 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.73 on 411 degrees of freedom ## Multiple R-squared: 0.4911, Adjusted R-squared: 0.4887 ## F-statistic: 198.3 on 2 and 411 DF, p-value: &lt; 2.2e-16 This shows that both age and distance are highly significant for predicting the price. In fact, this fitted object (lm.fit) and the summary object (lm.summary) are both saved as a list. This is pretty common to handle an output object with many things involved. We may peek into this object to see what are provided using a $ after the object. The str() function can also display all the items in a list. str(lm.summary) Usually, printing out the summary is sufficient. However, further details can be useful for other purposes. For example, if we interested in the residual vs. fits plot, we may use plot(lm.fit$fitted.values, lm.fit$residuals, xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;, col = &quot;darkorange&quot;, pch = 19, cex = 0.5) It seems that the error variance is not constant (as a function of the fitted values), hence additional techniques may be required to handle this issue. However, that is beyond the scope of this book. 6.3.1 Adding Covariates It is pretty simple if we want to include additional variables. This is usually done by connecting them with the + sign on the right hand side of ~. R also provide convenient ways to include interactions and higher order terms. The following code with the interaction term between age and distance, and a squared term of distance should be self-explanatory. lm.fit2 = lm(price ~ age + distance + age*distance + I(distance^2), data = realestate) summary(lm.fit2) ## ## Call: ## lm(formula = price ~ age + distance + age * distance + I(distance^2), ## data = realestate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.117 -4.160 -0.594 3.548 69.683 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.454e+01 1.099e+00 49.612 &lt; 2e-16 *** ## age -2.615e-01 4.931e-02 -5.302 1.87e-07 *** ## distance -1.603e-02 1.133e-03 -14.152 &lt; 2e-16 *** ## I(distance^2) 1.907e-06 2.416e-07 7.892 2.75e-14 *** ## age:distance 8.727e-06 4.615e-05 0.189 0.85 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.939 on 409 degrees of freedom ## Multiple R-squared: 0.5726, Adjusted R-squared: 0.5684 ## F-statistic: 137 on 4 and 409 DF, p-value: &lt; 2.2e-16 If you choose to include all covariates presented in the data, then simply use . on the right hand side of ~. However, you should always be careful when doing this because some dataset would contain meaningless variables such as subject ID. lm.fit3 = lm(price ~ ., data = realestate) 6.3.2 Categorical Variables The store variable has several different values. We can see that it has 11 different values. One strategy is to model this as a continuous variable. However, we may also consider to discretize it. For example, we may create a new variable, say store.cat, defined as follows table(realestate$stores) ## ## 0 1 2 3 4 5 6 7 8 9 10 ## 67 46 24 46 31 67 37 31 30 25 10 # define a new factor variable realestate$store.cat = as.factor((realestate$stores &gt; 0) + (realestate$stores &gt; 4)) table(realestate$store.cat) ## ## 0 1 2 ## 67 147 200 levels(realestate$store.cat) = c(&quot;None&quot;, &quot;Several&quot;, &quot;Many&quot;) head(realestate$store.cat) ## [1] Many Many Many Many Many Several ## Levels: None Several Many This variable is defined as a factor, which is often used for categorical variables. Since this variable has three different categories, if we include it in the linear regression, it will introduce two additional variables (using the third as the reference): lm.fit3 = lm(price ~ age + distance + store.cat, data = realestate) summary(lm.fit3) ## ## Call: ## lm(formula = price ~ age + distance + store.cat, data = realestate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.656 -5.360 -0.868 3.913 76.797 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 43.712887 1.751608 24.956 &lt; 2e-16 *** ## age -0.229882 0.040095 -5.733 1.91e-08 *** ## distance -0.005404 0.000470 -11.497 &lt; 2e-16 *** ## store.catSeveral 0.838228 1.435773 0.584 0.56 ## store.catMany 8.070551 1.646731 4.901 1.38e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.279 on 409 degrees of freedom ## Multiple R-squared: 0.5395, Adjusted R-squared: 0.535 ## F-statistic: 119.8 on 4 and 409 DF, p-value: &lt; 2.2e-16 There are usually two types of categorical variables: Ordinal: the numbers representing each category is ordered, e.g., how many stores in the neighborhood. Oftentimes nominal data can be treated as a continuous variable. Nominal: they are not ordered and can be represented using either numbers or letters, e.g., ethnic group. The above example is treating store.cat as a nominal variable, and the lm() function is using dummy variables for each category. This should be our default approach to handle nominal variables. 6.4 Model Selection Criteria We will use the diabetes dataset from the lars package as a demonstration of model selection. Ten baseline variables include age, sex, body mass index, average blood pressure, and six blood serum measurements. These measurements were obtained for each of n = 442 diabetes patients, as well as the outcome of interest, a quantitative measure of disease progression one year after baseline. More details are available in Efron et al. (2004). Our goal is to select a linear model, preferably with a small number of variables, that can predict the outcome. To select the best model, commonly used strategies include Marrow’s \\(C_p\\), AIC (Akaike information criterion) and BIC (Bayesian information criterion). Further derivations will be provide at a later section. # load the diabetes data library(lars) ## Loaded lars 1.3 data(diabetes) diab = data.frame(cbind(diabetes$x, &quot;Y&quot; = diabetes$y)) # fit linear regression with all covariates lm.fit = lm(Y~., data=diab) The idea of model selection is to apply some penalty on the number of parameters used in the model. In general, they are usually in the form of \\[\\text{Goodness-of-Fit} + \\text{Complexity Penality}\\] 6.4.1 Using Marrows’ \\(C_p\\) For example, the Marrows’ \\(C_p\\) criterion minimize the following quantity (a derivation is provided at Section 6.6): \\[\\text{RSS} + 2 p \\widehat\\sigma_{\\text{full}}^2\\] Note that the \\(\\sigma_{\\text{full}}^2\\) refers to the residual variance estimation based on the full model, i.e., will all variables. Hence, this formula cannot be used when \\(p &gt; n\\) because you would not be able to obtain a valid estimation of \\(\\sigma_{\\text{full}}^2\\). Nonetheless, we can calculate this quantity with the diabetes dataset # number of variables (including intercept) p = 11 n = nrow(diab) # obtain residual sum of squares RSS = sum(residuals(lm.fit)^2) # use the formula directly to calculate the Cp criterion Cp = RSS + 2*p*summary(lm.fit)$sigma^2 Cp ## [1] 1328502 We can compare this with another sub-model, say, with just age and glu: lm.fit_sub = lm(Y~ age + glu, data=diab) # obtain residual sum of squares RSS_sub = sum(residuals(lm.fit_sub)^2) # use the formula directly to calculate the Cp criterion Cp_sub = RSS_sub + 2*3*summary(lm.fit)$sigma^2 Cp_sub ## [1] 2240019 Comparing this with the previous one, the full model is better. 6.4.2 Using AIC and BIC Calculating the AIC and BIC criteria in R is a lot simpler, with the existing functions. The AIC score is given by \\[-2 \\text{Log-likelihood} + 2 p,\\] while the BIC score is given by \\[-2 \\text{Log-likelihood} + \\log(n) p,\\] Interestingly, when assuming that the error distribution is Gaussian, the log-likelihood part is just a function of the RSS. In general, AIC performs similarly to \\(C_p\\), while BIC tend to select a much smaller set due to the larger penalty. Theoretically, both AIC and \\(C_p\\) are interested in the prediction error, regardless of whether the model is specified correctly, while BIC is interested in selecting the true set of variables, while assuming that the true model is being considered. The AIC score can be done using the AIC() function. We can match this result by writing out the normal density function and plug in the estimated parameters. Note that this requires one additional parameter, which is the variance. Hence the total number of parameters is 12. We can calculate this with our own code: # ?AIC # a build-in function for calculating AIC using -2log likelihood AIC(lm.fit) ## [1] 4795.985 # Match the result n*log(RSS/n) + n + n*log(2*pi) + 2 + 2*p ## [1] 4795.985 Alternatively, the extractAIC() function can calculate both AIC and BIC. However, note that the n + n*log(2*pi) + 2 part in the above code does not change regardless of how many parameters we use. Hence, this quantify does not affect the comparison between different models. Then we can safely remove this part and only focus on the essential ones. # ?extractAIC # AIC for the full model extractAIC(lm.fit) ## [1] 11.000 3539.643 n*log(RSS/n) + 2*p ## [1] 3539.643 # BIC for the full model extractAIC(lm.fit, k = log(n)) ## [1] 11.000 3584.648 n*log(RSS/n) + log(n)*p ## [1] 3584.648 Now, we can compare AIC or BIC using of two different models and select whichever one that gives a smaller value. For example the AIC of the previous sub-model is # AIC for the sub-model extractAIC(lm.fit_sub) ## [1] 3.000 3773.077 6.5 Model Selection Algorithms In previous examples, we have to manually fit two models and calculate their respective selection criteria and compare them. This is a rather tedious process if we have many variables and a huge number of combinations to consider. To automatically compare different models and select the best one, there are two common computational approaches: best subset regression and step-wise regression. As their name suggest, the best subset selection will exhaust all possible combination of variables, while the step-wise regression would adjust the model by adding or subtracting one variable at a time to reach the best model. 6.5.1 Best Subset Selection with leaps Since the penalty is only affected by the number of variables, we may first choose the best model with the smallest RSS for each model size, and then compare across these models by attaching the penalty terms of their corresponding sizes. The leaps package can be used to calculate the best model of each model size. It essentially performs an exhaustive search, however, still utilizing some tricks to skip some really bad models. Note that the leaps package uses the data matrix directly, instead of specifying a formula. library(leaps) # The package specifies the X matrix and outcome y vector RSSleaps = regsubsets(x = as.matrix(diab[, -11]), y = diab[, 11]) summary(RSSleaps, matrix=T) ## Subset selection object ## 10 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## sex FALSE FALSE ## bmi FALSE FALSE ## map FALSE FALSE ## tc FALSE FALSE ## ldl FALSE FALSE ## hdl FALSE FALSE ## tch FALSE FALSE ## ltg FALSE FALSE ## glu FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## age sex bmi map tc ldl hdl tch ltg glu ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; The results is summarized in a matrix, with each row representing a model size. The \"*\" sign indicates that the variable is include in the model for the corresponding size. Hence, there should be only one of such in the first row, two in the second row, etc. By default, the algorithm would only consider models up to size 8. This is controlled by the argument nvmax. If we want to consider larger model sizes, then set this to a larger number. However, be careful that this many drastically increase the computational cost. # Consider maximum of 10 variables RSSleaps = regsubsets(x = as.matrix(diab[, -11]), y = diab[, 11], nvmax = 10) summary(RSSleaps,matrix=T) ## Subset selection object ## 10 Variables (and intercept) ## Forced in Forced out ## age FALSE FALSE ## sex FALSE FALSE ## bmi FALSE FALSE ## map FALSE FALSE ## tc FALSE FALSE ## ldl FALSE FALSE ## hdl FALSE FALSE ## tch FALSE FALSE ## ltg FALSE FALSE ## glu FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: exhaustive ## age sex bmi map tc ldl hdl tch ltg glu ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; # Obtain the matrix that indicates the variables sumleaps = summary(RSSleaps, matrix = T) # This object includes the RSS results, which is needed to calculate the scores sumleaps$rss ## [1] 1719582 1416694 1362708 1331430 1287879 1271491 1267805 1264712 1264066 1263983 # This matrix indicates whether a variable is in the best model(s) sumleaps$which ## (Intercept) age sex bmi map tc ldl hdl tch ltg glu ## 1 TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## 2 TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## 3 TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## 4 TRUE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE TRUE FALSE ## 5 TRUE FALSE TRUE TRUE TRUE FALSE FALSE TRUE FALSE TRUE FALSE ## 6 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE ## 7 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE ## 8 TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE ## 9 TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## 10 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # The package automatically produces the Cp statistic sumleaps$cp ## [1] 148.352561 47.072229 30.663634 21.998461 9.148045 5.560162 6.303221 7.248522 ## [9] 9.028080 11.000000 We can calculate different model selection criteria with the best models of each size. The model fitting result already produces the \\(C_p\\) and BIC results. However, please note that both quantities are modified slightly. For the \\(C_p\\) statistics, the quantity is divided by the estimated error variance, and also adjust for the sample size. For the BIC, the difference is a constant regardless of the model size. Hence these difference do will not affect the model selection result because the modification is the same regardless of the number of variables. modelsize=apply(sumleaps$which,1,sum) Cp = sumleaps$rss/(summary(lm.fit)$sigma^2) + 2*modelsize - n; AIC = n*log(sumleaps$rss/n) + 2*modelsize; BIC = n*log(sumleaps$rss/n) + modelsize*log(n); # Comparing the Cp scores cbind(&quot;Our Cp&quot; = Cp, &quot;leaps Cp&quot; = sumleaps$cp) ## Our Cp leaps Cp ## 1 148.352561 148.352561 ## 2 47.072229 47.072229 ## 3 30.663634 30.663634 ## 4 21.998461 21.998461 ## 5 9.148045 9.148045 ## 6 5.560162 5.560162 ## 7 6.303221 6.303221 ## 8 7.248522 7.248522 ## 9 9.028080 9.028080 ## 10 11.000000 11.000000 # Comparing the BIC results. The difference is a constant, # which is the score of an intercept model cbind(&quot;Our BIC&quot; = BIC, &quot;leaps BIC&quot; = sumleaps$bic, &quot;Difference&quot; = BIC-sumleaps$bic, &quot;Intercept Score&quot; = n*log(sum((diab[,11] - mean(diab[,11]))^2/n))) ## Our BIC leaps BIC Difference Intercept Score ## 1 3665.879 -174.1108 3839.99 3839.99 ## 2 3586.331 -253.6592 3839.99 3839.99 ## 3 3575.249 -264.7407 3839.99 3839.99 ## 4 3571.077 -268.9126 3839.99 3839.99 ## 5 3562.469 -277.5210 3839.99 3839.99 ## 6 3562.900 -277.0899 3839.99 3839.99 ## 7 3567.708 -272.2819 3839.99 3839.99 ## 8 3572.720 -267.2702 3839.99 3839.99 ## 9 3578.585 -261.4049 3839.99 3839.99 ## 10 3584.648 -255.3424 3839.99 3839.99 Finally, we may select the best model, using any of the criteria. The following code would produced a plot to visualize it. We can see that BIC selects 6 variables, while both AIC and \\(C_p\\) selects 7. # Rescale Cp, AIC and BIC to (0,1). inrange &lt;- function(x) { (x - min(x)) / (max(x) - min(x)) } Cp = inrange(Cp) BIC = inrange(BIC) AIC = inrange(AIC) plot(range(modelsize), c(0, 0.4), type=&quot;n&quot;, xlab=&quot;Model Size (with Intercept)&quot;, ylab=&quot;Model Selection Criteria&quot;, cex.lab = 1.5) points(modelsize, Cp, col = &quot;green4&quot;, type = &quot;b&quot;, pch = 19) points(modelsize, AIC, col = &quot;orange&quot;, type = &quot;b&quot;, pch = 19) points(modelsize, BIC, col = &quot;purple&quot;, type = &quot;b&quot;, pch = 19) legend(&quot;topright&quot;, legend=c(&quot;Cp&quot;, &quot;AIC&quot;, &quot;BIC&quot;), col=c(&quot;green4&quot;, &quot;orange&quot;, &quot;purple&quot;), lty = rep(1, 3), pch = 19, cex = 1.7) 6.5.2 Step-wise regression using step() The idea of step-wise regression is very simple: we start with a certain model (e.g. the intercept or the full mode), and add or subtract one variable at a time by making the best decision to improve the model selection score. The step() function implements this procedure. The following example starts with the full model and uses AIC as the selection criteria (default of the function). After removing several variables, the model ends up with six predictors. # k = 2 (AIC) is default; step(lm.fit, direction=&quot;both&quot;, k = 2) ## Start: AIC=3539.64 ## Y ~ age + sex + bmi + map + tc + ldl + hdl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - age 1 82 1264066 3537.7 ## - hdl 1 663 1264646 3537.9 ## - glu 1 3080 1267064 3538.7 ## - tch 1 3526 1267509 3538.9 ## &lt;none&gt; 1263983 3539.6 ## - ldl 1 5799 1269782 3539.7 ## - tc 1 10600 1274583 3541.3 ## - sex 1 45000 1308983 3553.1 ## - ltg 1 56015 1319998 3556.8 ## - map 1 72103 1336086 3562.2 ## - bmi 1 179028 1443011 3596.2 ## ## Step: AIC=3537.67 ## Y ~ sex + bmi + map + tc + ldl + hdl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - hdl 1 646 1264712 3535.9 ## - glu 1 3001 1267067 3536.7 ## - tch 1 3543 1267608 3536.9 ## &lt;none&gt; 1264066 3537.7 ## - ldl 1 5751 1269817 3537.7 ## - tc 1 10569 1274635 3539.4 ## + age 1 82 1263983 3539.6 ## - sex 1 45831 1309896 3551.4 ## - ltg 1 55963 1320029 3554.8 ## - map 1 73850 1337915 3560.8 ## - bmi 1 179079 1443144 3594.2 ## ## Step: AIC=3535.9 ## Y ~ sex + bmi + map + tc + ldl + tch + ltg + glu ## ## Df Sum of Sq RSS AIC ## - glu 1 3093 1267805 3535.0 ## - tch 1 3247 1267959 3535.0 ## &lt;none&gt; 1264712 3535.9 ## - ldl 1 7505 1272217 3536.5 ## + hdl 1 646 1264066 3537.7 ## + age 1 66 1264646 3537.9 ## - tc 1 26840 1291552 3543.2 ## - sex 1 46382 1311094 3549.8 ## - map 1 73536 1338248 3558.9 ## - ltg 1 97509 1362221 3566.7 ## - bmi 1 178537 1443249 3592.3 ## ## Step: AIC=3534.98 ## Y ~ sex + bmi + map + tc + ldl + tch + ltg ## ## Df Sum of Sq RSS AIC ## - tch 1 3686 1271491 3534.3 ## &lt;none&gt; 1267805 3535.0 ## - ldl 1 7472 1275277 3535.6 ## + glu 1 3093 1264712 3535.9 ## + hdl 1 738 1267067 3536.7 ## + age 1 0 1267805 3537.0 ## - tc 1 26378 1294183 3542.1 ## - sex 1 44686 1312491 3548.3 ## - map 1 82154 1349959 3560.7 ## - ltg 1 102520 1370325 3567.3 ## - bmi 1 189970 1457775 3594.7 ## ## Step: AIC=3534.26 ## Y ~ sex + bmi + map + tc + ldl + ltg ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1271491 3534.3 ## + tch 1 3686 1267805 3535.0 ## + glu 1 3532 1267959 3535.0 ## + hdl 1 395 1271097 3536.1 ## + age 1 11 1271480 3536.3 ## - ldl 1 39378 1310869 3545.7 ## - sex 1 41858 1313349 3546.6 ## - tc 1 65237 1336728 3554.4 ## - map 1 79627 1351119 3559.1 ## - bmi 1 190586 1462077 3594.0 ## - ltg 1 294094 1565585 3624.2 ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ltg ## 152.1 -226.5 529.9 327.2 -757.9 538.6 804.2 We can also use different settings, such as which model to start with, which is the minimum/maximum model, and do we allow to adding/subtracting. # use BIC (k = log(n))instead of AIC # trace = 0 will suppress the output of intermediate steps step(lm.fit, direction=&quot;both&quot;, k = log(n), trace=0) ## ## Call: ## lm(formula = Y ~ sex + bmi + map + tc + ldl + ltg, data = diab) ## ## Coefficients: ## (Intercept) sex bmi map tc ldl ltg ## 152.1 -226.5 529.9 327.2 -757.9 538.6 804.2 # Start with an intercept model, and use forward selection (adding only) step(lm(Y~1, data=diab), scope=list(upper=lm.fit, lower=~1), direction=&quot;forward&quot;, trace=0) ## ## Call: ## lm(formula = Y ~ bmi + ltg + map + tc + sex + ldl, data = diab) ## ## Coefficients: ## (Intercept) bmi ltg map tc sex ldl ## 152.1 529.9 804.2 327.2 -757.9 -226.5 538.6 We can see that these results are slightly different from the best subset selection. So which is better? Of course the best subset selection is better because it considers all possible candidates, which step-wise regression may stuck at a sub-optimal model, while adding and subtracting any variable do not benefit further. Hence, the results of step-wise regression may be unstable. On the other hand, best subset selection not really feasible for high-dimensional problems because of the computational cost. 6.6 Derivation of Marrows’ \\(C_p\\) Suppose we have a set of training data \\({\\cal D}_n = \\{x_i, \\color{DodgerBlue}{y_i}\\}_{i=1}^n\\) and a set of testing data, with the same covariates \\({\\cal D}_n^\\ast = \\{x_i, \\color{OrangeRed}{y_i^\\ast}\\}_{i=1}^n\\). Hence, this is an in-sample prediction problem. However, the \\(\\color{OrangeRed}{y_i^\\ast}\\)s are newly observed. Assuming that the data are generated from a linear model, i.e., in vector form, \\[\\color{DodgerBlue}{\\mathbf{y}}= \\boldsymbol{\\mu}+ \\color{DodgerBlue}{\\mathbf{e}}= \\mathbf{X}\\boldsymbol{\\beta}+ \\color{DodgerBlue}{\\mathbf{e}},\\] and \\[\\color{OrangeRed}{\\mathbf{y}^\\ast}= \\boldsymbol{\\mu}+ \\color{OrangeRed}{\\mathbf{e}^\\ast}= \\mathbf{X}\\boldsymbol{\\beta}+ \\color{OrangeRed}{\\mathbf{e}^\\ast},\\] where the error terms are i.i.d with mean 0 and variance \\(\\sigma^2\\). We want to know what is the best model that predicts \\(\\color{OrangeRed}{\\mathbf{y}^\\ast}\\). Let’s look at the testing error first: \\[\\begin{align} \\text{E}[\\color{OrangeRed}{\\text{Testing Error}}] =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{y}^\\ast}- \\mathbf{X}\\color{DodgerBlue}{\\widehat{\\boldsymbol{\\beta}}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\color{OrangeRed}{\\mathbf{y}^\\ast}- \\mathbf{X}\\boldsymbol{\\beta}) + (\\mathbf{X}\\boldsymbol{\\beta}- \\mathbf{X}\\color{DodgerBlue}{\\widehat{\\boldsymbol{\\beta}}}) \\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{e}^\\ast}\\rVert^2 + \\text{E}\\lVert \\mathbf{X}(\\color{DodgerBlue}{\\widehat{\\boldsymbol{\\beta}}}- \\boldsymbol{\\beta}) \\rVert^2 \\\\ =&amp; ~\\color{OrangeRed}{n \\sigma^2} + \\text{E}\\big[ \\text{Trace}\\big( (\\color{DodgerBlue}{\\widehat{\\boldsymbol{\\beta}}}- \\boldsymbol{\\beta})^\\text{T}\\mathbf{X}^\\text{T}\\mathbf{X}(\\color{DodgerBlue}{\\widehat{\\boldsymbol{\\beta}}}- \\boldsymbol{\\beta}) \\big) \\big] \\\\ =&amp; ~\\color{OrangeRed}{n \\sigma^2} + \\text{Trace}\\big(\\mathbf{X}^\\text{T}\\mathbf{X}\\text{Cov}(\\color{DodgerBlue}{\\widehat{\\boldsymbol{\\beta}}})\\big) \\\\ =&amp; ~\\color{OrangeRed}{n \\sigma^2} + \\color{DodgerBlue}{p \\sigma^2}. \\end{align}\\] In the above, we used properties \\(\\text{Trace}(ABC) = \\text{Trace}(CAB)\\) \\(\\text{E}[\\text{Trace}(A)] = \\text{Trace}(\\text{E}[A])\\) On the other hand, the training error is \\[\\begin{align} \\text{E}[\\color{DodgerBlue}{\\text{Training Error}}] =&amp; ~\\text{E}\\lVert \\mathbf{r}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H}) \\color{DodgerBlue}{\\mathbf{y}}\\lVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})(\\mathbf{X}\\boldsymbol{\\beta}+ \\color{DodgerBlue}{\\mathbf{e}}) \\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})\\color{DodgerBlue}{\\mathbf{e}}\\rVert^2 \\\\ =&amp; ~\\text{E}[\\text{Trace}(\\color{DodgerBlue}{\\mathbf{e}}^\\text{T}(\\mathbf{I}- \\mathbf{H})^\\text{T}(\\mathbf{I}- \\mathbf{H}) \\color{DodgerBlue}{\\mathbf{e}})]\\\\ =&amp; ~\\text{Trace}((\\mathbf{I}- \\mathbf{H})^\\text{T}(\\mathbf{I}- \\mathbf{H}) \\text{Cov}(\\color{DodgerBlue}{\\mathbf{e}})]\\\\ =&amp; ~\\color{DodgerBlue}{(n - p) \\sigma^2}. \\end{align}\\] In the above, we further used properties \\(\\mathbf{H}\\) and \\(\\mathbf{I}- \\mathbf{H}\\) are projection matrices \\(\\mathbf{H}\\mathbf{X}= \\mathbf{X}\\) If we contrast the two results above, the difference between the training and testing errors is \\(2 p \\sigma^2\\). Hence, if we can obtain a valid estimation of \\(\\sigma^2\\), then the training error plus \\(2 p \\widehat{\\sigma}^2\\) is a good approximation of the testing error, which we want to minimize. And that is exactly what Marrows’ \\(C_p\\) does. We can also generalize this result to the case when the underlying model is not a linear model. Assume that \\[\\color{DodgerBlue}{\\mathbf{y}}= f(\\mathbf{X}) + \\color{DodgerBlue}{\\mathbf{e}}= \\boldsymbol{\\mu}+ \\color{DodgerBlue}{\\mathbf{e}},\\] and \\[\\color{OrangeRed}{\\mathbf{y}^\\ast}= f(\\mathbf{X}) + \\color{OrangeRed}{\\mathbf{e}^\\ast}= \\boldsymbol{\\mu}+ \\color{OrangeRed}{\\mathbf{e}^\\ast}.\\] In this case, a linear model would not estimate \\(\\boldsymbol{\\mu}\\). Instead, it is only capable to produce the best linear approximation of \\(\\boldsymbol{\\mu}\\) using the columns in \\(\\mathbf{X}\\), which is \\(\\mathbf{H}\\boldsymbol{\\mu}\\), the projection of \\(\\boldsymbol{\\mu}\\) on the column space of \\(\\mathbf{X}\\). In general, \\(\\mathbf{H}\\boldsymbol{\\mu}\\neq \\boldsymbol{\\mu}\\), and the remaining part \\(\\boldsymbol{\\mu}- \\mathbf{H}\\boldsymbol{\\mu}\\) is called bias. This is a new concept that will appear frequently in this book. Selection variables will essentially trade between bias and variance of a model. The following derivation shows this phenomenon: \\[\\begin{align} \\text{E}[\\color{OrangeRed}{\\text{Testing Error}}] =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{y}^\\ast}- \\mathbf{X}\\color{DodgerBlue}{\\widehat{\\boldsymbol{\\beta}}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{y}^\\ast}- \\mathbf{H}\\color{DodgerBlue}{\\mathbf{y}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\color{OrangeRed}{\\mathbf{y}^\\ast}- \\boldsymbol{\\mu}) + (\\boldsymbol{\\mu}- \\mathbf{H}\\boldsymbol{\\mu}) + (\\mathbf{H}\\boldsymbol{\\mu}- \\mathbf{H}\\color{DodgerBlue}{\\mathbf{y}}) \\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{y}^\\ast}- \\boldsymbol{\\mu}\\rVert^2 + \\text{E}\\lVert \\boldsymbol{\\mu}- \\mathbf{H}\\boldsymbol{\\mu}\\rVert^2 + \\text{E}\\lVert \\mathbf{H}\\boldsymbol{\\mu}- \\mathbf{H}\\color{DodgerBlue}{\\mathbf{y}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{OrangeRed}{\\mathbf{e}^\\ast}\\rVert^2 + \\text{E}\\lVert \\boldsymbol{\\mu}- \\mathbf{H}\\boldsymbol{\\mu}\\rVert^2 + \\text{E}\\lVert \\mathbf{H}\\color{DodgerBlue}{\\mathbf{e}}\\rVert^2 \\\\ =&amp; ~\\color{OrangeRed}{n \\sigma^2} + \\text{Bias}^2 + \\color{DodgerBlue}{p \\sigma^2}, \\end{align}\\] while the training error is \\[\\begin{align} \\text{E}[\\color{DodgerBlue}{\\text{Training Error}}] =&amp; ~\\text{E}\\lVert \\color{DodgerBlue}{\\mathbf{y}}- \\mathbf{X}\\color{DodgerBlue}{\\widehat{\\boldsymbol{\\beta}}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert \\color{DodgerBlue}{\\mathbf{y}}- \\mathbf{H}\\color{DodgerBlue}{\\mathbf{y}}\\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})(\\boldsymbol{\\mu}+ \\color{DodgerBlue}{\\mathbf{e}}) \\rVert^2 \\\\ =&amp; ~\\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})\\boldsymbol{\\mu}\\rVert^2 + \\text{E}\\lVert (\\mathbf{I}- \\mathbf{H})\\color{DodgerBlue}{\\mathbf{e}}\\rVert^2\\\\ =&amp; ~\\text{Bias}^2 + \\color{DodgerBlue}{(n - p) \\sigma^2}. \\end{align}\\] We can notice again that the difference is \\(2p\\sigma^2\\). Note that this is regardless of whether the linear model is correct or not. Reference Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. “Least Angle Regression.” The Annals of Statistics 32 (2): 407–99. Yeh, I-Cheng, and Tzu-Kuang Hsu. 2018. “Building Real Estate Valuation Models with Comparative Approach Through Case-Based Reasoning.” Applied Soft Computing 65: 260–71. "],["ridge-regression.html", "Chapter 7 Ridge Regression 7.1 Motivation: Correlated Variables and Convexity 7.2 Ridge Penalty and the Reduced Variation 7.3 Bias and Variance of Ridge Regression 7.4 Degrees of Freedom 7.5 Using the lm.ridge() function 7.6 Cross-validation 7.7 Leave-one-out cross-validation 7.8 The glmnet package", " Chapter 7 Ridge Regression Ridge regression was proposed by Hoerl and Kennard (1970), but is also a special case of Tikhonov regularization. The essential idea is very simple: Knowing that the ordinary least squares (OLS) solution is not unique in an ill-posed problem, i.e., \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is not invertible, a ridge regression adds a ridge (diagonal matrix) on \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\): \\[\\widehat{\\boldsymbol{\\beta}}^\\text{ridge} = (\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y},\\] It provides a solution of linear regression when multicollinearity happens, especially when the number of variables is larger than the sample size. Alternatively, this is also the solution of a regularized least square estimator. We add an \\(\\ell_2\\) penalty to the residual sum of squares, i.e., \\[ \\begin{align} \\widehat{\\boldsymbol{\\beta}}^\\text{ridge} =&amp; \\arg\\min_{\\boldsymbol{\\beta}} (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})^\\text{T}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) + n \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_2^2\\\\ =&amp; \\arg\\min_{\\boldsymbol{\\beta}} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^\\text{T}\\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^p \\beta_j^2, \\end{align} \\] for some penalty \\(\\lambda &gt; 0\\). Another approach that leads to the ridge regression is a constraint on the \\(\\ell_2\\) norm of the parameters, which will be introduced in the next Chapter. Ridge regression is used extensively in genetic analyses to address “small-\\(n\\)-large-\\(p\\)” problems. We will start with a motivation example and then discuss the bias-variance trade-off issue. 7.1 Motivation: Correlated Variables and Convexity Ridge regression has many advantages. Most notably, it can address highly correlated variables. From an optimization point of view, having highly correlated variables means that the objective function (\\(\\ell_2\\) loss) becomes “flat” along certain directions in the parameter domain. This can be seen from the following example, where the true parameters are both \\(1\\) while the estimated parameters concludes almost all effects to the first variable. You can change different seed to observe the variability of these parameter estimates and notice that they are quite large. Instead, if we fit a ridge regression, the parameter estimates are relatively stable. library(MASS) set.seed(2) n = 30 # create highly correlated variables and a linear model X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2)) y = rnorm(n, mean = X[,1] + X[,2]) # compare parameter estimates summary(lm(y~X-1))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## X1 1.8461255 1.294541 1.42608527 0.1648987 ## X2 0.0990278 1.321283 0.07494822 0.9407888 # note that the true parameters are all 1&#39;s # Be careful that the `lambda` parameter in lm.ridge is our (n*lambda) lm.ridge(y~X-1, lambda=5) ## X1 X2 ## 0.9413221 0.8693253 The variance of both \\(\\beta_1\\) and \\(\\beta_2\\) are quite large. This is expected because we know from linear regression that the variance of \\(\\widehat{\\boldsymbol{\\beta}}\\) is \\(\\sigma^2 (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\). However, since the columns of \\(\\mathbf{X}\\) are highly correlated, the smallest eigenvalue of \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is close to 0, making the largest eigenvalue of \\((\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}\\) very large. This can also be interpreted through an optimization point of view. The objective function for an OLS estimator is demonstrated in the following. beta1 &lt;- seq(0, 3, 0.005) beta2 &lt;- seq(-1, 2, 0.005) allbeta &lt;- data.matrix(expand.grid(beta1, beta2)) rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2), X, y), length(beta1), length(beta2)) # quantile levels for drawing contour quanlvl = c(0.01, 0.025, 0.05, 0.2, 0.5, 0.75) # plot the contour contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) box() # the truth points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) # the data betahat &lt;- coef(lm(y~X-1)) points(betahat[1], betahat[2], pch = 19, col = &quot;blue&quot;, cex = 2) Over many simulation runs, the solution lies around the line of \\(\\beta_1 + \\beta_2 = 2\\). # the truth plot(NA, NA, xlim = c(-1, 3), ylim = c(-1, 3)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) # generate many datasets in a simulation for (i in 1:200) { X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2)) y = rnorm(n, mean = X[,1] + X[,2]) betahat &lt;- solve(t(X) %*% X) %*% t(X) %*% y points(betahat[1], betahat[2], pch = 19, col = &quot;blue&quot;, cex = 0.5) } 7.2 Ridge Penalty and the Reduced Variation If we add a ridge regression penalty, the contour is forced to be more convex due to the added eigenvalues on \\(\\mathbf{X}^\\mathbf{X}\\), making the eignvalues of \\((\\mathbf{X}^\\mathbf{X})^{-1}\\) smaller. Here is a plot of the Ridge \\(\\ell_2\\) penalty. Hence, by adding this to the OLS objective function, the solution is more stable. This may be interpreted in several different ways such as: 1) the objective function is more convex; 2) the variance of the estimator is smaller. However, this causes some bias too. Choosing the tuning parameter is a balance of the bias-variance trade-off, which will be discussed in the following. par(mfrow=c(1, 2)) # adding a L2 penalty to the objective function rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + b %*% b, X, y), length(beta1), length(beta2)) # the ridge solution bh = solve(t(X) %*% X + diag(2)) %*% t(X) %*% y contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) points(bh[1], bh[2], pch = 19, col = &quot;blue&quot;, cex = 2) box() # adding a larger penalty rss &lt;- matrix(apply(allbeta, 1, function(b, X, y) sum((y - X %*% b)^2) + 10*b %*% b, X, y), length(beta1), length(beta2)) bh = solve(t(X) %*% X + 10*diag(2)) %*% t(X) %*% y # the ridge solution contour(beta1, beta2, rss, levels = quantile(rss, quanlvl)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) points(bh[1], bh[2], pch = 19, col = &quot;blue&quot;, cex = 2) box() We can check the ridge solution over many simulation runs par(mfrow=c(1, 1)) # the truth plot(NA, NA, xlim = c(-1, 3), ylim = c(-1, 3)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) # generate many datasets in a simulation for (i in 1:200) { X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2)) y = rnorm(n, mean = X[,1] + X[,2]) # betahat &lt;- solve(t(X) %*% X + 2*diag(2)) %*% t(X) %*% y betahat &lt;- lm.ridge(y ~ X - 1, lambda = 2)$coef points(betahat[1], betahat[2], pch = 19, col = &quot;blue&quot;, cex = 0.5) } This effect is gradually changing as we increase the penalty level. The following simulation shows how the variation of \\(\\boldsymbol{\\beta}\\) changes. We show this with two penalty values, and see how the estimated parameters are away from the truth. par(mfrow = c(1, 2)) # small penalty plot(NA, NA, xlim = c(-1, 3), ylim = c(-1, 3)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) # generate many datasets in a simulation for (i in 1:200) { X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2)) y = rnorm(n, mean = X[,1] + X[,2]) betahat &lt;- lm.ridge(y ~ X - 1, lambda = 2)$coef points(betahat[1], betahat[2], pch = 19, col = &quot;blue&quot;, cex = 0.5) } # large penalty plot(NA, NA, xlim = c(-1, 3), ylim = c(-1, 3)) points(1, 1, pch = 19, col = &quot;red&quot;, cex = 2) # generate many datasets in a simulation for (i in 1:200) { X = mvrnorm(n, c(0, 0), matrix(c(1,0.99, 0.99, 1), 2,2)) y = rnorm(n, mean = X[,1] + X[,2]) # betahat &lt;- solve(t(X) %*% X + 30*diag(2)) %*% t(X) %*% y betahat &lt;- lm.ridge(y ~ X - 1, lambda = 30)$coef points(betahat[1], betahat[2], pch = 19, col = &quot;blue&quot;, cex = 0.5) } 7.3 Bias and Variance of Ridge Regression We can set a relationship between Ridge and OLS, assuming that the OLS estimator exist. \\[\\begin{align} \\widehat{\\boldsymbol{\\beta}}^\\text{ridge} =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}\\\\ =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} (\\mathbf{X}^\\text{T}\\mathbf{X}) \\color{OrangeRed}{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}}\\\\ =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} (\\mathbf{X}^\\text{T}\\mathbf{X}) \\color{OrangeRed}{\\widehat{\\boldsymbol{\\beta}}^\\text{ols}} \\end{align}\\] This leads to a biased estimator (since the OLS estimator is unbiased) if we use any nonzero \\(\\lambda\\). As \\(\\lambda \\rightarrow 0\\), the ridge solution is eventually the same as OLS As \\(\\lambda \\rightarrow \\infty\\), \\(\\widehat{\\boldsymbol{\\beta}}^\\text{ridge} \\rightarrow 0\\) It can be easier to analyze a case with \\(\\mathbf{X}^\\text{T}\\mathbf{X}= n \\mathbf{I}\\), i.e, with standardized and orthogonal columns in \\(\\mathbf{X}\\). Note that in this case, each \\(\\beta_j^{\\text{ols}}\\) is just the projection of \\(\\mathbf{y}\\) onto \\(\\mathbf{x}_j\\), the \\(j\\)th column of the design matrix. We also have \\[\\begin{align} \\widehat{\\boldsymbol{\\beta}}^\\text{ridge} =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} (\\mathbf{X}^\\text{T}\\mathbf{X}) \\widehat{\\boldsymbol{\\beta}}^\\text{ols}\\\\ =&amp; (\\mathbf{I}+ \\lambda \\mathbf{I})^{-1}\\widehat{\\boldsymbol{\\beta}}^\\text{ols}\\\\ =&amp; (1 + \\lambda)^{-1} \\widehat{\\boldsymbol{\\beta}}^\\text{ols}\\\\ \\Longrightarrow \\widehat{\\beta}_j^{\\text{ridge}} =&amp; \\frac{1}{1 + \\lambda} \\widehat{\\beta}_j^\\text{ols} \\end{align}\\] Then in this case, the bias and variance of the ridge estimator can be explicitly expressed. Here for the bias term, we assume that the OLS estimator is unbiased, i.e., \\(\\text{E}(\\widehat{\\beta}_j^\\text{ols}) = \\beta_j\\). Then \\(\\text{Bias}(\\widehat{\\beta}_j^{\\text{ridge}}) = E[\\widehat{\\beta}_j^{\\text{ridge}} - \\beta_j] = \\frac{-\\lambda}{1 + \\lambda} \\beta_j\\) (not zero) \\(\\text{Var}(\\widehat{\\beta}_j^{\\text{ridge}}) = \\frac{1}{(1 + \\lambda)^2} \\text{Var}(\\widehat{\\beta}_j^\\text{ols})\\) (reduced from OLS) Of course, we can ask the question: is it worth it? We could proceed with a simple analysis of the MSE of \\(\\beta\\) (dropping \\(j\\) for simplicity). Note that this is also the MSE for predicting the expected value of a subject (\\(x^\\text{T}\\beta\\)) with covariate \\(x = 1\\). \\[\\begin{align} \\text{MSE}(\\beta) &amp;= \\text{E}(\\widehat{\\beta} - \\beta)^2 \\\\ &amp;= \\text{E}[\\widehat{\\beta} - \\text{E}(\\widehat{\\beta})]^2 + \\text{E}[\\widehat{\\beta} - \\beta]^2 \\\\ &amp;= \\text{E}[\\widehat{\\beta} - \\text{E}(\\widehat{\\beta})]^2 + 0 + [\\text{E}(\\widehat{\\beta}) - \\beta]^2 \\\\ &amp;= \\text{Var}(\\widehat{\\beta}) + \\text{Bias}^2. \\end{align}\\] This bias-variance breakdown formula will appear multiple times in later lectures. Now, plugging in the results developed earlier based on the orthogonal design matrix, and investigate the derivative of the MSE of the Ridge estimator, we have \\[\\begin{align} \\frac{\\partial \\text{MSE}(\\widehat{\\beta}^\\text{ridge})}{ \\partial \\lambda} =&amp; \\frac{\\partial}{\\partial \\lambda} \\left[ \\frac{1}{(1+\\lambda)^2} \\text{Var}(\\widehat{\\beta}^\\text{ols}) + \\frac{\\lambda^2}{(1 + \\lambda)^2} \\beta^2 \\right] \\\\ =&amp; \\frac{2}{(1+\\lambda)^3} \\left[ \\lambda \\beta^2 - \\text{Var}(\\widehat{\\beta}^\\text{ols}) \\right] \\end{align}\\] Note that when the derivative is negative, increasing \\(\\lambda\\) would decrease the MSE. This implies that we can reduce the MSE by choosing a small \\(\\lambda\\). However, we are not able to know the best choice of \\(\\lambda\\) since the true \\(\\beta\\) is unknown. One the other hand, the situation is much more involving when the columns in \\(\\mathbf{X}\\) are not orthogonal. The following analysis helps to understand a non-orthogonal case. It is essentially re-organizing the columns of \\(\\mathbf{X}\\) into its principal components so that they are still orthogonal. Let’s first take a singular value decomposition (SVD) of \\(\\mathbf{X}\\), with \\(\\mathbf{X}= \\mathbf{U}\\mathbf{D}\\mathbf{V}^\\text{T}\\), then the columns in \\(\\mathbf{U}\\) form an orthogonal basis and columns in \\(\\mathbf{U}\\mathbf{D}\\) are the principal components and \\(\\mathbf{V}\\) defines the principal directions. In addition, we have \\(n \\widehat{\\boldsymbol \\Sigma} = \\mathbf{X}^\\text{T}\\mathbf{X}= \\mathbf{V}\\mathbf{D}^2 \\mathbf{V}^\\text{T}\\). Assuming that \\(p &lt; n\\), and \\(\\mathbf{X}\\) has full column ranks, then the Ridge estimator fitted \\(\\mathbf{y}\\) value can be decomposed as \\[\\begin{align} \\widehat{\\mathbf{y}}^\\text{ridge} =&amp; \\mathbf{X}\\widehat{\\beta}^\\text{ridge} \\\\ =&amp; \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda)^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}\\\\ =&amp; \\mathbf{U}\\mathbf{D}\\mathbf{V}^\\text{T}( \\mathbf{V}\\mathbf{D}^2 \\mathbf{V}^\\text{T}+ n \\lambda \\mathbf{V}\\mathbf{V}^\\text{T})^{-1} \\mathbf{V}\\mathbf{D}\\mathbf{U}^\\text{T}\\mathbf{y}\\\\ =&amp; \\mathbf{U}\\mathbf{D}^2 (n \\lambda + \\mathbf{D}^2)^{-1} \\mathbf{U}^\\text{T}\\mathbf{y}\\\\ =&amp; \\sum_{j = 1}^p \\mathbf{u}_j \\left( \\frac{d_j^2}{n \\lambda + d_j^2} \\mathbf{u}_j^\\text{T}\\mathbf{y}\\right), \\end{align}\\] where \\(d_j\\) is the \\(j\\)th eigenvalue of the PCA. Hence, the Ridge regression fitted value can be understood as Perform PCA of \\(\\mathbf{X}\\) Project \\(\\mathbf{y}\\) onto the PCs Shrink the projection \\(\\mathbf{u}_j^\\text{T}\\mathbf{y}\\) by the factor \\(d_j^2 / (n \\lambda + d_j^2)\\) Reassemble the PCs using all the shrunken length Hence, the bias-variance notion can be understood as the trade-off on these derived directions \\(\\mathbf{u}_j\\) and their corresponding parameters \\(\\mathbf{u}_j^\\text{T}\\mathbf{y}\\). 7.4 Degrees of Freedom We know that for a linear model, the degrees of freedom (DF) is simply the number of parameters used. There is a formal definition, using \\[\\begin{align} \\text{DF}(\\widehat{f}) =&amp; \\frac{1}{\\sigma^2} \\sum_{i = 1}^n \\text{Cov}(\\widehat{y}_i, y_i)\\\\ =&amp; \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\widehat{\\mathbf{y}}, \\mathbf{y})] \\end{align}\\] We can check that for a linear regression (assuming the intercept is already included in \\(\\mathbf{X}\\)), the DF is \\[\\begin{align} \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\widehat{\\mathbf{y}}^\\text{ols}, \\mathbf{y})] =&amp; \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}, \\mathbf{y})] \\\\ =&amp; \\text{Trace}(\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}) \\\\ =&amp; \\text{Trace}(\\mathbf{I}_{p\\times p})\\\\ =&amp; p \\end{align}\\] For the Ridge regression, we can perform the same analysis on ridge regression. \\[\\begin{align} \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\widehat{\\mathbf{y}}^\\text{ridge}, \\mathbf{y})] =&amp; \\frac{1}{\\sigma^2} \\text{Trace}[\\text{Cov}(\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}, \\mathbf{y})] \\\\ =&amp; \\text{Trace}(\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X}+ n \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}) \\\\ =&amp; \\text{Trace}(\\mathbf{U}\\mathbf{D}\\mathbf{V}^\\text{T}( \\mathbf{V}\\mathbf{D}^2 \\mathbf{V}^\\text{T}+ n \\lambda \\mathbf{V}\\mathbf{V}^\\text{T})^{-1} \\mathbf{V}\\mathbf{D}\\mathbf{U}^\\text{T})\\\\ =&amp; \\sum_{j = 1}^p \\frac{d_j^2}{d_j^2 + n\\lambda} \\end{align}\\] Note that this is smaller than \\(p\\) as long as \\(\\lambda \\neq 0\\). This implies that the Ridge regression does not use the full potential of all \\(p\\) variables, since there is a risk of over-fitting. 7.5 Using the lm.ridge() function We have seen how the lm.ridge() can be used to fit a Ridge regression. However, keep in mind that the lambda parameter used in the function actually specifies the \\(n \\lambda\\) entirely we used in our notation. However, regardless, our goal is mainly to tune this parameter to achieve a good balance of bias-variance trade off. However, the difficulty here is to evaluate the performance without knowing the truth. Let’s first use a simulated example, in which we do know the truth and then introduce the cross-validation approach for real data where we do not know the truth. We use the prostate cancer data prostate from the ElemStatLearn package. The dataset contains 8 explanatory variables and one outcome lpsa, the log prostate-specific antigen value. # ElemStatLearn is currently archived, install a previous version # library(devtools) # install_version(&quot;ElemStatLearn&quot;, version = &quot;2015.6.26&quot;, repos = &quot;http://cran.r-project.org&quot;) library(ElemStatLearn) head(prostate) ## lcavol lweight age lbph svi lcp gleason pgg45 lpsa train ## 1 -0.5798185 2.769459 50 -1.386294 0 -1.386294 6 0 -0.4307829 TRUE ## 2 -0.9942523 3.319626 58 -1.386294 0 -1.386294 6 0 -0.1625189 TRUE ## 3 -0.5108256 2.691243 74 -1.386294 0 -1.386294 7 20 -0.1625189 TRUE ## 4 -1.2039728 3.282789 58 -1.386294 0 -1.386294 6 0 -0.1625189 TRUE ## 5 0.7514161 3.432373 62 -1.386294 0 -1.386294 6 0 0.3715636 TRUE ## 6 -1.0498221 3.228826 50 -1.386294 0 -1.386294 6 0 0.7654678 TRUE 7.5.1 Scaling Issue We can use lm.ridge() with a fixed \\(\\lambda\\) value, as we have shown in the previous example. Its syntax is again similar to the lm() function, with an additional argument lambda. We can also compare that with our own code. # lm.ridge function from the MASS package lm.ridge(lpsa ~., data = prostate[, 1:9], lambda = 1) ## lcavol lweight age lbph svi lcp gleason ## 0.14716982 0.55209405 0.61998311 -0.02049376 0.09488234 0.74846397 -0.09399009 0.05227074 ## pgg45 ## 0.00424397 # using our own code X = cbind(1, data.matrix(prostate[, 1:8])) y = prostate[, 9] solve(t(X) %*% X + diag(9)) %*% t(X) %*% y ## [,1] ## 0.07941225 ## lcavol 0.55985143 ## lweight 0.60398302 ## age -0.01957258 ## lbph 0.09395770 ## svi 0.68809341 ## lcp -0.08863685 ## gleason 0.06288206 ## pgg45 0.00416878 However, they look different. This is because ridge regression has a scaling issue: it would shrink parameters differently if the corresponding covariates have different scales. This can be seen from our previous development of the SVD analysis. Since the shrinkage is the same for all \\(d_j\\)s, it would apply a larger shrinkage for small \\(d_j\\). A commonly used approach to deal with the scaling issue is to standardize all covariates such that they are treated the same way. In addition, we will also center both \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) before performing the ridge regression. An interesting consequence of centering is that we do not need the intercept anymore, since \\(\\mathbf{X}\\boldsymbol{\\beta}= \\mathbf{0}\\) for all \\(\\boldsymbol{\\beta}\\). One last point is that when performing scaling, lm.ridge() use the \\(n\\) factor instead of \\(n-1\\) when calculating the standard deviation. Hence, incorporating all these, we have # perform centering and scaling X = scale(data.matrix(prostate[, 1:8]), center = TRUE, scale = TRUE) # use n instead of (n-1) for standardization n = nrow(X) X = X * sqrt(n / (n-1)) # center y but not scaling y = scale(prostate[, 9], center = TRUE, scale = FALSE) # getting the estimated parameter mybeta = solve(t(X) %*% X + diag(8)) %*% t(X) %*% y ridge.fit = lm.ridge(lpsa ~., data = prostate[, 1:9], lambda = 1) # note that $coef obtains the coefficients internally from lm.ridge # however coef() would transform these back to the original scale version cbind(mybeta, ridge.fit$coef) ## [,1] [,2] ## lcavol 0.64734891 0.64734891 ## lweight 0.26423507 0.26423507 ## age -0.15178989 -0.15178989 ## lbph 0.13694453 0.13694453 ## svi 0.30825889 0.30825889 ## lcp -0.13074243 -0.13074243 ## gleason 0.03755141 0.03755141 ## pgg45 0.11907848 0.11907848 7.5.2 Multiple \\(\\lambda\\) values Since we now face the problem of bias-variance trade-off, we can fit the model with multiple \\(\\lambda\\) values and select the best. This can be done using the following code. library(MASS) fit = lm.ridge(lpsa~., data = prostate[, -10], lambda = seq(0, 100, by=0.2)) For each \\(\\lambda\\), the coefficients of all variables are recorded. The plot shows how these coefficients change as a function of \\(\\lambda\\). We can easily see that as \\(\\lambda\\) becomes larger, the coefficients are shrunken towards 0. This is consistent with our understanding of the bias. On the very left hand size of the plot, the value of each parameter corresponds to the OLS result since no penalty is applied. Be careful that the coefficients of the fitted objects fit$coef are scaled by the standard deviation of the covariates. If you need the original scale, make sure to use coef(fit). matplot(coef(fit)[, -1], type = &quot;l&quot;, xlab = &quot;Lambda&quot;, ylab = &quot;Coefficients&quot;) text(rep(50, 8), coef(fit)[1,-1], colnames(prostate)[1:8]) title(&quot;Prostate Cancer Data: Ridge Coefficients&quot;) To select the best \\(\\lambda\\) value, there can be several different methods. We will discuss two approaches among them: \\(k\\)-fold cross-validation and generalized cross-validation (GCV). 7.6 Cross-validation Cross-validation (CV) is a technique to evaluate the performance of a model on an independent set of data. The essential idea is to separate out a subset of the data and do not use that part during the training, while using it for testing. We can then rotate to or sample a different subset as the testing data. Different cross-validation methods differs on the mechanisms of generating such testing data. \\(K\\)-fold cross-validation is probably the the most popular among them. The method works in the following steps: Randomly split the data into \\(K\\) equal portions For each \\(k\\) in \\(1, \\ldots, K\\): use the \\(k\\)th portion as the testing data and the rest as training data, obtain the testing error Average all \\(K\\) testing errors Here is a graphical demonstration of a \\(10\\)-fold CV: There are also many other cross-validation procedures, for example, the Monte Carlo cross-validation randomly splits the data into training and testing (instead of fix \\(K\\) portions) each time and repeat the process as many times as we like. The benefit of such procedure is that if this is repeated enough times, the estimated testing error becomes fairly stable, and not affected much by the random mechanism. On the other hand, we can also repeat the entire \\(K\\)-fold CV process many times, then average the errors. This is also trying to reduced the influence of randomness. 7.7 Leave-one-out cross-validation Regarding the randomness, the leave-one-out cross-validation is completely nonrandom. It is essentially the \\(k\\)-fold CV approach, but with \\(k\\) equal to \\(n\\), the sample size. A standard approach would require to re-fit the model \\(n\\) times, however, some linear algebra can show that there is an equivalent form using the “Hat” matrix when fitting a linear regression: \\[\\begin{align} \\text{CV}(n) =&amp; \\frac{1}{n}\\sum_{i=1}^n (y_i - \\widehat{y}_{[i]})^2\\\\ =&amp; \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\widehat{y}_i}{1 - \\mathbf{H}_{ii}} \\right)^2, \\end{align}\\] where \\(\\widehat{y}_{i}\\) is the fitted value using the whole dataset, but \\(\\widehat{y}_{[i]}\\) is the prediction of \\(i\\)th observation using the data without it when fitting the model. And \\(\\mathbf{H}_{ii}\\) is the \\(i\\)th diagonal element of the hat matrix \\(\\mathbf{H}= \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}\\). The proof is essentially an application of the Sherman–Morrison–Woodbury formula (SMW, see 4.3.1), which is also used when deriving the rank-one update of a quasi-Newton optimization approach. Proof. Denote \\(\\mathbf{X}_{[i]}\\) and \\(\\mathbf{y}_{[i]}\\) the data derived from \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), but with the \\(i\\) observation (\\(x_i\\), \\(y_i\\)) removed. We then have the properties that \\[\\mathbf{X}_{[i]}^\\text{T}\\mathbf{X}_{[i]} = \\mathbf{X}^\\text{T}\\mathbf{X}- x_i x_i^\\text{T}, \\] and \\[\\mathbf{H}_{ii} = x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i.\\] By the SMW formula, we have \\[(\\mathbf{X}_{[i]}^\\text{T}\\mathbf{X}_{[i]})^{-1} = (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} + \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}x_i x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}}{ 1 - \\mathbf{H}_{ii}}, \\] Further notice that \\[\\mathbf{X}_{[i]}^\\text{T}\\mathbf{y}_{[i]} = \\mathbf{X}^\\text{T}\\mathbf{y}- x_i y_i, \\] we can then reconstruct the fitted parameter when observation \\(i\\) is removed: \\[\\begin{align} \\widehat{\\boldsymbol{\\beta}}_{[i]} =&amp; (\\mathbf{X}_{[i]}^\\text{T}\\mathbf{X}_{[i]})^{-1} \\mathbf{X}_{[i]}^\\text{T}\\mathbf{y}_{[i]} \\\\ =&amp; \\left[ (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} + \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}x_i x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}}{ 1 - \\mathbf{H}_{ii}} \\right] (\\mathbf{X}^\\text{T}\\mathbf{y}- x_i y_i)\\\\ =&amp; (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}+ \\left[ - (\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i y_i + \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}x_i x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1}}{ 1 - \\mathbf{H}_{ii}} (\\mathbf{X}^\\text{T}\\mathbf{y}- x_i y_i) \\right] \\\\ =&amp; \\widehat{\\boldsymbol{\\beta}} - \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i}{1 - \\mathbf{H}_{ii}} \\left[ y_i (1 - \\mathbf{H}_{ii}) - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} + \\mathbf{H}_{ii} y_i \\right]\\\\ =&amp; \\widehat{\\boldsymbol{\\beta}} - \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i}{1 - \\mathbf{H}_{ii}} \\left( y_i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} \\right) \\end{align}\\] Then the error of the \\(i\\)th obervation from the leave-one-out model is \\[\\begin{align} y _i - \\widehat{y}_{[i]} =&amp; y _i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}}_{[i]} \\\\ =&amp; y _i - x_i^\\text{T}\\left[ \\widehat{\\boldsymbol{\\beta}} - \\frac{(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i}{1 - \\mathbf{H}_{ii}} \\left( y_i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} \\right) \\right]\\\\ =&amp; y _i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} + \\frac{x_i^\\text{T}(\\mathbf{X}^\\text{T}\\mathbf{X})^{-1} x_i}{1 - \\mathbf{H}_{ii}} \\left( y_i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} \\right)\\\\ =&amp; y _i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} + \\frac{\\mathbf{H}_{ii}}{1 - \\mathbf{H}_{ii}} \\left( y_i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} \\right)\\\\ =&amp; \\frac{y_i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}}}{1 - \\mathbf{H}_{ii}} \\end{align}\\] This completes the proof. 7.7.1 Generalized cross-validation The generalized cross-validation (GCV, Golub, Heath, and Wahba (1979)) is a modified version of the leave-one-out CV: \\[\\text{GCV}(\\lambda) = \\frac{\\sum_{i=1}^n (y_i - x_i^\\text{T}\\widehat{\\boldsymbol{\\beta}}^\\text{ridge}_\\lambda)}{(n - \\text{Trace}(\\mathbf{S}_\\lambda))}\\] where \\(\\mathbf{S}_\\lambda\\) is the hat matrix corresponding to the ridge regression: \\[\\mathbf{S}_\\lambda = \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X}+ \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\] The following plot shows how GCV value changes as a function of \\(\\lambda\\). # use GCV to select the best lambda plot(fit$lambda[1:500], fit$GCV[1:500], type = &quot;l&quot;, col = &quot;darkorange&quot;, ylab = &quot;GCV&quot;, xlab = &quot;Lambda&quot;, lwd = 3) title(&quot;Prostate Cancer Data: GCV&quot;) We can select the best \\(\\lambda\\) that produces the smallest GCV. fit$lambda[which.min(fit$GCV)] ## [1] 6.8 round(coef(fit)[which.min(fit$GCV), ], 4) ## lcavol lweight age lbph svi lcp gleason pgg45 ## 0.0170 0.4949 0.6050 -0.0169 0.0863 0.6885 -0.0420 0.0634 0.0034 7.8 The glmnet package The glmnet package implements the \\(k\\)-fold cross-validation. To perform a ridge regression with cross-validation, we need to use the cv.glmnet() function with \\(alpha = 0\\). Here, the \\(\\alpha\\) is a parameter that controls the \\(\\ell_2\\) and \\(\\ell_1\\) (Lasso) penalties. In addition, the lambda values are also automatically selected, on the log-scale. library(glmnet) ## Loading required package: Matrix ## Loaded glmnet 4.1-10 set.seed(3) fit2 = cv.glmnet(data.matrix(prostate[, 1:8]), prostate$lpsa, nfolds = 10, alpha = 0) plot(fit2$glmnet.fit, &quot;lambda&quot;) It is useful to plot the cross-validation error against the \\(\\lambda\\) values , then select the corresponding \\(\\lambda\\) with the smallest error. The corresponding coefficient values can be obtained using the s = \"lambda.min\" option in the coef() function. However, this can still be subject to over-fitting, and sometimes practitioners use s = \"lambda.1se\" to select a slightly heavier penalized version based on the variations observed from different folds. plot(fit2) coef(fit2, s = &quot;lambda.min&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## lambda.min ## (Intercept) 0.011566731 ## lcavol 0.492211875 ## lweight 0.604155671 ## age -0.016727236 ## lbph 0.085820464 ## svi 0.685477646 ## lcp -0.039717080 ## gleason 0.063806235 ## pgg45 0.003411982 coef(fit2, s = &quot;lambda.1se&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## lambda.1se ## (Intercept) 0.035381749 ## lcavol 0.264613825 ## lweight 0.421408730 ## age -0.002555681 ## lbph 0.049916919 ## svi 0.452500472 ## lcp 0.075346975 ## gleason 0.083894617 ## pgg45 0.002615235 7.8.1 Scaling Issue The glmnet package would using the same strategies for scaling: center and standardize \\(\\mathbf{X}\\) and center \\(\\mathbf{y}\\). A slight difference is that it considers using \\(1/(2n)\\) as the normalizing factor of the residual sum of squares, but also uses \\(\\lambda/2 \\lVert \\boldsymbol{\\beta}\\rVert_2^2\\) as the penalty. This does not affect our formulation since the \\(1/2\\) cancels out. However, it would slightly affect the Lasso formulation introduced in the next Chapter since the \\(\\ell_1\\) penalty does not apply this \\(1/2\\) factor. Nonetheless, we can check the (nearly) equivalence between lm.ridge and glmnet(): n = 100 p = 5 X &lt;- as.matrix(scale(matrix(rnorm(n*p), n, p))) y &lt;- as.matrix(scale(X[, 1] + X[,2]*0.5 + rnorm(n, sd = 0.5))) lam = 10^seq(-1, 3, 0.1) fit1 &lt;- lm.ridge(y ~ X, lambda = lam) fit2 &lt;- glmnet(X, y, alpha = 0, lambda = lam / nrow(X)) # the estimated parameters par(mfrow=c(1, 2)) matplot(apply(coef(fit1), 2, rev), type = &quot;l&quot;, main = &quot;lm.ridge&quot;) matplot(t(as.matrix(coef(fit2))), type = &quot;l&quot;, main = &quot;glmnet&quot;) # Check differences max(abs(apply(coef(fit1), 2, rev) - t(as.matrix(coef(fit2))))) ## [1] 0.0009968625 Reference Golub, Gene H, Michael Heath, and Grace Wahba. 1979. “Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter.” Technometrics 21 (2): 215–23. Hoerl, Arthur E, and Robert W Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67. "],["lasso.html", "Chapter 8 Lasso 8.1 One-Variable Lasso and Shrinkage 8.2 Constrained Optimization View 8.3 The Solution Path 8.4 Path-wise Coordinate Descent 8.5 Using the glmnet package 8.6 Elastic-Net", " Chapter 8 Lasso Lasso (Tibshirani 1996) is among the most popular machine learning models. Different from the Ridge regression, its adds \\(\\ell_1\\) penalty on the fitted parameters: \\[\\begin{align} \\widehat{\\boldsymbol{\\beta}}^\\text{lasso} =&amp; \\arg\\min_{\\boldsymbol{\\beta}} (\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta})^\\text{T}(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) + n \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1\\\\ =&amp; \\arg\\min_{\\boldsymbol{\\beta}} \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^\\text{T}\\boldsymbol{\\beta})^2 + \\lambda \\sum_{i=1}^p |\\beta_j|, \\end{align}\\] The main advantage of adding such a penalty is that small \\(\\widehat{\\beta}_j\\) values can be shrunk to zero. This may prevents over-fitting and also improve the interpretability especially when the number of variables is large. We will analyze the Lasso starting with a single variable case, and then discuss the application of coordinate descent algorithm to obtain the solution. 8.1 One-Variable Lasso and Shrinkage To illustrate how Lasso shrink a parameter estimate to zero, let’s consider an orthogonal design matrix case, i.e., \\(\\mathbf{X}^\\text{T}\\mathbf{X}= n \\mathbf{I}\\), which will eventually reduce to a one-variable problem. Note that the intercept term is not essential because we can always pre-center the observed data \\(x_i\\) and \\(y_i\\)s so that they can be recovered after this one variable problem. Our objective function is \\[\\frac{1}{n}\\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}\\rVert^2 + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1\\] We are going to relate the solution the OLS solution, which exists in this case because \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is invertible. Hence, we have \\[\\begin{align} &amp;\\frac{1}{n}\\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}\\rVert^2 + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1\\\\ =&amp;\\frac{1}{n}\\lVert \\mathbf{y}- \\color{OrangeRed}{\\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols} + \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols}} - \\mathbf{X}\\boldsymbol{\\beta}\\rVert^2 + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1\\\\ =&amp;\\frac{1}{n}\\lVert \\mathbf{y}- \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols} \\rVert^2 + \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\mathbf{X}\\boldsymbol{\\beta}\\rVert^2 + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1 \\end{align}\\] The cross-term is zero because the OLS residual term is orthogonal to the columns of \\(\\mathbf{X}\\): \\[\\begin{align} &amp;2(\\mathbf{y}- \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols})^\\text{T}(\\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\mathbf{X}\\boldsymbol{\\beta})\\\\ =&amp; 2\\mathbf{r}^\\text{T}\\mathbf{X}(\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\boldsymbol{\\beta})\\\\ =&amp; 0 \\end{align}\\] Then we just need to optimize the part that involves \\(\\boldsymbol{\\beta}\\): \\[\\begin{align} &amp;\\underset{\\boldsymbol{\\beta}}{\\arg\\min} \\frac{1}{n}\\lVert \\mathbf{y}- \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols} \\rVert^2 + \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\mathbf{X}\\boldsymbol{\\beta}\\rVert^2 + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1\\\\ =&amp;\\underset{\\boldsymbol{\\beta}}{\\arg\\min} \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\mathbf{X}\\boldsymbol{\\beta}\\rVert^2 + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1\\\\ =&amp;\\underset{\\boldsymbol{\\beta}}{\\arg\\min} \\frac{1}{n} (\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\boldsymbol{\\beta})^\\text{T}\\mathbf{X}^\\text{T}\\mathbf{X}(\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1\\\\ =&amp;\\underset{\\boldsymbol{\\beta}}{\\arg\\min} \\frac{1}{n} (\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\boldsymbol{\\beta})^\\text{T}n \\mathbf{I}(\\widehat{\\boldsymbol{\\beta}}^\\text{ols} - \\boldsymbol{\\beta}) + \\lambda \\lVert\\boldsymbol{\\beta}\\rVert_1\\\\ =&amp;\\underset{\\boldsymbol{\\beta}}{\\arg\\min} \\sum_{j = 1}^p (\\widehat{\\boldsymbol{\\beta}}^\\text{ols}_j - \\boldsymbol{\\beta}_j )^2 + \\lambda \\sum_j |\\boldsymbol{\\beta}_j|\\\\ \\end{align}\\] This is a separable problem meaning that we can solve each \\(\\beta_j\\) independently since they do not interfere each other. Then the univariate problem is \\[\\underset{\\beta}{\\arg\\min} \\,\\, (\\beta - a)^2 + \\lambda |\\beta|\\] We learned that to solve for an optimizer, we can set the gradient to be zero. However, the function is not everywhere differentiable. Still, we can separate this into two cases: \\(\\beta &gt; 0\\) and \\(\\beta &lt; 0\\). For the positive side, we have \\[\\begin{align} 0 =&amp; \\frac{\\partial}{\\partial \\beta} \\,\\, (\\beta - a)^2 + \\lambda |\\beta| = 2 (\\beta - a) + \\lambda \\\\ \\Longrightarrow \\quad \\beta =&amp;\\, a - \\lambda/2 \\end{align}\\] However, this will maintain positive only when \\(\\beta\\) is greater than \\(a - \\lambda/2\\). The negative size is similar. And whenever \\(\\beta\\) falls in between, it will be shrunk to zero. Overall, for our previous univariate optimization problem, the solution is \\[\\begin{align} \\hat\\beta_j^\\text{lasso} &amp;= \\begin{cases} \\hat\\beta_j^\\text{ols} - \\lambda/2 &amp; \\text{if} \\quad \\hat\\beta_j^\\text{ols} &gt; \\lambda/2 \\\\ 0 &amp; \\text{if} \\quad |\\hat\\beta_j^\\text{ols}| &lt; \\lambda/2 \\\\ \\hat\\beta_j^\\text{ols} + \\lambda/2 &amp; \\text{if} \\quad \\hat\\beta_j^\\text{ols} &lt; -\\lambda/2 \\\\ \\end{cases}\\\\ &amp;= \\text{sign}(\\hat\\beta_j^\\text{ols}) \\left(|\\hat\\beta_j^\\text{ols}| - \\lambda/2 \\right)_+ \\\\ &amp;\\doteq \\text{SoftTH}(\\hat\\beta_j^\\text{ols}, \\lambda) \\end{align}\\] This is called a soft-thresholding function. This implies that when \\(\\lambda\\) is large enough, the estimated \\(\\beta\\) parameter of Lasso will be shrunk towards zero. The following animated figure demonstrates how adding an \\(\\ell_1\\) penalty can change the optimizer. The objective function is \\(0.5 + (\\beta - 1)^2\\) and based on our previous analysis, once the penalty is larger than 2, the optimizer would stay at 0. 8.2 Constrained Optimization View Of course in a multivariate case, this is much more complicated since one variable may affect the optimizer of another. A commonly used alternative interpretation of the Lasso problem is the constrained optimization formulation: \\[\\begin{align} \\min_{\\boldsymbol{\\beta}} \\,\\,&amp; \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}\\rVert^2\\\\ \\text{subject to} \\,\\, &amp; \\lVert\\boldsymbol{\\beta}\\rVert_1 \\leq t \\end{align}\\] We can see from the left penal of the following figure that, the Lasso penalty imposes a constraint with the rhombus, i.e., the solution has to stay within the shaded area. The objective function is shown with the contour, and once the contained area is sufficiently small, some \\(\\beta\\) parameter will be shrunk to exactly zero. On the other hand, the Ridge regression also has a similar interpretation. However, since the constrained areas is a circle, it will never for the estimated parameters to be zero. Figure from online sources. The equivalence of these two forms in a Ridge regression setting can be seen from the Lagrangian and the Karush-Kuhn-Tucker (KKT, (Boyd and Vandenberghe 2004)) conditions. The constrained form can be written as: \\[ \\begin{aligned} \\min_{\\boldsymbol{\\beta}} \\quad &amp; \\frac{1}{2} \\|\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 \\\\ \\text{s.t.} \\quad &amp; \\|\\boldsymbol{\\beta}\\|_2^2 \\leq t \\end{aligned} \\] The Lagrangian for the constrained form is: \\[ \\mathcal{L}(\\boldsymbol{\\beta}, \\alpha) = \\frac{1}{2} \\|\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\alpha \\left( \\|\\boldsymbol{\\beta}\\|_2^2 - t \\right) \\] The KKT conditions are as follows: Stationarity Conditions: \\[ \\nabla_\\boldsymbol{\\beta}\\mathcal{L} = -\\mathbf{X}^T(\\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}) + \\alpha \\boldsymbol{\\beta}= 0 \\] \\[ \\Rightarrow \\boldsymbol{\\beta}^* = (\\mathbf{X}^T \\mathbf{X}+ \\alpha I)^{-1} \\mathbf{X}^T \\mathbf{y} \\] Primal Feasibility: \\[ \\|\\boldsymbol{\\beta}^*\\|_2^2 \\leq t \\] Dual Feasibility: \\[ \\alpha \\geq 0 \\] Complementary Slackness: \\[ \\alpha (\\|\\boldsymbol{\\beta}^*\\|_2^2 - t) = 0 \\] Noticing that from the penalized form, we know that the solution of a Ridge regression is \\(\\beta = (\\mathbf{X}^T \\mathbf{X}+ \\lambda \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{y}\\). The penalized and constrained forms are equivalent in that for each value of \\(\\lambda\\) in the penalized form, there exists a corresponding value of \\(t\\) in the constrained form, such that both yield the same optimal \\(\\boldsymbol{\\beta}\\). The KKT conditions play a crucial role in establishing the equivalence. The Primal Feasibility condition ensures that the optimized \\(\\boldsymbol{\\beta}^*\\) adheres to the constraint \\(\\|\\boldsymbol{\\beta}^*\\|_2^2 \\leq t\\), affirming the legitimacy of the solution within the defined problem space. On the other hand, the Dual Feasibility condition \\(\\alpha \\geq 0\\) guarantees that the Lagrange multiplier is non-negative, which is a requisite for the constrained optimization problem to have a dual. Finally, the Complementary Slackness condition \\(\\alpha (\\|\\boldsymbol{\\beta}^*\\|_2^2 - t) = 0\\) essentially ties the primal and dual problems together. It indicates that if the constraint \\(\\|\\boldsymbol{\\beta}^*\\|_2^2 \\leq t\\) is strictly met (not at the boundary), then \\(\\alpha\\) must be zero; conversely, if \\(\\alpha &gt; 0\\), then the constraint must be “binding,” meaning \\(\\|\\boldsymbol{\\beta}^*\\|_2^2 = t\\). These conditions collectively affirm the equivalence of the penalized and constrained forms by establishing a one-to-one correspondence between their solutions for given values of \\(\\lambda\\) and \\(t\\). 8.3 The Solution Path We are interested in getting the fitted model with a given \\(\\lambda\\) value, however, for selecting the tuning parameter, it would be much more stable to obtain the solution on a sequence of \\(\\lambda\\) values. The corresponding \\(\\boldsymbol{\\beta}\\) parameter estimates are called the solution path, i.e., the path how parameter changes as \\(\\lambda\\) changes. We have seen an example of this with the Ridge regression. For Lasso, the the solution path has an interpretation as the forward-stagewise regression. This is different than the forward stepwise model we introduced before. A forward stagewise regression works in the following way: Start with the Null model (intercept) and choose the best variable out of all \\(p\\), such that when its parameter grows by a small magnitude \\(\\epsilon\\) (either positive or negative), the RSS reduces the most. Grow the parameter estimate of this variable by \\(\\epsilon\\) and repeat. The stage-wise regression solution has been shown to give the same solution path as the Lasso, if we start with a sufficiently large \\(\\lambda\\), and gradually reduces it towards zero. This can be done with the least angle regression (lars) package. Note that the lars package introduces another computationally more efficient approach to obtain the same solution, but we will not discuss it in details. We comparing the two approaches (stagewise and stepwise) using the prostate data from the ElemStatLearn package. library(lars) library(ElemStatLearn) data(prostate) lars.fit = lars(x = data.matrix(prostate[, 1:8]), y = prostate$lpsa, type = &quot;forward.stagewise&quot;) plot(lars.fit) lars.fit = lars(x = data.matrix(prostate[, 1:8]), y = prostate$lpsa, type = &quot;stepwise&quot;) plot(lars.fit) At each vertical line, a new variable enters the model by growing its parameter out of zero. You can relate this to our previous animated graph where as \\(\\lambda\\) decreases, the parameter estimate eventually comes out of zero. However, they may change their grow rate as a new variable comes. This is due to the covariance structure. 8.4 Path-wise Coordinate Descent The coordinate descent algorithm (J. Friedman, Hastie, and Tibshirani 2010) is probably the most efficient way to solve the Lasso solution up to now. The idea shares similarities with the stage-wise regression. However, with some careful analysis, we can obtain coordinate updates exactly, instead of moving a small step size. And this is done on a decreasing grid of \\(\\lambda\\) values. A pseudo algorithm proceed in the following way: Start with a \\(\\lambda\\) value sufficiently large such that all parameter estimates are zero. Reduce \\(\\lambda\\) by a fraction, e.g., 0.05, and perform coordinate descent updates: For \\(j = 1, \\ldots p\\), update \\(\\beta_j\\) using a one-variable penalized formulation. Repeat i) until convergence. Record the corresponding \\(\\widehat{\\boldsymbol{\\beta}}^\\text{lasso}_\\lambda\\). Repeat steps 2) and 3) until \\(\\lambda\\) is sufficiently small or there are already \\(n\\) nonzero parameters entered into the model. Output \\(\\widehat{\\boldsymbol{\\beta}}^\\text{lasso}_\\lambda\\) for all \\(\\lambda\\) values. The crucial step is then figuring out the explicit formula of the coordinate update. Recall that in a coordinate descent algorithm of OLS at Section 5.8, we update \\(\\beta_j\\) using \\[ \\underset{\\boldsymbol \\beta_j}{\\text{argmin}} \\,\\, \\frac{1}{n} ||\\mathbf{y}- X_j \\beta_j - \\mathbf{X}_{(-j)} \\boldsymbol{\\beta}_{(-j)} ||^2 \\] Since this is a one-variable OLS problem, the solution is \\[ \\beta_j = \\frac{X_j^T \\mathbf{r}}{X_j^T X_j} \\] with \\(\\mathbf{r}= \\mathbf{y}- \\mathbf{X}_{(-j)} \\boldsymbol{\\beta}_{(-j)}\\). Now, adding the penalty \\(|\\beta_j|\\), we essentially reduces back to the previous example of the single variable lasso problem, where we have the OLS solution. Hence, all we need to do is to apply the soft-thresholding function. The the Lasso coordinate update becomes \\[\\beta_j^\\text{new} = \\text{SoftTH}\\left(\\frac{X_j^T \\mathbf{r}}{X_j^T X_j}, \\lambda\\right) \\] Incorporate this into the previous algorithm, we can obtain the entire solution path of a Lasso problem. This algorithm is implemented in the glmnet package. We will show an example of it. 8.5 Using the glmnet package We still use the prostate cancer data prostate data. The dataset contains 8 explanatory variables and one outcome lpsa, the log prostate-specific antigen value. We fit the model using the glmnet package. The tuning parameter can be selected using cross-validation with the cv.glmnet function. You can specify nfolds for the number of folds in the cross-validation. The default is 10. For Lasso, we should use alpha = 1, while alpha = 0 is for Ridge. However, it is the default value that you do not need to specify. library(glmnet) set.seed(3) fit2 = cv.glmnet(data.matrix(prostate[, 1:8]), prostate$lpsa, nfolds = 10, alpha = 1) The left plot demonstrates how \\(\\lambda\\) changes the cross-validation error. There are two vertical lines, which represents lambda.min and lambda.1se respectively. The right plot shows how \\(\\lambda\\) changes the parameter values, with each line representing a variable. The x-axis in the figure is in terms of \\(\\log(\\lambda)\\), hence their is a larger penalty to the right. Note that the glmnet package uses \\(1/(2n)\\) in the loss function instead of \\(1/n\\), hence the corresponding soft-thresholding function would reduce the magnitude of \\(\\lambda\\) by \\(\\lambda\\) instead of half of it. Moreover, the package will perform scaling before the model fitting, which essentially changes the corresponding one-variable OLS solution. The solution on the original scale will be retrieved once the entire solution path is finished. However, we usually do not need to worry about these computationally issues in practice. The main advantage of Lasso is shown here that the model can be sparse, with some parameter estimates shrunk to exactly 0. par(mfrow = c(1, 2)) plot(fit2) plot(fit2$glmnet.fit, &quot;lambda&quot;) We can obtain the estimated coefficients from the best \\(\\lambda\\) value. Similar to the ridge regression example, there are two popular options, lambda.min and lambda.1se. The first one is the value that minimizes the cross-validation error, the second one is slightly more conservative, which gives larger penalty value with more shrinkage. You can notice that lambda.min contains more nonzero parameters. coef(fit2, s = &quot;lambda.min&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## lambda.min ## (Intercept) 0.1537694867 ## lcavol 0.5071477800 ## lweight 0.5455934491 ## age -0.0084065349 ## lbph 0.0618168146 ## svi 0.5899942923 ## lcp . ## gleason 0.0009732887 ## pgg45 0.0023140828 coef(fit2, s = &quot;lambda.1se&quot;) ## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## lambda.1se ## (Intercept) 0.6435469 ## lcavol 0.4553889 ## lweight 0.3142829 ## age . ## lbph . ## svi 0.3674270 ## lcp . ## gleason . ## pgg45 . Prediction can be done using the predict() function. pred = predict(fit2, data.matrix(prostate[, 1:8]), s = &quot;lambda.min&quot;) # training error mean((pred - prostate$lpsa)^2) ## [1] 0.4594359 8.6 Elastic-Net Lasso may suffer in the case where two variables are strongly correlated. The situation is similar to OLS, however, in Lasso, it would only select one out of the two, instead of letting both parameter estimates to be large. This is not preferred in some practical situations such as genetic studies because expressions of genes from the same pathway may have large correlation, but biologist want to identify all of them instead of just one. The Ridge penalty may help in this case because it naturally considers the correlation structure. Hence the Elastic-Net (Zou and Hastie 2005) penalty has been proposed to address this issue: the data contains many correlated variables and we want to select them together if they are important for prediction. The glmnet package uses the following definition of an Elastic-Net penalty, which is a mixture of \\(\\ell_1\\) and \\(\\ell_2\\) penalties: \\[\\lambda \\left[ (1 - \\alpha)/2 \\lVert \\boldsymbol{\\beta}\\rVert_2^2 + \\alpha |\\boldsymbol{\\beta}|_1 \\right],\\] which involves two tuning parameters. However, in practice, it is very common to simply use \\(\\alpha = 0.5\\). Reference Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge university press. Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33 (1): 1. Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20. "],["spline.html", "Chapter 9 Spline 9.1 Using Linear models for Nonlinear Trends 9.2 A Motivating Example and Polynomials 9.3 Piecewise Polynomials 9.4 Splines 9.5 Spline Basis 9.6 Natural Cubic Spline 9.7 Smoothing Spline 9.8 Fitting Smoothing Splines 9.9 Extending Splines to Multiple Varibles", " Chapter 9 Spline 9.1 Using Linear models for Nonlinear Trends In previous chapters, we mainly focused linear models. Modeling nonlinear trends can still be done with linear model by introducing higher-order terms, or nonlinear transformations. For example, \\(x^2\\), \\(\\log(x)\\) are all very commonly used approaches to model nonlinear effects. There is another class of approaches that is more flexible with nice theoretical properties, the splines. In this chapter, we mainly focus on univariate regression problems. 9.2 A Motivating Example and Polynomials We use the U.S. birth rate data as an example. The data records birth rates from 1917 to 2003. The birth rate trend is obviously very nonlinear. birthrates= read.csv(&quot;data/birthrate2.csv&quot;, row.names = 1) head(birthrates) ## Year Birthrate ## 1 1917 183.1 ## 2 1918 183.9 ## 3 1919 163.1 ## 4 1920 179.5 ## 5 1921 181.4 ## 6 1922 173.4 par(mar = c(4,4,1,1)) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) It might be interesting to fit a linear regression with high order polynomials to approximate this curve. This can be carried out using the poly() function, which calculates all polynomials up to a certain power. Please note that this is a more stable method compared with writing out the powers such as I(Year^2), I(Year^3) etc because the Year variable is very large, and is numerically unstable. par(mfrow=c(1,2)) par(mar = c(2,3,2,0)) lmfit &lt;- lm(Birthrate ~ poly(Year, 3), data = birthrates) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = &quot;deepskyblue&quot;, lwd = 2) title(&quot;degree = 3&quot;) par(mar = c(2,3,2,0)) lmfit &lt;- lm(Birthrate ~ poly(Year, 5), data = birthrates) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = &quot;deepskyblue&quot;, lwd = 2) title(&quot;degree = 5&quot;) These fittings do not seem to perform very well. How about we take a different approach to model the curve locally. Well, we know there is an approach that works in a similar way – \\(k\\)NN. But we will try something new. Let’s first divide the year range into several non-overlapping intervals, say, every 10 years. Then we will estimate the regression coefficients within each interval by averaging the observations, just like \\(k\\)NN. The only difference is that for prediction, we do not recalculate the neighbors anymore, just check the intervals. par(mfrow=c(1,1)) par(mar = c(2,2,2,0)) mybasis = matrix(NA, nrow(birthrates), 8) for (l in 1:8) mybasis[, l] = birthrates$Year*(birthrates$Year &gt;= 1917 + l*10) lmfit &lt;- lm(birthrates$Birthrate ~ ., data = data.frame(mybasis)) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) lines(birthrates$Year, lmfit$fitted.values, lty = 1, type = &quot;s&quot;, col = &quot;deepskyblue&quot;, lwd = 2) title(&quot;Histgram Regression&quot;) The method is called a histogram regression. Suppose the interval that contains a given testing point \\(x\\) is \\(\\phi(x)\\), then, we are fitting a model with \\[ \\widehat{f}(x) = \\frac{\\sum_{i=1}^n Y_i \\,\\, I\\{X_i \\in \\phi(x)\\} }{ \\sum_{i=1}^n I\\{X_i \\in \\phi(x)\\}} \\] You may know the word histogram from the plotting the density of a set of observations. Yes, these two are actually motivated by the same philosophy. We will discuss the connection later on. For the purpose of fitting a regression function, the histogram regression does not seem to perform ideally since there will be jumps at the edge of an interval. Hence we need a more flexible framework. 9.3 Piecewise Polynomials Instead of fitting constant functions within each interval (between two knots), we may consider fitting a line. Consider a simpler case, where we just use 3 knots at 1938, 1960, 1978, which gives 4 intervals. par(mfrow=c(1,2)) myknots = c(1936, 1960, 1978) bounds = c(1917, myknots, 2003) # piecewise constant mybasis = cbind(&quot;x_1&quot; = (birthrates$Year &lt; myknots[1]), &quot;x_2&quot; = (birthrates$Year &gt;= myknots[1])*(birthrates$Year &lt; myknots[2]), &quot;x_3&quot; = (birthrates$Year &gt;= myknots[2])*(birthrates$Year &lt; myknots[3]), &quot;x_4&quot; = (birthrates$Year &gt;= myknots[3])) lmfit &lt;- lm(birthrates$Birthrate ~ . -1, data = data.frame(mybasis)) par(mar = c(2,3,2,0)) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) abline(v = myknots, lty = 2) title(&quot;Piecewise constant&quot;) for (k in 1:4) points(c(bounds[k], bounds[k+1]), rep(lmfit$coefficients[k], 2), type = &quot;l&quot;, lty = 1, col = &quot;deepskyblue&quot;, lwd = 4) # piecewise linear mybasis = cbind(&quot;x_1&quot; = (birthrates$Year &lt; myknots[1]), &quot;x_2&quot; = (birthrates$Year &gt;= myknots[1])*(birthrates$Year &lt; myknots[2]), &quot;x_3&quot; = (birthrates$Year &gt;= myknots[2])*(birthrates$Year &lt; myknots[3]), &quot;x_4&quot; = (birthrates$Year &gt;= myknots[3]), &quot;x_11&quot; = birthrates$Year*(birthrates$Year &lt; myknots[1]), &quot;x_21&quot; = birthrates$Year*(birthrates$Year &gt;= myknots[1])*(birthrates$Year &lt; myknots[2]), &quot;x_31&quot; = birthrates$Year*(birthrates$Year &gt;= myknots[2])*(birthrates$Year &lt; myknots[3]), &quot;x_41&quot; = birthrates$Year*(birthrates$Year &gt;= myknots[3])) lmfit &lt;- lm(birthrates$Birthrate ~ .-1, data = data.frame(mybasis)) par(mar = c(2,3,2,0)) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) abline(v = myknots, lty = 2) title(&quot;Piecewise linear&quot;) for (k in 1:4) points(c(bounds[k], bounds[k+1]), lmfit$coefficients[k] + c(bounds[k], bounds[k+1])*lmfit$coefficients[k+4], type = &quot;l&quot;, lty = 1, col = &quot;deepskyblue&quot;, lwd = 4) 9.4 Splines However, these functions are not continuous. Hence we use a trick to construct continuous basis: par(mfrow=c(1,1)) pos &lt;- function(x) x*(x&gt;0) mybasis = cbind(&quot;int&quot; = 1, &quot;x_1&quot; = birthrates$Year, &quot;x_2&quot; = pos(birthrates$Year - myknots[1]), &quot;x_3&quot; = pos(birthrates$Year - myknots[2]), &quot;x_4&quot; = pos(birthrates$Year - myknots[3])) par(mar = c(2,2,2,0)) matplot(birthrates$Year, mybasis[, -1], type = &quot;l&quot;, lty = 1, yaxt = &#39;n&#39;, ylim = c(0, 50), lwd = 2) title(&quot;Spline Basis Functions&quot;) With this definition, any fitted model will be Continuous everywhere Linear everywhere except the knots Has a different slot for each region The resulted model is called a spline. lmfit &lt;- lm(birthrates$Birthrate ~ .-1, data = data.frame(mybasis)) par(mar = c(2,3,2,0)) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = &quot;deepskyblue&quot;, lwd = 4) abline(v = myknots, lty = 2) title(&quot;Linear Spline&quot;) Of course, writing this out explicitly is very tedious, hence we have the bs function in the splines package to help us. par(mar = c(2,2,2,0)) lmfit &lt;- lm(Birthrate ~ splines::bs(Year, degree = 1, knots = myknots), data = birthrates) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = &quot;deepskyblue&quot;, lwd = 4) title(&quot;Linear spline with the bs() function&quot;) The next step is to increase the degree to account for more complicated functions. A few things we need to consider here: How many knots should be used Where to place the knots What is the degree of functions in each region For example, we consider this setting par(mar = c(2,2,2,0)) lmfit &lt;- lm(Birthrate ~ splines::bs(Year, degree = 3, knots = myknots), data = birthrates) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = &quot;deepskyblue&quot;, lwd = 4) title(&quot;Cubic spline with 3 knots&quot;) All of them affects the performance. In particular, the number of knots and the number of degrees in each region will determine the total number of degrees of freedom. For simplicity, we can control that using the df parameter. We use a total of 6 parameters, chosen by the function automatically. However, this does not seems to perform better than the knots we implemented. The choice of knots can be crucial. par(mar = c(2,2,2,0)) lmfit &lt;- lm(Birthrate ~ splines::bs(Year, df = 5), data = birthrates) plot(birthrates, pch = 19, col = &quot;darkorange&quot;) lines(birthrates$Year, lmfit$fitted.values, lty = 1, col = &quot;deepskyblue&quot;, lwd = 4) title(&quot;Linear spline with 6 degrees of parameters&quot;) 9.5 Spline Basis There are different ways to construct spline basis. We used two techniques previously, the regression spline and basis spline (B-spline). The B-spline has slight more advantages computationally. Here is a comparision of B-spline with different degrees. par(mfrow = c(4, 1), mar = c(0, 0, 2, 0)) for (d in 0:3) { bs_d = splines2::bSpline(1:100, degree = d, knots = seq(10, 90, 10), intercept = TRUE) matplot(1:100, bs_d, type = ifelse(d == 0, &quot;s&quot;, &quot;l&quot;), lty = 1, ylab = &quot;spline&quot;, xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, ylim = c(-0.05, 1.05), lwd = 2) title(paste(&quot;degree =&quot;, d)) } 9.6 Natural Cubic Spline Extrapolations are generally dangerous because the functions could be extream outside the range of the observed data. In linear models fit by bs(), extrapolations outside the boundaries will trigger a warning. par(mfrow=c(1,1)) library(splines) fit.bs = lm(Birthrate ~ bs(Year, df=6), data=birthrates) plot(birthrates$Year, birthrates$Birthrate, ylim=c(0,280), pch = 19, xlim = c(1900, 2020), xlab = &quot;year&quot;, ylab = &quot;rate&quot;, col = &quot;darkorange&quot;) lines(seq(1900, 2020), predict(fit.bs, data.frame(&quot;Year&quot;= seq(1900, 2020))), col=&quot;deepskyblue&quot;, lty=1, lwd = 3) ## Warning in bs(Year, degree = 3L, knots = c(1938.5, 1960, 1981.5), Boundary.knots = c(1917L, : some ## &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases fit.ns = lm(Birthrate ~ ns(Year, df=6), data=birthrates) lines(seq(1900, 2020), predict(fit.ns, data.frame(&quot;Year&quot;= seq(1900, 2020))), col=&quot;darkgreen&quot;, lty=1, lwd = 3) legend(&quot;topright&quot;, c(&quot;Cubic B-Spline&quot;, &quot;Natural Cubic Spline&quot;), col = c(&quot;deepskyblue&quot;, &quot;darkgreen&quot;), lty = 1, lwd = 3, cex = 1.2) title(&quot;Birth rate extrapolation&quot;) Hence this motivates us to consider setting additional constrains that forces the extrapolations to be come more regular. This is done by forcing the second and third derivatives to be 0 if beyond the two extreme knots. par(mar = c(0, 2, 2, 0)) ncs = ns(1:100, df = 6, intercept = TRUE) matplot(1:100, ncs, type = &quot;l&quot;, lty = 1, ylab = &quot;spline&quot;, xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, lwd = 3) title(&quot;Natural Cubic Spline&quot;) 9.7 Smoothing Spline The motivation is to trying to solve the knots selection issue. Instead, let’s start with a “horrible” idea, by putting knots at all each observed data point \\((x_1, x_2, \\ldots, x_n)\\). Then, we can create \\(n\\) natural cubic spline basis. However, we also know that this leads to over-fitting since there are too many parameters. Let’s utilize the ridge regression idea by adding some penalties. This leads to the objective function \\[\\begin{equation} \\underset{\\boldsymbol{\\beta}}{\\text{min}} \\,\\, \\lVert \\mathbf{y}- \\mathbf{F}\\boldsymbol{\\beta}\\rVert^2 + \\lambda \\boldsymbol{\\beta}^\\text{T}\\Omega \\boldsymbol{\\beta}, \\tag{9.1} \\end{equation}\\] where \\(\\mathbf{F}\\) is the matrix of all \\(n\\) natural cubic spline basis, and \\(\\Omega\\) is some covariance matrix that takes care of some form of relationship among different basis, which we will define later, and \\(\\lambda\\) is similar to the ridge regression. The question is, will this type of regression problem provide a good solution with nice properties? Let’s consider fitting a regression model by solving a regression function \\(g(x)\\) with the following penalized criteria: \\[\\begin{equation} \\frac{1}{n} \\sum_{i=1}^n \\big(y_i - g(x_i)\\big)^2 + \\lambda \\int_a^b \\big[g&#39;&#39;(x) \\big]^2 dx. \\tag{9.2} \\end{equation}\\] This is the sum of \\(\\ell_2\\) loss and a roughness penalty that enforce certain smoothness on \\(g(x)\\). And we shall show that the optimal \\(g(x)\\) that minimize this objective function will take the ridge penalty form in (9.1) mentioned previously. We will consider all absolutely continuous functions on \\([a, b] = [\\min(x_i), \\max(x_i)]\\), with finite roughness, i.e., \\(\\int_a^b \\big[g&#39;&#39;(x) \\big]^2 dx &lt; \\infty\\). This is known as the second order Sobolev space. Let’s first define \\(g(\\cdot)\\) as the optimal solution to Equation (9.2). Since the loss part in (9.2) only involves \\(n\\) data points, we can find define a natural cubic spline (NCS) fit \\(\\tilde{g}(\\cdot)\\) such that it matches with \\(g(\\cdot)\\) on all the observed data, i.e., \\[g(x_i) = \\widetilde{g}(x_i), \\quad i = 1, \\ldots, n.\\] Note that the roughness of NCS fit \\(\\widetilde{g}\\) is also finite, hence \\(\\widetilde{g}\\) is within the space of functions we are considering. Also, such matching on all observed data points is doable when we have \\(n\\) basis in the natural cubic spline. In this case, the loss corresponds to \\(\\tilde{g}\\) and \\(g\\) are identical. Hence, it only matters if the penalty part of \\(\\tilde{g}(\\cdot)\\) is the same. To analyze this, we define the difference between these two functions as \\[h(x) = g(x) - \\tilde{g}(x).\\] It is then obvious that \\(h(x_i) = 0\\) for all observed \\(i\\). Then we have \\[\\int g&#39;&#39;^2 dx = \\int \\widetilde{g}&#39;&#39;^2 dx + \\int h&#39;&#39;^2 dx + 2 \\int \\widetilde{g}&#39;&#39; h&#39;&#39; dx\\] The first and second term on the right hand side are both non-negative. Hence, only the third term matters. WLOG, we assume that \\(x_i\\)’s are ordered from the smallest to the largest. Then \\[\\begin{align} \\int \\tilde{g}&#39;&#39; h&#39;&#39; dx =&amp; ~\\tilde{g}&#39;&#39; h&#39; \\Big|_a^b - \\int_a^b h&#39; \\tilde{g}^{(3)} dx \\nonumber \\\\ =&amp;~ 0 - \\int_a^b h&#39; \\tilde{g}^{(3)} dx \\nonumber \\\\ =&amp;~ - \\sum_{i=1}^{n-1} \\tilde{g}^{(3)}(x_j^+) \\int_{x_j}^{x_{j+1}} h&#39; dx \\quad \\nonumber \\\\ =&amp;~ - \\sum_{i=1}^{n-1} \\tilde{g}^{(3)}(x_j^+) \\big(h(x_{j+1}) - h(x_j)\\big) \\nonumber \\\\ =&amp;~ 0 \\end{align}\\] The second equation is because \\(\\tilde{g}\\) is a NCS and suppose to have 0 second derivative on the two boundaries \\(a\\) and \\(b\\). The third equation is because \\(\\tilde{g}\\) is at most \\(x^3\\) on any regions and have constant third derivatives, which we can pull out of the integration. And the last equation is because \\(h(x) = 0\\) on all \\(x_i\\)’s. Hence, this shows that the roughness penalty \\[\\int \\widetilde{g}&#39;&#39;^2 dx\\] of our NCS solution is no larger than the best solution \\(g\\). Noticing that \\(\\widetilde{g}\\) is also with the space of functions we are considering, then \\(g\\) must be our NCS solution. Hence, \\(g\\) has a finite sample representation \\[\\widehat g(x) = \\sum_{j=1}^n \\beta_j N_j(x)\\] where \\(N_j\\)’s are a set of natural cubic spline basis functions with knots at each of the unique \\(x\\) values. Then Equation (9.2) becomes \\[\\begin{align} &amp; \\lVert \\mathbf{y}- \\sum_{j=1}^n \\beta_j N_j(x) \\rVert^2 + \\int \\Big( \\sum_{j=1}^n \\beta_j N_j&#39;&#39;(x)\\Big)^2 dx \\\\ =&amp; \\lVert \\mathbf{y}- \\mathbf{F}\\boldsymbol{\\beta}\\rVert^2 + \\boldsymbol{\\beta}^\\text{T}\\Omega \\boldsymbol{\\beta}, \\end{align}\\] where \\(\\mathbf{F}\\) is the design matrix corresponds to the \\(n\\) NCS basis \\(N_j\\)’s, and \\(\\Omega\\) is an \\(n \\times n\\) matrix with \\(\\Omega_{ij} = \\int N_i&#39;&#39;(x) N_j(x) dx.\\) The solution is essentially a ridge solution: \\[\\widehat{\\boldsymbol{\\beta}} = (\\mathbf{F}^\\text{T}\\mathbf{F}+ \\lambda \\Omega)^{-1} \\mathbf{F}^\\text{T}\\mathbf{y}.\\] and the penalty \\(\\lambda\\) can be tuned using GCV. 9.8 Fitting Smoothing Splines Fitting a smoothing spline can be done by using the smooth.spline package. However, since the birthrate data has little variation in adjacent years, over-fitting is quite severe. The function will automatically use GCV to tune the parameter. # smoothing spline fit = smooth.spline(birthrates$Year, birthrates$Birthrate) plot(birthrates$Year, birthrates$Birthrate, pch = 19, xlab = &quot;Year&quot;, ylab = &quot;BirthRates&quot;, col = &quot;darkorange&quot;) lines(seq(1917, 2003), predict(fit, seq(1917, 2003))$y, col=&quot;deepskyblue&quot;, lty=1, lwd = 3) # the degrees of freedom is very large fit$df ## [1] 60.7691 Let’s look at another simulation example, where this method performs reasonably well. set.seed(1) n = 100 x = seq(0, 1, length.out = n) y = sin(12*(x+0.2))/(x+0.2) + rnorm(n) # fit smoothing spline fit = smooth.spline(x, y) # the degrees of freedom fit$df ## [1] 9.96443 # fitted model plot(x, y, pch = 19, xlim = c(0, 1), xlab = &quot;x&quot;, ylab = &quot;y&quot;, col = &quot;darkorange&quot;) lines(x, sin(12*(x+0.2))/(x+0.2), col=&quot;red&quot;, lty=1, lwd = 3) lines(x, predict(fit, x)$y, col=&quot;deepskyblue&quot;, lty=1, lwd = 3) legend(&quot;bottomright&quot;, c(&quot;Truth&quot;, &quot;Smoothing Splines&quot;), col = c(&quot;red&quot;, &quot;deepskyblue&quot;), lty = 1, lwd = 3, cex = 1.2) 9.9 Extending Splines to Multiple Varibles Since all spline approaches can be transformed into some kind of linear model, if we postulate an additive structure, we can fit a multivariate model with \\[f(x) = \\sum_j h_j(x_j) = \\sum_j \\sum_k N_{jk}(x_j) \\beta_{jk}\\] where \\(h_j(x_j)\\) is a univariate function for \\(x_j\\) that can be approximated by splines basis \\(N_{jk}(\\cdot), k = 1, \\ldots, K\\). This works for both linear regression and generalized linear regressions. For the South Africa Heart Disease data, we use the gam() function in the gam (generalized additive models) package. We compute a logistic regression model using natural splines (note famhist is included as a factor). library(ElemStatLearn) library(gam) ## Loading required package: foreach ## Loaded gam 1.22-5 form = formula(&quot;chd ~ ns(sbp,df=4) + ns(tobacco,df=4) + ns(ldl,df=4) + famhist + ns(obesity,df=4) + ns(alcohol,df=4) + ns(age,df=4)&quot;) # note that we can also do # m = glm(form, data=SAheart, family=binomial) # print(summary(m), digits=3) # however, the gam function provides more information m = gam(form, data=SAheart, family=binomial) summary(m) ## ## Call: gam(formula = form, family = binomial, data = SAheart) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7245 -0.8265 -0.3884 0.8870 2.9589 ## ## (Dispersion Parameter for binomial family taken to be 1) ## ## Null Deviance: 596.1084 on 461 degrees of freedom ## Residual Deviance: 457.6318 on 436 degrees of freedom ## AIC: 509.6318 ## ## Number of Local Scoring Iterations: 6 ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ns(sbp, df = 4) 4 6.31 1.5783 1.4242 0.224956 ## ns(tobacco, df = 4) 4 18.09 4.5218 4.0802 0.002941 ** ## ns(ldl, df = 4) 4 12.05 3.0137 2.7194 0.029290 * ## famhist 1 19.70 19.7029 17.7788 3.019e-05 *** ## ns(obesity, df = 4) 4 3.66 0.9161 0.8266 0.508701 ## ns(alcohol, df = 4) 4 1.28 0.3200 0.2887 0.885278 ## ns(age, df = 4) 4 17.64 4.4100 3.9794 0.003496 ** ## Residuals 436 483.19 1.1082 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 par(mfrow = c(3, 3), mar = c(5, 5, 2, 0)) plot(m, se = TRUE, residuals = TRUE, pch = 19, col = &quot;darkorange&quot;) "],["logistic-regression.html", "Chapter 10 Logistic Regression 10.1 Modeling Binary Outcomes 10.2 Example: Cleveland Clinic Heart Disease Data 10.3 Interpretation of the Parameters 10.4 Solving a Logistic Regression 10.5 Example: South Africa Heart Data 10.6 Penalized Logistic Regression", " Chapter 10 Logistic Regression 10.1 Modeling Binary Outcomes To model binary outcomes using a logistic regression, we will use the 0/1 coding of \\(Y\\). We need to set its connection with covariates. Recall in a linear regression, the outcome is continuous, and we set \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\] However, this does not work for classification since \\(Y\\) can only be 0 or 1. Hence we turn to consider modeling the probability \\(P(Y = 1 | X = \\mathbf{x})\\). Hence, \\(Y\\) is a Bernoulli random variable given \\(X\\), and this is modeled by a function of \\(X\\): \\[ P(Y = 1 | X = \\mathbf{x}) = \\frac{\\exp(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})}\\] Note that although \\(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta}\\) may ranges from 0 to infinity as \\(X\\) changes, the probability will still be bounded between 0 and 1. This is an example of Generalized Linear Models. The relationship is still represented using a linear function of \\(\\mathbf{x}\\), \\(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta}\\). This is called a logit link function (a function to connect the conditional expectation of \\(Y\\) with \\(\\boldsymbol{\\beta}^\\text{T}\\mathbf{x}\\)): \\[\\eta(a) = \\frac{\\exp(a)}{1 + \\exp(a)}\\] Hence, \\(P(Y = 1 | X = \\mathbf{x}) = \\eta(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})\\). We can reversely solve this and get \\[\\begin{aligned} P(Y = 1 | X = \\mathbf{x}) = \\eta(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta}) &amp;= \\frac{\\exp(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})}\\\\ 1 - \\eta(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta}) &amp;= \\frac{1}{1 + \\exp(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})} \\\\ \\text{Odds} = \\frac{\\eta(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})}{1-\\eta(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})} &amp;= \\exp(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta})\\\\ \\log(\\text{Odds}) = \\mathbf{x}^\\text{T}\\boldsymbol{\\beta} \\end{aligned}\\] Hence, the parameters in a logistic regression is explained as log odds. Let’s look at a concrete example. 10.2 Example: Cleveland Clinic Heart Disease Data We use use the Cleveland clinic heart disease dataset. The goal is to model and predict a class label of whether the patient has a hearth disease or not. This is indicated by whether the num variable is \\(0\\) (no presence) or \\(&gt;0\\) (presence). heart = read.csv(&quot;data/processed_cleveland.csv&quot;) heart$Y = as.factor(heart$num &gt; 0) table(heart$Y) ## ## FALSE TRUE ## 164 139 Let’s model the probability of heart disease using the Age variable. This can be done using the glm() function, which stands for the Generalized Linear Model. The syntax of glm() is almost the same as a linear model. Note that it is important to use family = binomial to specify the logistic regression. logistic.fit &lt;- glm(Y~age, data = heart, family = binomial) summary(logistic.fit) ## ## Call: ## glm(formula = Y ~ age, family = binomial, data = heart) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.00591 0.75913 -3.960 7.5e-05 *** ## age 0.05199 0.01367 3.803 0.000143 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 417.98 on 302 degrees of freedom ## Residual deviance: 402.54 on 301 degrees of freedom ## AIC: 406.54 ## ## Number of Fisher Scoring iterations: 4 The result is similar to a linear regression, with some differences. The parameter estimate of age is 0.05199. It is positive, meaning that increasing age would increase the change of having heart disease. However, this does not mean that 1 year older would increase the change by 0.05. Since, by our previous formula, the probably is not directly expressed as \\(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta}\\). This calculation can be realized when predicting a new target point. Let’s consider a new subject with Age = 55. What is the predicted probability of heart disease? Based on our formula, we have \\[\\beta_0 + \\beta_1 X = -3.00591 + 0.05199 \\times 55 = -0.14646\\] And the estimated probability is \\[ P(Y = 1 | X) = \\frac{\\exp(\\beta_0 + \\beta_1 X)}{1 + \\exp(\\beta_0 + \\beta_1 X)} = \\frac{\\exp(-0.14646)}{1 + \\exp(-0.14646)} = 0.4634503\\] Hence, the estimated probability for this subject is 46.3%. This can be done using R code. Please note that if you want to predict the probability, you need to specify type = \"response\". Otherwise, only \\(\\beta_0 + \\beta_1 X\\) is provided. testdata = data.frame(&quot;age&quot; = 55) predict(logistic.fit, newdata = testdata) ## 1 ## -0.1466722 predict(logistic.fit, newdata = testdata, type = &quot;response&quot;) ## 1 ## 0.4633975 If we need to make a 0/1 decision about this subject, a natural idea is to see if the predicted probability is greater than 0.5. In this case, we would predict this subject as 0. 10.3 Interpretation of the Parameters Recall that \\(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta}\\) is the log odds, we can further interpret the effect of a single variable. Let’s define the following two, with an arbitrary age value \\(a\\): A subject with age \\(= a\\) A subject with age \\(= a + 1\\) Then, if we look at the odds ratio corresponding to these two target points, we have \\[\\begin{aligned} \\text{Odds Ratio} &amp;= \\frac{\\text{Odds in Group 2}}{\\text{Odds in Group 1}}\\\\ &amp;= \\frac{\\exp(\\beta_0 + \\beta_1 (a+1))}{\\exp(\\beta_0 + \\beta_1 a)}\\\\ &amp;= \\frac{\\exp(\\beta_0 + \\beta_1 a) \\times \\exp(\\beta_1)}{\\exp(\\beta_0 + \\beta_1 a)}\\\\ &amp;= \\exp(\\beta_1) \\end{aligned}\\] Taking \\(\\log\\) on both sides, we have \\[\\log(\\text{Odds Ratio}) = \\beta_1\\] Hence, the odds ratio between these two subjects (they differ only with one unit of age) can be directly interpreted as the exponential of the parameter of age. After taking the log, we can also say that The parameter \\(\\beta\\) of a varaible in a logistic regression represents the log of odds ratio associated with one-unit increase of this variable. Please note that we usually do not be explicit about what this odds ratio is about (what two subject we are comparing). Because the interpretation of the parameter does not change regardless of the value \\(a\\), as long as the two subjects differ in one unit. And also note that this conclusion is regardless of the values of other covaraites. When we have a multivariate model, as long as all other covariates are held the same, the previous derivation will remain the same. 10.4 Solving a Logistic Regression The logistic regression is solved by maximizing the log-likelihood function. Note that the log-likelihood is given by \\[\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\log \\, p(y_i | x_i, \\boldsymbol{\\beta}).\\] Using the probabilities of Bernoulli distribution, we have \\[\\begin{align} \\ell(\\boldsymbol{\\beta}) =&amp; \\sum_{i=1}^n \\log \\left\\{ \\eta(\\mathbf{x}_i)^{y_i} [1-\\eta(\\mathbf{x}_i)]^{1-y_i} \\right\\}\\\\ =&amp; \\sum_{i=1}^n y_i \\log \\frac{\\eta(\\mathbf{x}_i)}{1-\\eta(\\mathbf{x}_i)} + \\log [1-\\eta(\\mathbf{x}_i)] \\\\ =&amp; \\sum_{i=1}^n y_i \\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}- \\log [ 1 + \\exp(\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta})] \\end{align}\\] Since this objective function is relatively simple, we can use Newton’s method to update. The gradient is given by \\[\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} =~ \\sum_{i=1}^n y_i \\mathbf{x}_i^\\text{T}- \\sum_{i=1}^n \\frac{\\exp(\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}) \\mathbf{x}_i^\\text{T}}{1 + \\exp(\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta})},\\] and the Hessian matrix is given by \\[\\frac{\\partial^2 \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}\\partial \\boldsymbol{\\beta}^\\text{T}} =~ - \\sum_{i=1}^n \\mathbf{x}_i \\mathbf{x}_i^\\text{T}\\eta(\\mathbf{x}_i) [1- \\eta(\\mathbf{x}_i)].\\] This leads to the update \\[\\boldsymbol{\\beta}^{\\,\\text{new}} = \\boldsymbol{\\beta}^{\\,\\text{old}} - \\left[\\frac{\\partial^2 \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}\\partial \\boldsymbol{\\beta}^\\text{T}}\\right]^{-1} \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}\\] 10.5 Example: South Africa Heart Data We use the South Africa heart data as a demonstration. The goal is to estimate the probability of chd, the indicator of coronary heart disease. library(ElemStatLearn) data(SAheart) heart = SAheart heart$famhist = as.numeric(heart$famhist)-1 n = nrow(heart) p = ncol(heart) heart.full = glm(chd~., data=heart, family=binomial) round(summary(heart.full)$coef, dig=3) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.151 1.308 -4.701 0.000 ## sbp 0.007 0.006 1.135 0.256 ## tobacco 0.079 0.027 2.984 0.003 ## ldl 0.174 0.060 2.915 0.004 ## adiposity 0.019 0.029 0.635 0.526 ## famhist 0.925 0.228 4.061 0.000 ## typea 0.040 0.012 3.214 0.001 ## obesity -0.063 0.044 -1.422 0.155 ## alcohol 0.000 0.004 0.027 0.978 ## age 0.045 0.012 3.728 0.000 # fitted value yhat = (heart.full$fitted.values&gt;0.5) table(yhat, SAheart$chd) ## ## yhat 0 1 ## FALSE 256 77 ## TRUE 46 83 Based on what we learned in class, we can solve this problem ourselves using numerical optimization. Here we will demonstrate an approach that uses general solver optim(). First, write the objective function of the logistic regression, for any value of \\(\\boldsymbol \\beta\\), \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\). # the negative log-likelihood function of logistic regression my.loglik &lt;- function(b, x, y) { bm = as.matrix(b) xb = x %*% bm # this returns the negative loglikelihood return(sum(y*xb) - sum(log(1 + exp(xb)))) } # Gradient my.gradient &lt;- function(b, x, y) { bm = as.matrix(b) expxb = exp(x %*% bm) return(t(x) %*% (y - expxb/(1+expxb))) } Let’s check the result of this function for some arbitrary \\(\\boldsymbol \\beta\\) value. # prepare the data matrix, I am adding a column of 1 for intercept x = as.matrix(cbind(&quot;intercept&quot; = 1, heart[, 1:9])) y = as.matrix(heart[,10]) # check my function b = rep(0, ncol(x)) my.loglik(b, x, y) # scalar ## [1] -320.234 # check the optimal value and the likelihood my.loglik(heart.full$coefficients, x, y) ## [1] -236.07 Then we optimize this objective function # Use a general solver to get the optimal value # Note that we are doing maximization instead of minimization, # we need to specify &quot;fnscale&quot; = -1 optim(b, fn = my.loglik, gr = my.gradient, method = &quot;BFGS&quot;, x = x, y = y, control = list(&quot;fnscale&quot; = -1)) ## $par ## [1] -6.150733305 0.006504017 0.079376464 0.173923988 0.018586578 0.925372019 0.039595096 ## [8] -0.062909867 0.000121675 0.045225500 ## ## $value ## [1] -236.07 ## ## $counts ## function gradient ## 74 16 ## ## $convergence ## [1] 0 ## ## $message ## NULL This matches our glm() solution. Now, if we do not have a general solver, we should consider using the Newton-Raphson. You need to write a function to calculate the Hessian matrix and proceed with an optimization update. # my Newton-Raphson method # set up an initial value # this is sometimes crucial... b = rep(0, ncol(x)) mybeta = my.logistic(b, x, y, tol = 1e-10, maxitr = 20, gr = my.gradient, hess = my.hessian, verbose = TRUE) ## at iteration 1, current beta is ## -4.032 0.005 0.066 0.133 0.009 0.694 0.024 -0.045 -0.001 0.027 ## at iteration 2, current beta is ## -5.684 0.006 0.077 0.167 0.017 0.884 0.037 -0.061 0 0.041 ## at iteration 3, current beta is ## -6.127 0.007 0.079 0.174 0.019 0.924 0.039 -0.063 0 0.045 ## at iteration 4, current beta is ## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045 ## at iteration 5, current beta is ## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045 ## at iteration 6, current beta is ## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045 ## at iteration 7, current beta is ## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045 # the parameter value mybeta ## [,1] ## intercept -6.1507208650 ## sbp 0.0065040171 ## tobacco 0.0793764457 ## ldl 0.1739238981 ## adiposity 0.0185865682 ## famhist 0.9253704194 ## typea 0.0395950250 ## obesity -0.0629098693 ## alcohol 0.0001216624 ## age 0.0452253496 # get the standard error estimation mysd = sqrt(diag(solve(-my.hessian(mybeta, x, y)))) With this solution, I can then get the standard errors and the p-value. You can check them with the glm() function solution. # my summary matrix round(data.frame(&quot;beta&quot; = mybeta, &quot;sd&quot; = mysd, &quot;z&quot; = mybeta/mysd, &quot;pvalue&quot; = 2*(1-pnorm(abs(mybeta/mysd)))), dig=5) ## beta sd z pvalue ## intercept -6.15072 1.30826 -4.70145 0.00000 ## sbp 0.00650 0.00573 1.13500 0.25637 ## tobacco 0.07938 0.02660 2.98376 0.00285 ## ldl 0.17392 0.05966 2.91517 0.00355 ## adiposity 0.01859 0.02929 0.63458 0.52570 ## famhist 0.92537 0.22789 4.06053 0.00005 ## typea 0.03960 0.01232 3.21382 0.00131 ## obesity -0.06291 0.04425 -1.42176 0.15509 ## alcohol 0.00012 0.00448 0.02714 0.97835 ## age 0.04523 0.01213 3.72846 0.00019 # check that with the glm fitting round(summary(heart.full)$coef, dig=5) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.15072 1.30826 -4.70145 0.00000 ## sbp 0.00650 0.00573 1.13500 0.25637 ## tobacco 0.07938 0.02660 2.98376 0.00285 ## ldl 0.17392 0.05966 2.91517 0.00355 ## adiposity 0.01859 0.02929 0.63458 0.52570 ## famhist 0.92537 0.22789 4.06053 0.00005 ## typea 0.03960 0.01232 3.21382 0.00131 ## obesity -0.06291 0.04425 -1.42176 0.15509 ## alcohol 0.00012 0.00448 0.02714 0.97835 ## age 0.04523 0.01213 3.72846 0.00019 10.6 Penalized Logistic Regression Similar to a linear regression, we can also apply penalties to a logistic regression to address collinearity problems or select variables in a high-dimensional setting. For example, if we use the Lasso penalty, the objective function is \\[\\sum_{i=1}^n \\log \\, p(y_i | x_i, \\boldsymbol{\\beta}) + \\lambda |\\boldsymbol{\\beta}|_1\\] This can be done using the glmnet package. Specifying family = \"binomial\" will ensure that a logistic regression is used, even your y is not a factor (but as numerical 0/1). library(glmnet) lasso.fit = cv.glmnet(x = data.matrix(SAheart[, 1:9]), y = SAheart[,10], nfold = 10, family = &quot;binomial&quot;) plot(lasso.fit) The procedure is essentially the same as in a linear regression. And we could obtain the estimated parameters by selecting the best \\(\\lambda\\) value. coef(lasso.fit, s = &quot;lambda.min&quot;) ## 10 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## lambda.min ## (Intercept) -6.007447249 ## sbp 0.002418113 ## tobacco 0.064475340 ## ldl 0.126087677 ## adiposity . ## famhist 0.735876295 ## typea 0.023547995 ## obesity . ## alcohol . ## age 0.040806121 "],["discriminant-analysis.html", "Chapter 11 Discriminant Analysis 11.1 Bayes Rule 11.2 Example: Linear Discriminant Analysis (LDA) 11.3 Linear Discriminant Analysis 11.4 Example: Quadratic Discriminant Analysis (QDA) 11.5 Quadratic Discriminant Analysis 11.6 Example: the Hand Written Digit Data", " Chapter 11 Discriminant Analysis When we model the probability of \\(Y\\) given \\(X\\), such as using a logistic regression, the approach is often called a soft classification, meaning that we do not directly produce the class label for prediction. However, we can also view the task as finding a function, with 0/1 as the output. In this case, the function is called a classifier: \\[f : \\mathbb{R}^p \\longrightarrow \\{0, 1\\}\\] In this case, we can directly evaluate the prediction error, which is calculated from the 0-1 loss: \\[L\\big(f(\\mathbf{x}), y \\big) = \\begin{cases} 0 \\quad \\text{if} \\quad y = f(\\mathbf{x})\\\\ 1 \\quad \\text{o.w.} \\end{cases}\\] The goal is to minimize the overall risk, the integrated loss: \\[\\text{R}(f) = \\text{E}_{X, Y} \\left[ L\\big(f(X), Y\\big) \\right]\\] Continuing the notation from the logistic regression, with \\(\\eta(\\mathbf{x}) = \\text{P}(Y = 1 | X = \\mathbf{x})\\), we can easily see the decision rule to minimize the risk is to take the dominate label for any given \\(\\mathbf{x}\\), this leads to the Bayes rule: \\[\\begin{align} f_B(\\mathbf{x}) = \\underset{f}{\\arg\\min} \\,\\, \\text{R}(f) = \\begin{cases} 1 &amp; \\text{if} \\quad \\eta(\\mathbf{x}) \\geq 1/2 \\\\ 0 &amp; \\text{if} \\quad \\eta(\\mathbf{x}) &lt; 1/2. \\\\ \\end{cases} \\end{align}\\] Note that it doesn’t matter when \\(\\eta(\\mathbf{x}) = 1/2\\) since we will make 50% mistake anyway. The risk associated with this rule is called the Bayes risk, which is the best risk we could achieve with a classification model with 0/1 loss. 11.1 Bayes Rule The essential idea of Discriminant Analysis is to estimate the densities functions of each class, and compare the densities at any given target point to perform classification. Let’s construct the Bayes rule from the Bayes prospective: \\[\\begin{align} \\text{P}(Y = 1 | X = \\mathbf{x}) &amp;= \\frac{\\text{P}(X = \\mathbf{x}| Y = 1)\\text{P}(Y = 1)}{\\text{P}(X = \\mathbf{x})} \\\\ \\text{P}(Y = 0 | X = \\mathbf{x}) &amp;= \\frac{\\text{P}(X = \\mathbf{x}| Y = 0)\\text{P}(Y = 0)}{\\text{P}(X = \\mathbf{x})} \\end{align}\\] Lets further define marginal probabilities (prior) \\(\\pi_1 = P(Y = 1)\\) and \\(\\pi_0 = 1 - \\pi_1 = P(Y = 0)\\), then, denote the conditional densities of \\(X\\) as \\[\\begin{align} f_1 = \\text{P}(X = \\mathbf{x}| Y = 1)\\\\ f_0 = \\text{P}(X = \\mathbf{x}| Y = 0)\\\\ \\end{align}\\] Note that the Bayes rule suggests to make the decision 1 when \\(\\eta(\\mathbf{x}) \\geq 1/2\\), this is equivalent to \\(\\pi_1 &gt; \\pi_0\\). Utilizing the Bayes Theorem, we have \\[\\begin{align} f_B(\\mathbf{x}) = \\underset{f}{\\arg\\min} \\,\\, \\text{R}(f) = \\begin{cases} 1 &amp; \\text{if} \\quad \\pi_1 f_1(\\mathbf{x}) \\geq \\pi_0 f_0(\\mathbf{x}) \\\\ 0 &amp; \\text{if} \\quad \\pi_1 f_1(\\mathbf{x}) &lt; \\pi_0 f_0(\\mathbf{x}). \\\\ \\end{cases} \\end{align}\\] This suggests that we can model the conditional density of \\(X\\) given \\(Y\\) instead of modeling \\(P(Y | X)\\) to make the decision. 11.2 Example: Linear Discriminant Analysis (LDA) We create two density functions that use the same covariance matrix: \\(X_1 \\sim \\cal{N}(\\mu_1, \\Sigma)\\) and \\(X_2 \\sim \\cal{N}(\\mu_2, \\Sigma)\\), with \\(\\mu_1 = (0.5, -1)^\\text{T}\\), \\(\\mu_2 = (-0.5, 0.5)^\\text{T}\\), and \\(\\Sigma_{2\\times2}\\) has diagonal elements 1 and off diagonal elements 0.5. Let’s first generate some observations. library(mvtnorm) set.seed(1) # generate two sets of samples Sigma = matrix(c(1, 0.5, 0.5, 1), 2, 2) mu1 = c(0.5, -1) mu2 = c(-0.5, 0.5) # define prior p1 = 0.4 p2 = 0.6 n = 1000 Class1 = rmvnorm(n*p1, mean = mu1, sigma = Sigma) Class2 = rmvnorm(n*p2, mean = mu2, sigma = Sigma) plot(Class1, pch = 19, col = &quot;darkorange&quot;, xlim = c(-4, 4), ylim = c(-4, 4)) points(Class2, pch = 19, col = &quot;deepskyblue&quot;) If we know their true density functions, then the decision line is linear. 11.3 Linear Discriminant Analysis As we demonstrated earlier using the Bayes rule, the conditional probability can be formulated using Bayes Theorem. For this time, we will assume in generate that there are \\(K\\) classes instead of just two. However, the notation are similar to the previous case: \\[\\begin{align} \\text{P}(Y = k | X = \\mathbf{x}) =&amp;~ \\frac{\\text{P}(X = \\mathbf{x}| Y = k)\\text{P}(Y = k)}{\\text{P}(X = \\mathbf{x})}\\\\ =&amp;~ \\frac{\\text{P}(X = \\mathbf{x}| Y = k)\\text{P}(Y = k)}{\\sum_{l=1}^K \\text{P}(X = \\mathbf{x}| Y = l) \\text{P}(Y = l)}\\\\ =&amp;~ \\frac{\\pi_k f_k(\\mathbf{x})}{\\sum_{l=1}^K \\pi_l f_l(\\mathbf{x})} \\end{align}\\] Given any target point \\(\\mathbf{x}\\), the best prediction is simply picking the one that maximizes the posterior \\[\\underset{k}{\\arg\\max} \\,\\, \\pi_k f_k(x)\\] Both LDA and QDA model \\(f_k\\) as a normal density function. Suppose we model each class density as multivariate Gaussian \\({\\cal N}(\\boldsymbol{\\mu}_k, \\Sigma_k)\\), and . Then \\[f_k(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left[ -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)^\\text{T}\\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) \\right].\\] The log-likelihood function for the conditional distribution is \\[\\begin{align} \\log f_k(\\mathbf{x}) =&amp;~ -\\log \\big((2\\pi)^{p/2} |\\Sigma|^{1/2} \\big) - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)^\\text{T}\\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) \\\\ =&amp;~ - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)^\\text{T}\\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) + \\text{Constant} \\end{align}\\] The maximum a posteriori probability (MAP) estimate is simply \\[\\begin{align} \\widehat f(\\mathbf{x}) =&amp; ~\\underset{k}{\\arg\\max} \\,\\, \\log \\big( \\pi_k f_k(\\mathbf{x}) \\big) \\\\ =&amp; ~\\underset{k}{\\arg\\max} \\,\\, - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)^\\text{T}\\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) + \\log(\\pi_k) \\end{align}\\] The term \\((\\mathbf{x}- \\boldsymbol{\\mu}_k)^\\text{T}\\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k)\\) is simply the between \\(x\\) and the centroid \\(\\boldsymbol{\\mu}_k\\) for class \\(k\\). Hence, this is essentially classifying \\(x\\) to the class label with the closest centroid (after adjusting the for prior). This sets a connection with the \\(k\\)NN algorithm. A special case is that when \\(\\Sigma = \\mathbf{I}\\), i.e., only Euclidean distance is needed, and we have \\[\\underset{k}{\\arg\\max} \\,\\, - \\frac{1}{2} \\lVert x - \\boldsymbol{\\mu}_k \\rVert^2 + \\log(\\pi_k)\\] The decision boundary of LDA, as its name suggests, is a linear function of \\(\\mathbf{x}\\). To see this, let’s look at the terms in the MAP. Note that anything that does not depends on the class index \\(k\\) is irrelevant to the decision. \\[\\begin{align} &amp; - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)^\\text{T}\\Sigma^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) + \\log(\\pi_k)\\\\ =&amp;~ \\mathbf{x}^\\text{T}\\Sigma^{-1} \\boldsymbol{\\mu}_k - \\frac{1}{2}\\boldsymbol{\\mu}_k^\\text{T}\\Sigma^{-1} \\boldsymbol{\\mu}_k + \\log(\\pi_k) \\text{irrelevant terms} \\\\ =&amp;~ \\mathbf{x}^\\text{T}\\mathbf{w}_k + b_k + \\text{irrelevant terms} \\end{align}\\] Then, the decision boundary between two classes, \\(k\\) and \\(l\\) is \\[\\begin{align} \\mathbf{x}^\\text{T}\\mathbf{w}_k + b_k &amp;= \\mathbf{x}^\\text{T}\\mathbf{w}_l + b_l \\\\ \\Leftrightarrow \\quad \\mathbf{x}^\\text{T}(\\mathbf{w}_k - \\mathbf{w}_l) + (b_k - b_l) &amp;= 0, \\\\ \\end{align}\\] which is a linear function of \\(\\mathbf{x}\\). The previous density plot already showed this effect. Estimating the parameters in LDA is very simple: Prior probabilities: \\(\\widehat{\\pi}_k = n_k / n = n^{-1} \\sum_k \\mathbf{1}\\{y_i = k\\}\\), where \\(n_k\\) is the number of observations in class \\(k\\). Centroids: \\(\\widehat{\\boldsymbol{\\mu}}_k = n_k^{-1} \\sum_{i: \\,y_i = k} x_i\\) Pooled covariance matrix: \\[\\widehat \\Sigma = \\frac{1}{n-K} \\sum_{k=1}^K \\sum_{i : \\, y_i = k} (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k) (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k)^\\text{T}\\] 11.4 Example: Quadratic Discriminant Analysis (QDA) When we assume that each class has its own covariance structure, the decision boundary will not be linear anymore. Let’s visualize this by creating two density functions that use different covariance matrices. 11.5 Quadratic Discriminant Analysis QDA simply abandons the assumption of the common covariance matrix. Hence, \\(\\Sigma_k\\)’s are not equal. In this case, the determinant \\(|\\Sigma_k|\\) of each covariance matrix will be different. In addition, the MAP decision becomes a quadratic function of the target point \\(\\mathbf{x}\\) \\[\\begin{align} &amp; \\underset{k}{\\max} \\,\\, \\log \\big( \\pi_k f_k(x) \\big) \\\\ =&amp; ~\\underset{k}{\\max} \\,\\, -\\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu}_k)^\\text{T}\\Sigma_k^{-1} (\\mathbf{x}- \\boldsymbol{\\mu}_k) + \\log(\\pi_k) \\\\ =&amp; \\mathbf{x}^\\text{T}\\mathbf{W}_k \\mathbf{x}+ \\mathbf{w}_k^\\text{T}\\mathbf{x}+ b_k \\end{align}\\] This leads to quadratic decision boundary between class \\(k\\) and \\(l\\) \\[\\big\\{\\mathbf{x}: \\mathbf{x}^\\text{T}(\\mathbf{W}_k - \\mathbf{W}_l) \\mathbf{x}+ \\mathbf{x}^\\text{T}(\\mathbf{w}_k - \\mathbf{w}_l) + (b_k - b_l) = 0\\big\\}.\\] The estimation procedure is also similar: Prior probabilities: \\(\\widehat{\\pi}_k = n_k / n = n^{-1} \\sum_k \\mathbf{1}\\{y_i = k\\}\\), where \\(n_k\\) is the number of observations in class \\(k\\). Centroid: \\(\\widehat{\\boldsymbol{\\mu}}_k = n_k^{-1} \\sum_{i: \\,y_i = k} \\mathbf{x}_i\\) Sample covariance matrix for each class: \\[\\widehat \\Sigma_k = \\frac{1}{n_k-1} \\sum_{i : \\, y_i = k} (\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_i - \\widehat{\\boldsymbol{\\mu}}_k)^\\text{T}\\] 11.6 Example: the Hand Written Digit Data We first sample 100 data from both the training and testing sets. library(ElemStatLearn) # a plot of some samples findRows &lt;- function(zip, n) { # Find n (random) rows with zip representing 0,1,2,...,9 res &lt;- vector(length=10, mode=&quot;list&quot;) names(res) &lt;- 0:9 ind &lt;- zip[,1] for (j in 0:9) { res[[j+1]] &lt;- sample( which(ind==j), n ) } return(res) } set.seed(1) # find 100 samples for each digit for both the training and testing data train.id &lt;- findRows(zip.train, 100) train.id = unlist(train.id) test.id &lt;- findRows(zip.test, 100) test.id = unlist(test.id) X = zip.train[train.id, -1] Y = zip.train[train.id, 1] dim(X) ## [1] 1000 256 Xtest = zip.test[test.id, -1] Ytest = zip.test[test.id, 1] dim(Xtest) ## [1] 1000 256 We can then fit LDA and QDA and predict. # fit LDA library(MASS) dig.lda=lda(X,Y) Ytest.pred=predict(dig.lda, Xtest)$class table(Ytest, Ytest.pred) ## Ytest.pred ## Ytest 0 1 2 3 4 5 6 7 8 9 ## 0 92 0 2 2 0 0 1 0 3 0 ## 1 0 94 0 0 4 0 2 0 0 0 ## 2 2 2 66 7 5 2 4 2 10 0 ## 3 2 0 3 75 2 8 0 3 6 1 ## 4 0 4 2 1 76 1 3 2 2 9 ## 5 2 0 3 10 0 79 0 0 3 3 ## 6 0 0 4 1 3 4 86 0 1 1 ## 7 0 0 0 2 5 0 0 87 0 6 ## 8 2 0 4 5 6 7 1 0 72 3 ## 9 0 0 0 1 4 0 0 5 0 90 mean(Ytest != Ytest.pred) ## [1] 0.183 However, QDA does not work in this case because there are too many parameters dig.qda = qda(X, Y) # error message "],["k-neariest-neighber.html", "Chapter 12 K-Neariest Neighber 12.1 Definition 12.2 Tuning \\(k\\) 12.3 The Bias-variance Trade-off 12.4 KNN for Classification 12.5 Example 1: An artificial data 12.6 Degrees of Freedom 12.7 Tuning with the caret Package 12.8 Distance Measures 12.9 1NN Error Bound 12.10 Example 2: Handwritten Digit Data 12.11 Curse of Dimensionality", " Chapter 12 K-Neariest Neighber Method we introduced before usually have very clear definition of what the parameters are. In a linear model, we have a set of parameters \\(\\boldsymbol{\\beta}\\) and our estimated function value, for any target point \\(x_0\\) is \\(x_0^\\text{T}\\boldsymbol{\\beta}\\). As the model gets more complicated, we may need models that can be even more flexible. An example we saw previously is the smoothing spline, in which the features (knots) are adaptively defined using the observed samples, and there are \\(n\\) parameters. Many of the nonparametric models shares this property. Probably the simplest one of them is the \\(K\\) nearest neighbor (KNN) method, which is based on the idea of local averaging, i.e., we estimate \\(f(x_0)\\) using observations close to \\(x_0\\). KNN can be used for both regression and classification problems. This is traditionally called nonparametric models in statistics. 12.1 Definition Suppose we collect a set of observations \\(\\{x_i, y_i\\}_{i=1}^n\\), the prediction at a new target point \\(x_0\\) is \\[\\widehat y = \\frac{1}{k} \\sum_{x_i \\in N_k(x_0)} y_i,\\] where \\(N_k(x_0)\\) defines the \\(k\\) samples from the training data that are closest to \\(x_0\\). As default, closeness is defined using a distance measure, such as the Euclidean distance. Here is a demonstration of the fitted regression function. # generate training data with 2*sin(x) and random Gaussian errors set.seed(1) x &lt;- runif(15, 0, 2*pi) y &lt;- 2*sin(x) + rnorm(length(x)) # generate testing data points where we evaluate the prediction function test.x = seq(0, 1, 0.001)*2*pi # &quot;1-nearest neighbor&quot; regression using kknn package library(kknn) knn.fit = kknn(y ~ x, train = data.frame(x = x, y = y), test = data.frame(x = test.x), k = 1, kernel = &quot;rectangular&quot;) test.pred = knn.fit$fitted.values # plot the data par(mar=rep(2,4)) plot(x, y, xlim = c(0, 2*pi), pch = &quot;o&quot;, cex = 2, xlab = &quot;&quot;, ylab = &quot;&quot;, cex.lab = 1.5) title(main=&quot;1-Nearest Neighbor Regression&quot;, cex.main = 1.5) # plot the true regression line lines(test.x, 2*sin(test.x), col = &quot;deepskyblue&quot;, lwd = 3) # plot the fitted line lines(test.x, test.pred, type = &quot;s&quot;, col = &quot;darkorange&quot;, lwd = 3) legend(&quot;topright&quot;, c(&quot;Fitted line&quot;, &quot;True function&quot;), col = c(&quot;darkorange&quot;, &quot;deepskyblue&quot;), lty = 1, cex = 1.5) 12.2 Tuning \\(k\\) Tuning \\(k\\) is crucial for \\(k\\)NN. Let’s observe its effect. This time, I generate 200 observations. For 1NN, the fitted regression line is very jumpy. # generate more data set.seed(1) n = 200 x &lt;- runif(n, 0, 2*pi) y &lt;- 2*sin(x) + rnorm(length(x)) test.y = 2*sin(test.x) + rnorm(length(test.x)) # 1-nearest neighbor knn.fit = kknn(y ~ x, train = data.frame(x = x, y = y), test = data.frame(x = test.x), k = 1, kernel = &quot;rectangular&quot;) test.pred = knn.fit$fitted.values We can evaluate the prediction error of this model: # prediction error mean((test.pred - test.y)^2) ## [1] 2.097488 If we consider different values of \\(k\\), we can observe the trade-off between bias and variance. ## Prediction Error for K = 1 : 2.09748780881932 ## Prediction Error for K = 5 : 1.39071867992277 ## Prediction Error for K = 10 : 1.24696415340282 ## Prediction Error for K = 33 : 1.21589627474692 ## Prediction Error for K = 66 : 1.37604707375972 ## Prediction Error for K = 100 : 1.42868908518756 As \\(k\\) increases, we have a more stable model, i.e., smaller variance. However, the bias is also increased. As \\(k\\) decreases, the bias decreases, but the model is less stable. 12.3 The Bias-variance Trade-off Formally, the prediction error (at a given target point \\(x_0\\)) can be broke down into three parts: the irreducible error, the bias squared, and the variance. \\[\\begin{aligned} \\text{E}\\Big[ \\big( Y - \\widehat f(x_0) \\big)^2 \\Big] &amp;= \\text{E}\\Big[ \\big( Y - f(x_0) + f(x_0) - \\text{E}[\\widehat f(x_0)] + \\text{E}[\\widehat f(x_0)] - \\widehat f(x_0) \\big)^2 \\Big] \\\\ &amp;= \\text{E}\\Big[ \\big( Y - f(x_0) \\big)^2 \\Big] + \\text{E}\\Big[ \\big(f(x_0) - \\text{E}[\\widehat f(x_0)] \\big)^2 \\Big] + \\text{E}\\Big[ \\big(E[\\widehat f(x_0)] - \\widehat f(x_0) \\big)^2 \\Big] + \\text{Cross Terms}\\\\ &amp;= \\underbrace{\\text{E}\\Big[ ( Y - f(x_0))^2 \\big]}_{\\text{Irreducible Error}} + \\underbrace{\\Big(f(x_0) - \\text{E}[\\widehat f(x_0)]\\Big)^2}_{\\text{Bias}^2} + \\underbrace{\\text{E}\\Big[ \\big(\\widehat f(x_0) - \\text{E}[\\widehat f(x_0)] \\big)^2 \\Big]}_{\\text{Variance}} \\end{aligned}\\] As we can see from the previous example, when \\(k=1\\), the prediction error is about 2. This is because for all the testing points, the theoretical irreducible error is 1 (variance of the error term), the bias is almost 0 since the function is smooth, and the variance is the variance of 1 nearest neighbor, which is again 1. On the other extreme side, when \\(k = n\\), the variance should be in the level of \\(1/n\\), the bias is the difference between the sin function and the overall average. Overall, we can expect the trend: As \\(k\\) increases, bias increases and variance decreases As \\(k\\) decreases, bias decreases and variance increases 12.4 KNN for Classification For classification, KNN is different from the regression model in term of finding neighbers. The only difference is to majority voting instead of averaging. Majority voting means that we look for the most popular class label among its neighbors. For 1NN, it is simply the class of the closest neighbor. The visualization of 1NN is a Voronoi tessellation. The plot on the left is some randomly observed data in \\([0, 1]^2\\), and the plot on the right is the corresponding 1NN classification model. 12.5 Example 1: An artificial data We use artificial data from the ElemStatLearn package. library(ElemStatLearn) x &lt;- mixture.example$x y &lt;- mixture.example$y xnew &lt;- mixture.example$xnew par(mar=rep(2,4)) plot(x, col=ifelse(y==1, &quot;darkorange&quot;, &quot;deepskyblue&quot;), axes = FALSE, pch = 19) box() The decision boundary is highly nonlinear. We can utilize the contour() function to demonstrate the result. # knn classification k = 15 knn.fit &lt;- knn(x, xnew, y, k=k) px1 &lt;- mixture.example$px1 px2 &lt;- mixture.example$px2 pred &lt;- matrix(knn.fit == &quot;1&quot;, length(px1), length(px2)) contour(px1, px2, pred, levels=0.5, labels=&quot;&quot;,axes=FALSE) box() title(paste(k, &quot;-Nearest Neighbor&quot;, sep= &quot;&quot;)) points(x, col=ifelse(y==1, &quot;darkorange&quot;, &quot;deepskyblue&quot;), pch = 19) mesh &lt;- expand.grid(px1, px2) points(mesh, pch=&quot;.&quot;, cex=1.2, col=ifelse(pred, &quot;darkorange&quot;, &quot;deepskyblue&quot;)) We can evaluate the in-sample prediction result of this model using a confusion matrix: # the confusion matrix knn.fit &lt;- knn(x, x, y, k = 15) xtab = table(knn.fit, y) library(caret) confusionMatrix(xtab) ## Confusion Matrix and Statistics ## ## y ## knn.fit 0 1 ## 0 82 13 ## 1 18 87 ## ## Accuracy : 0.845 ## 95% CI : (0.7873, 0.8922) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.69 ## ## Mcnemar&#39;s Test P-Value : 0.4725 ## ## Sensitivity : 0.8200 ## Specificity : 0.8700 ## Pos Pred Value : 0.8632 ## Neg Pred Value : 0.8286 ## Prevalence : 0.5000 ## Detection Rate : 0.4100 ## Detection Prevalence : 0.4750 ## Balanced Accuracy : 0.8450 ## ## &#39;Positive&#39; Class : 0 ## 12.6 Degrees of Freedom From our ridge lecture, we have a definition of the degrees of freedom. For a linear model, the degrees of freedom is \\(p\\). For KNN, we can simply verify this by \\[\\begin{align} \\text{DF}(\\widehat{f}) =&amp; \\frac{1}{\\sigma^2} \\sum_{i = 1}^n \\text{Cov}(\\widehat{y}_i, y_i)\\\\ =&amp; \\frac{1}{\\sigma^2} \\sum_{i = 1}^n \\frac{\\sigma^2}{k}\\\\ =&amp; \\frac{n}{k} \\end{align}\\] 12.7 Tuning with the caret Package The caret package has some built-in feature that can tune some popular machine learning models using cross-validation. The cross-validation settings need to be specified using the \\(trainControl()\\) function. library(caret) control &lt;- trainControl(method = &quot;cv&quot;, number = 10) There are other cross-validation methods, such as repeatedcv the repeats the CV several times, and leave-one-out CV LOOCV. For more details, you can read the documentation. We can then setup the training by specifying a grid of \\(k\\) values, and also the CV setup. Make sure that you specify method = \"knn\" and also construct the outcome as a factor in a data frame. set.seed(1) knn.cvfit &lt;- train(y ~ ., method = &quot;knn&quot;, data = data.frame(&quot;x&quot; = x, &quot;y&quot; = as.factor(y)), tuneGrid = data.frame(k = seq(1, 40, 1)), trControl = control) plot(knn.cvfit$results$k, 1-knn.cvfit$results$Accuracy, xlab = &quot;K&quot;, ylab = &quot;Classification Error&quot;, type = &quot;b&quot;, pch = 19, col = &quot;darkorange&quot;) Print out the fitted object, we can see that the best \\(k\\) is 6. And there is a clear “U” shaped pattern that shows the potential bias-variance trade-off. 12.8 Distance Measures Closeness between two points needs to be defined based on some distance measures. By default, we use the squared Euclidean distance (\\(\\ell_2\\) norm) for continuous variables: \\[d^2(\\mathbf{u}, \\mathbf{v}) = \\lVert \\mathbf{u}- \\mathbf{v}\\rVert_2^2 = \\sum_{j=1}^p (u_j, v_j)^2.\\] However, this measure is not scale invariant. A variable with large scale can dominate this measure. Hence, we often consider a normalized version: \\[d^2(\\mathbf{u}, \\mathbf{v}) = \\sum_{j=1}^p \\frac{(u_j, v_j)^2}{\\sigma_j^2},\\] where \\(\\sigma_j^2\\) can be estimated using the sample variance of variable \\(j\\). Another choice that further taking the covariance structure into consideration is the Mahalanobis distance: \\[d^2(\\mathbf{u}, \\mathbf{v}) = (\\mathbf{u}- \\mathbf{v})^\\text{T}\\Sigma^{-1} (\\mathbf{u}- \\mathbf{v}),\\] where \\(\\Sigma\\) is the covariance matrix, and can be estimated using the sample covariance. In the following plot, the red cross and orange cross have the same Euclidean distance to the center. However, the red cross is more of a “outlier” based on the joint distribution. The Mahalanobis distance would reflect this. x=rnorm(100) y=1 + 0.3*x + 0.3*rnorm(100) library(car) dataEllipse(x, y, levels=c(0.6, 0.9), col = c(&quot;black&quot;, &quot;deepskyblue&quot;), pch = 19) points(1, 0.5, col = &quot;red&quot;, pch = 4, cex = 2, lwd = 4) points(1, 1.5, col = &quot;darkorange&quot;, pch = 4, cex = 3, lwd = 4) For categorical variables, the Hamming distance is commonly used: \\[d(\\mathbf{u}, \\mathbf{v}) = \\sum_{j=1}^p I(u_j \\neq v_j).\\] It simply counts how many entries are not the same. 12.9 1NN Error Bound We can show that 1NN can achieve reasonable performance for fixed \\(p\\), as \\(n \\rightarrow \\infty\\) by showing that the 1NN error is no more than twice of the Bayes error, which is the smaller one of \\(P(Y = 1 | X = x_0)\\) and \\(1 - P(Y = 1 | X = x_0)\\). Let’s denote \\(x_{1nn}\\) the closest neighbor of \\(x_0\\), we have \\(d(x_0, x_{1nn}) \\rightarrow 0\\) by the law of large numbers. Assuming smoothness, we have \\(P(Y = 1 | x_{1nn}) \\rightarrow P(Y = 1 | x_0)\\). Hence, the 1NN error is the chance we make a wrong prediction, which can happen in two situations. WLOG, let’s assume that \\(P(Y = 1 | X = x_0)\\) is larger than \\(1-P(Y = 1 | X = x_0)\\), then \\[\\begin{align} &amp; \\,P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})] + P(Y=1|x_0)[ 1 - P(Y=1|x_{1nn})]\\\\ \\leq &amp; \\,[ 1 - P(Y=1|x_{1nn})] + [ 1 - P(Y=1|x_{1nn})]\\\\ \\rightarrow &amp; \\, 2[ 1 - P(Y=1|x_0)]\\\\ = &amp; \\,2 \\times \\text{Bayes Error}\\\\ \\end{align}\\] This is a crude bound, but shows that 1NN can still be a reasonable estimator when the noise is small (Bayes error small). 12.10 Example 2: Handwritten Digit Data Let’s consider another example using the handwritten digit data. Each observation in this data is a \\(16 \\times 16\\) pixel image. Hence, the total number of variables is \\(256\\). At each pixel, we have the gray scale as the numerical value. # Handwritten Digit Recognition Data library(ElemStatLearn) # the first column is the true digit dim(zip.train) ## [1] 7291 257 dim(zip.test) ## [1] 2007 257 # look at one sample image(zip2image(zip.train, 1), col=gray(256:0/256), zlim=c(0,1), xlab=&quot;&quot;, ylab=&quot;&quot;, axes = FALSE) ## [1] &quot;digit 6 taken&quot; We use 3NN to predict all samples in the testing data. The model is fairly accurate. # fit 3nn model and calculate the error knn.fit &lt;- knn(zip.train[, 2:257], zip.test[, 2:257], zip.train[, 1], k=3) # overall prediction error mean(knn.fit != zip.test[,1]) ## [1] 0.05430992 # the confusion matrix table(knn.fit, zip.test[,1]) ## ## knn.fit 0 1 2 3 4 5 6 7 8 9 ## 0 355 0 6 1 0 3 3 0 4 2 ## 1 0 258 0 0 2 0 0 1 0 0 ## 2 1 0 182 2 0 2 1 1 2 0 ## 3 0 0 1 153 0 4 0 1 5 0 ## 4 0 3 1 0 183 0 2 4 0 3 ## 5 0 0 0 7 2 146 0 0 1 0 ## 6 0 2 2 0 2 0 164 0 0 0 ## 7 2 1 2 1 2 0 0 138 1 4 ## 8 0 0 4 0 1 1 0 1 151 0 ## 9 1 0 0 2 8 4 0 1 2 168 12.11 Curse of Dimensionality Many of the practical problems we encounter today are high-dimensional. The resolution of the handwritten digit example is \\(16 \\times 16 = 256\\). Genetic studies often involves more than 25K gene expressions, etc. For a given sample size \\(n\\), as the number of variables \\(p\\) increases, the data becomes very sparse. Nearest neighbor methods usually do not perform very well on high-dimensional data. This is because for any given target point, there will not be enough training data that lies close enough. To see this, let’s consider a \\(p\\)-dimensional hyper-cube. Suppose we have \\(n=1000\\) observations uniformly spread out in this cube, and we are interested in predicting a target point with \\(k=10\\) neighbors. If these neighbors are really close to the target point, then this would be a good estimation with small bias. Suppose \\(p=2\\), then we know that if we take a cube (square) with height and width both \\(l = 0.1\\), then there will be \\(1000 \\times 0.1^2 = 10\\) observations within the square. In general, we have the relationship \\[l^p = \\frac{k}{n}\\] Try different \\(p\\), we have If \\(p = 2\\), \\(l = 0.1\\) If \\(p = 10\\), \\(l = 0.63\\) If \\(p = 100\\), \\(l = 0.955\\) This implies that if we have 100 dimensions, then the nearest 10 observations would be 0.955 away from the target point at each dimension, this is almost at the other corner of the cube. Hence there will be a very large bias. Decreasing \\(k\\) does not help much in this situation since even the closest point can be really far away, and the model would have large variance. However, why our model performs well in the handwritten digit example? There is possibly (approximately) a lower dimensional representation of the data so that when you evaluate the distance on the high-dimensional space, it is just as effective as working on the low dimensional space. Dimension reduction is an important topic in statistical learning and machine learning. Many methods, such as sliced inverse regression (Li 1991) and UMAP (McInnes, Healy, and Melville 2018) have been developed based on this concept. Image from Cayton (2005). Reference Cayton, Lawrence. 2005. “Algorithms for Manifold Learning.” Univ. Of California at San Diego Tech. Rep 12 (1-17): 1. Li, Ker-Chau. 1991. “Sliced Inverse Regression for Dimension Reduction.” Journal of the American Statistical Association 86 (414): 316–27. McInnes, Leland, John Healy, and James Melville. 2018. “Umap: Uniform Manifold Approximation and Projection for Dimension Reduction.” arXiv Preprint arXiv:1802.03426. "],["kernel-smoothing.html", "Chapter 13 Kernel Smoothing 13.1 KNN vs. Kernel 13.2 Kernel Density Estimations 13.3 Bias-variance trade-off 13.4 Gaussian Kernel Regression 13.5 Choice of Kernel Functions 13.6 Local Linear Regression 13.7 Local Polynomial Regression 13.8 R Implementations", " Chapter 13 Kernel Smoothing Fundamental ideas of local regression approaches are similar to \\(k\\)NN. But most approaches would address a fundamental drawback of \\(k\\)NN that the estimated function is not smooth. Having a smoothed estimation would also allow us to estimate the derivative, which is essentially used when estimating the density function. We will start with the intuition of the kernel estimator and then discuss the bias-variance trade-off using kernel density estimation as an example. 13.1 KNN vs. Kernel We first compare the \\(K\\)NN method with a Gaussian kernel regression. \\(K\\)NN has jumps while Gaussian kernel regression is smooth. 13.2 Kernel Density Estimations A natural estimator, by using the counts, is \\[\\widehat f(x) = \\frac{\\#\\big\\{x_i: x_i \\in [x - \\frac{h}{2}, x + \\frac{h}{2}]\\big\\}}{h n}\\] This maybe compared with the histogram estimator library(ggplot2) data(mpg) # histogram hist(mpg$hwy, breaks = seq(6, 50, 2)) # uniform kernel xgrid = seq(6, 50, 0.1) histden = sapply(xgrid, FUN = function(x, obs, h) sum( ((x-h/2) &lt;= obs) * ((x+h/2) &gt; obs))/h/length(obs), obs = mpg$hwy, h = 2) plot(xgrid, histden, type = &quot;s&quot;) This can be view in two ways. The easier interpretation is that, for each target point, we count how many observations are close-by. We can also interpret it as evenly distributing the point-mass of each observation to a close-by region with width \\(h\\), and then stack them all. \\[\\widehat f(x) = \\frac{1}{h n} \\sum_i \\,\\underbrace{ \\mathbf{1} \\Big(x \\in [x_i - \\frac{h}{2}, x_i + \\frac{h}{2}]}_{\\text{uniform density centered at } x_i} \\Big)\\] Here is a close-up demonstration of how those uniform density functions are stacked for all observations. However, this is will lead to jumps at the end of each small uniform density. Let’s consider using a smooth function instead. Naturally, we can use the Gaussian kernel function to calculate the numerator in the above equation. We apply this to the mpg dataset. xgrid = seq(6, 50, 0.1) kernelfun &lt;- function(x, obs, h) sum(exp(-0.5*((x-obs)/h)^2)/sqrt(2*pi)/h) plot(xgrid, sapply(xgrid, FUN = kernelfun, obs = mpg$hwy, h = 1.5)/length(mpg$hwy), type = &quot;l&quot;, xlab = &quot;MPG&quot;, ylab = &quot;Estimated Density&quot;, col = &quot;darkorange&quot;, lwd = 3) The ggplot2 packages provides some convenient features to plot the density and histogram. ggplot(mpg, aes(x=hwy)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;white&quot;)+ geom_density(alpha=.2, fill=&quot;#ff8c00&quot;) ## Warning: The dot-dot notation (`..density..`) was deprecated in ## ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where ## this warning was generated. 13.3 Bias-variance trade-off Let’s consider estimating a density, using the Parzen estimator \\[\\widehat f(x) = \\frac{1}{n} \\sum_{i=1}^n K_{h} (x, x_i)\\] here, \\(K_h(\\mathbf{u}, \\mathbf{v}) = K(|\\mathbf{u}- \\mathbf{v}|/h)/h\\) is a kernel function that satisfies \\(\\int K(u)du = 1\\) (a proper density) \\(K(-u) = K(u)\\) (symmetric) \\(\\int K(u) u^2 du \\leq \\infty\\) (finite second moment) Note that \\(h\\) simply scales the covariate and adjust the density accordingly. Our goal is to estimate a target point \\(x\\) using a set of iid data. First, we can analyze the bias: \\[\\begin{align} \\text{E}\\big[ \\widehat f(x) \\big] &amp;= \\text{E}\\left[ K\\left( \\frac{x - x_1}{h} \\right) \\Big/ h \\right] \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{1}{h} K\\left(\\frac{x-x_1}{h}\\right) f(x_1) d x_1 \\\\ &amp;= \\int_{\\infty}^{-\\infty} \\frac{1}{h} K(t) f(x - th) d (x-th) \\\\ (\\text{Taylor expansion}) \\quad &amp;= f(x) + \\frac{h^2}{2} f&#39;&#39;(x) \\int_{-\\infty}^\\infty K(t) t^2 dt + o(h^2) \\\\ (\\text{as } ha \\rightarrow 0) \\quad &amp;\\rightarrow f(x) \\end{align}\\] Since the density is over the entire domain, we can define the integrated Bias\\(^2\\): \\[\\begin{align} \\text{Bias}^2 &amp;= \\int \\left( E[\\widehat f(x)] - f(x)\\right)^2 dx \\\\ &amp;\\approx \\frac{h^4 \\sigma_K^4}{4} \\int \\big[ f&#39;&#39;(x)\\big]^2 dx \\end{align}\\] where \\(\\sigma_K^2 = \\int_{-\\infty}^\\infty K(t) t^2 dt\\). On the other hand, the variance term is \\[\\begin{align} \\text{Var}\\big[ \\widehat f(x) \\big] &amp;= \\frac{1}{n} \\text{Var}\\Big[\\frac{1}{h}K\\big( \\frac{x - x_1}{h} \\big) \\Big] \\\\ &amp;= \\frac{1}{n} \\text{E}\\bigg[ \\frac{1}{h^2} K^2\\big( \\frac{x - x_1}{h}\\big) - \\text{E}\\Big[ \\frac{1}{h} K\\big( \\frac{x - x_1}{h} \\big)\\Big]^2 \\bigg]\\\\ &amp;= \\frac{1}{n} \\Big[ \\int \\frac{1}{h} K^2( \\frac{x - x_1}{h} ) f(x_1) dx_1 + O(1) \\Big] \\\\ &amp;= \\frac{1}{n} \\Big[ \\frac{1}{h} \\int K^2( u ) f(x) du + O(1) \\Big] \\\\ &amp;= \\frac{f(x)}{nh} \\int K^2( u ) du \\end{align}\\] with the integrated variance being \\[\\frac{1}{nh} \\int K^2( u ) dt \\] By minimizing the asymptotic mean integrated squared error (AMISE), defined as the sum of integrated Bias\\(^2\\) and variance, we have the optimal \\(h\\) being \\[h^\\text{opt} = \\bigg[\\frac{1}{n} \\frac{\\int K^2(u)du}{ \\sigma^2_K \\int f&#39;&#39;(u)du} \\bigg]^{1/5},\\] and the optimal \\(h\\) is in the order of \\(\\cal O(n^{-4/5})\\). 13.4 Gaussian Kernel Regression A Nadaraya-Watson kernel regression model has the following formula. Note that we use \\(h\\) as the bandwidth instead of \\(h\\). \\[\\widehat f(x) = \\frac{\\sum_i K_h(x, x_i) y_i}{\\sum_i K_h(x, x_i)},\\] where \\(h\\) is the bandwidth. At each target point \\(x\\), training data \\(x_i\\)s that are closer to \\(x\\) receives higher weights \\(K_h(x, x_i)\\), hence their \\(y_i\\) values are more influential in terms of estimating \\(f(x)\\). For the Gaussian kernel, we use \\[K_h(x, x_i) = \\frac{1}{h\\sqrt{2\\pi}} \\exp\\left\\{ -\\frac{(x - x_i)^2}{2 h^2}\\right\\}\\] 13.4.1 Bias-variance Trade-off The bandwidth \\(h\\) is an important tuning parameter that controls the bias-variance trade-off. It behaves the same as the density estimation. By setting a large \\(h\\), the estimator is more stable but has more bias. # a small bandwidth ksmooth.fit1 = ksmooth(x, y, bandwidth = 0.5, kernel = &quot;normal&quot;, x.points = testx) # a large bandwidth ksmooth.fit2 = ksmooth(x, y, bandwidth = 2, kernel = &quot;normal&quot;, x.points = testx) # plot both plot(x, y, xlim = c(0, 2*pi), pch = 19, xaxt=&#39;n&#39;, yaxt=&#39;n&#39;) lines(testx, 2*sin(testx), col = &quot;deepskyblue&quot;, lwd = 3) lines(testx, ksmooth.fit1$y, type = &quot;s&quot;, col = &quot;darkorange&quot;, lwd = 3) lines(testx, ksmooth.fit2$y, type = &quot;s&quot;, col = &quot;red&quot;, lwd = 3) legend(&quot;topright&quot;, c(&quot;h = 0.5&quot;, &quot;h = 2&quot;), col = c(&quot;darkorange&quot;, &quot;red&quot;), lty = 1, lwd = 2, cex = 1.5) 13.5 Choice of Kernel Functions Other kernel functions can also be used. The most efficient kernel is the Epanechnikov kernel, which will minimize the mean integrated squared error (MISE). The efficiency is defined as \\[ \\Big(\\int u^2K(u) du\\Big)^\\frac{1}{2} \\int K^2(u) du, \\] Different kernel functions can be visualized in the following. Most kernels are bounded within \\([-h/2, h/2]\\), except the Gaussian kernel. 13.6 Local Linear Regression Local averaging will suffer severe bias at the boundaries. One solution is to use the local polynomial regression. The following examples are local linear regressions, evaluated as different target points. We are solving for a linear model weighted by the kernel weights \\[\\sum_{i = 1}^n K_h(x, x_i) \\big( y_i - \\beta_0 - \\beta_1 x_i \\big)^2\\] 13.7 Local Polynomial Regression The following examples are local polynomial regressions, evaluated as different target points. We can easily extend the local linear model to inccorperate higher orders terms: \\[\\sum_{i=1}^n K_h(x, x_i) \\Big[ y_i - \\beta_0(x) - \\sum_{r=1}^d \\beta_j(x) x_i^r \\Big]^2\\] The followings are local quadratic fittings, which will further correct the bias. 13.8 R Implementations Some popular R functions implements the local polynomial regressions: loess, locfit, locploy, etc. These functions automatically calculate the fitted value for each target point (essentially all the observed points). This can be used in combination with ggplot2. The point-wise confidence intervals are also calculated. ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(col = &quot;red&quot;, method = &quot;loess&quot;, span = 0.5) ## `geom_smooth()` using formula = &#39;y ~ x&#39; A toy example that compares different bandwidth. Be careful that different methods may formulat the bandwidth parameter in different ways. # local polynomial fitting using locfit and locpoly library(KernSmooth) library(locfit) n &lt;- 100 x &lt;- runif(n,0,1) y &lt;- sin(2*pi*x)+rnorm(n,0,1) y = y[order(x)] x = sort(x) plot(x, y, pch = 19) points(x, sin(2*pi*x), lwd = 3, type = &quot;l&quot;, col = 1) lines(locpoly(x, y, bandwidth=0.15, degree=2), col=2, lwd = 3) lines(locfit(y~lp(x, nn = 0.3, h=0.05, deg=2)), col=4, lwd = 3) legend(&quot;topright&quot;, c(&quot;locpoly&quot;, &quot;locfit&quot;), col = c(2,4), lty = 1, cex = 1.5, lwd =2) "],["support-vector-machines.html", "Chapter 14 Support Vector Machines 14.1 Maximum-margin Classifier 14.2 Linearly Separable SVM 14.3 Linearly Non-separable SVM with Slack Variables 14.4 Example: SAheart Data 14.5 Nonlinear SVM via Kernel Trick 14.6 Example: mixture.example Data 14.7 SVM as a Penalized Model 14.8 Kernel and Feature Maps: Another Example", " Chapter 14 Support Vector Machines Support Vector Machine (SVM) is one of the most popular classification models. The original SVM was proposed by Vladimir Vapnik and Alexey Chervonenkis in 1963. Then two important improvements was developed in the 90’s: the soft margin version (Cortes and Vapnik 1995) and the nonlinear SVM using the kernel trick (Boser, Guyon, and Vapnik 1992). We will start with the hard margin version, and then introduce all other techniques. 14.1 Maximum-margin Classifier This is the original SVM proposed in 1963. It shares similarities with the perception algorithm, but in certain sense is a stable version. We observe the training data \\({\\cal D}_n = \\{\\mathbf{x}_i, y_i\\}_{i=1}^n\\), where we code \\(y_i\\) as a binary outcome from \\(\\{-1, 1\\}\\). The advantages of using this coding instead of \\(0/1\\) will be seen later. The goal is to find a linear classification rule \\(f(\\mathbf{x}) = \\beta_0 + \\mathbf{x}^\\text{T}\\boldsymbol{\\beta}\\) such that the classification rule is the sign of \\(f(\\mathbf{x})\\): \\[ \\hat{y} = \\begin{cases} +1, \\quad \\text{if} \\quad f(\\mathbf{x}) &gt; 0\\\\ -1, \\quad \\text{if} \\quad f(\\mathbf{x}) &lt; 0 \\end{cases} \\] Hence, a correct classification would satisfy \\(y_i f(\\mathbf{x}_i) &gt; 0\\). Let’s look at the following example of data from two classes. set.seed(1) n &lt;- 6 p &lt;- 2 # Generate positive and negative examples xneg &lt;- matrix(rnorm(n*p,mean=0,sd=1),n,p) xpos &lt;- matrix(rnorm(n*p,mean=3,sd=1),n,p) x &lt;- rbind(xpos,xneg) y &lt;- matrix(as.factor(c(rep(1,n),rep(-1,n)))) # plot plot(x,col=ifelse(y&gt;0,&quot;blue&quot;,&quot;red&quot;), pch = 19, cex = 1.2, lwd = 2, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;, cex.lab = 1.5) legend(&quot;bottomright&quot;, c(&quot;Positive&quot;, &quot;Negative&quot;),col=c(&quot;blue&quot;, &quot;red&quot;), pch=c(19, 19), text.col=c(&quot;blue&quot;, &quot;red&quot;), cex = 1.5) There are many linear lines that can perfectly separate the two classes. But which is better? The SVM defines this as the line that maximizes the margin, which can be seen in the following. We use the e1071 package to fit the SVM. There is a cost parameter \\(C\\), with default value 1. This parameter has a significant impact on non-separable problems. However, for this separable case, we should set this to be a very large value, meaning that the cost for having a wrong classification is very large. We also need to specify the linear kernel. library(e1071) svm.fit &lt;- svm(y ~ ., data = data.frame(x, y), type=&#39;C-classification&#39;, kernel=&#39;linear&#39;, scale=FALSE, cost = 10000) The following code can recover the fitted linear separation margin. Note here that the points on the margins are the ones with \\(\\alpha_i &gt; 0\\) (will be introduced later): coefs provides the \\(y_i \\alpha_i\\) for the support vectors SV are the \\(x_i\\) values correspond to the support vectors rho is negative \\(\\beta_0\\) b &lt;- t(svm.fit$coefs) %*% svm.fit$SV b0 &lt;- -svm.fit$rho # an alternative of b0 as the lecture note b0 &lt;- -(max(x[y == -1, ] %*% t(b)) + min(x[y == 1, ] %*% t(b)))/2 # plot on the data plot(x,col=ifelse(y&gt;0,&quot;blue&quot;,&quot;red&quot;), pch = 19, cex = 1.2, lwd = 2, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;, cex.lab = 1.5) legend(&quot;bottomleft&quot;, c(&quot;Positive&quot;,&quot;Negative&quot;),col=c(&quot;blue&quot;,&quot;red&quot;), pch=c(19, 19),text.col=c(&quot;blue&quot;,&quot;red&quot;), cex = 1.5) abline(a= -b0/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=1, lwd = 2) # mark the support vectors points(x[svm.fit$index, ], col=&quot;black&quot;, cex=3) # the two margin lines abline(a= (-b0-1)/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=3, lwd = 2) abline(a= (-b0+1)/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=3, lwd = 2) As we can see, the separation line is trying to have the maximum distance from both classes. This is why it is called the Maximum-margin Classifier. 14.2 Linearly Separable SVM In linearly SVM, \\(f(\\mathbf{x}) = \\beta_0 + \\mathbf{x}^\\text{T}\\boldsymbol{\\beta}\\). When \\(f(\\mathbf{x}) = 0\\), it corresponds to a hyperplane that separates the two classes: \\[\\{ \\mathbf{x}: \\beta_0 + \\mathbf{x}^\\text{T} \\boldsymbol \\beta = 0 \\}\\] Hence, for this separable case, all observations with \\(y_i = 1\\) are on one side \\(f(\\mathbf{x}) &gt; 0\\), and observations with \\(y_i = -1\\) are on the other side. First, let’s calculate the distance from any point \\(\\mathbf{x}\\) to this hyperplane. We can first find a point \\(\\mathbf{x}_0\\) on the hyperplane, such that \\(\\mathbf{x}_0^\\text{T}\\boldsymbol{\\beta}= - \\beta_0\\). By taking the difference between \\(\\mathbf{x}\\) and \\(\\mathbf{x}_0\\), and project this vector to the direction of \\(\\boldsymbol{\\beta}\\), we have that the distance from \\(\\mathbf{x}\\) to the hyperplane is the projection of \\(\\mathbf{x}- \\mathbf{x}_0\\) onto the normed vector \\(\\frac{\\boldsymbol{\\beta}}{\\lVert \\boldsymbol{\\beta}\\lVert}\\): \\[\\begin{align} &amp; \\left \\langle \\frac{\\boldsymbol{\\beta}}{\\lVert \\boldsymbol{\\beta}\\lVert}, \\mathbf{x}- \\mathbf{x}_0 \\right \\rangle \\\\ =&amp; \\frac{1}{\\lVert \\boldsymbol{\\beta}\\lVert} (\\mathbf{x}- \\mathbf{x}_0)^\\text{T}\\boldsymbol{\\beta}\\\\ =&amp; \\frac{1}{\\lVert \\boldsymbol{\\beta}\\lVert} (\\mathbf{x}^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) \\\\ =&amp; \\frac{1}{\\lVert \\boldsymbol{\\beta}\\lVert} f(\\mathbf{x}) \\\\ \\end{align}\\] Since the goal of SVM is to create the maximum margin, let’s denote this as \\(M\\). Then we want all observations to be lied on the correct side, with at least an margin \\(M\\). This means \\(y_i (\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) \\geq M\\). But the scale of \\(\\boldsymbol{\\beta}\\) is also playing a role in calculating the margin. Hence, we will use the normed version. Then, the linearly separable SVM is to solve this constrained optimization problem: \\[\\begin{align} \\underset{\\boldsymbol{\\beta}, \\beta_0}{\\text{max}} \\quad &amp; M \\\\ \\text{subject to} \\quad &amp; \\frac{1}{\\lVert \\boldsymbol{\\beta}\\lVert} y_i(\\mathbf{x}^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) \\geq M, \\,\\, i = 1, \\ldots, n. \\end{align}\\] Note that the scale of \\(\\boldsymbol{\\beta}\\) can be arbitrary, let’s set it as \\(\\lVert \\boldsymbol{\\beta}\\rVert = 1/M\\). The maximization becomes minimization, and its equivalent to minimizing \\(\\frac{1}{2} \\lVert \\boldsymbol{\\beta}\\rVert^2\\). Then we have the primal form of the SVM optimization problem. \\[\\begin{align} \\underset{\\boldsymbol{\\beta}, \\beta_0}{\\text{min}} \\quad &amp; \\frac{1}{2} \\lVert \\boldsymbol{\\beta}\\rVert^2 \\\\ \\text{subject to} \\quad &amp; y_i(\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) \\geq 1, \\,\\, i = 1, \\ldots, n. \\end{align}\\] 14.2.1 From Primal to Dual This is a general inequality constrained optimization problem. \\[\\begin{align} \\text{min} \\quad &amp; g(\\boldsymbol{\\theta}) \\\\ \\text{subject to} \\quad &amp; h_i(\\boldsymbol{\\theta}) \\leq 0, \\,\\, i = 1, \\ldots, n. \\end{align}\\] We can consider the corresponding Lagrangian (with all \\(\\alpha_i\\)’s positive): \\[{\\cal L}(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha}) = g(\\boldsymbol{\\theta}) + \\sum_{i = 1}^n \\alpha_i h_i(\\boldsymbol{\\theta})\\] Then there can be two ways to optimize this. If we maximize \\(\\alpha_i\\)’s first, for any fixed \\(\\boldsymbol{\\theta}\\), then for any \\(\\boldsymbol{\\theta}\\) that violates the constraint, i.e., \\(h_i(\\boldsymbol{\\theta}) &gt; 0\\) for some \\(i\\), we can always choose an extremely large \\(\\alpha_i\\) so that \\(\\cal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha})\\) is infinity. Hence the solution of this primal form must satisfy the constraint. \\[\\underset{\\boldsymbol{\\theta}}{\\min} \\underset{\\boldsymbol{\\alpha}\\succeq 0}{\\max} {\\cal L}(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha})\\] On the other hand, if we minimize \\(\\boldsymbol{\\theta}\\) first, then maximize for \\(\\boldsymbol{\\alpha}\\), we have the dual form: \\[\\underset{\\boldsymbol{\\alpha}\\succeq 0}{\\max} \\underset{\\boldsymbol{\\theta}}{\\min} {\\cal L}(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha})\\] In general, the two are not the same: \\[\\underbrace{\\underset{\\boldsymbol{\\alpha}\\succeq 0}{\\max} \\underset{\\boldsymbol{\\theta}}{\\min} {\\cal L}(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha})}_{\\text{duel}} \\leq \\underbrace{\\underset{\\boldsymbol{\\theta}}{\\min} \\underset{\\boldsymbol{\\alpha}\\succeq 0}{\\max} {\\cal L}(\\boldsymbol{\\theta}, \\boldsymbol{\\alpha})}_{\\text{primal}}\\] But a sufficient condition is that if both \\(g\\) and \\(h_i\\)’s are convex and also the constraints \\(h_i\\)’s are feasible. We will use this technique to solve the SVM problem. First, rewrite the problem as \\[\\begin{align} \\text{min} \\quad &amp; \\frac{1}{2} \\lVert \\boldsymbol{\\beta}\\rVert^2 \\\\ \\text{subject to} \\quad &amp; - \\{ y_i(\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) - 1\\} \\leq 0, i = 1, \\ldots, n. \\end{align}\\] Then the Lagrangian is \\[{\\cal L}(\\boldsymbol{\\beta}, \\beta_0, \\boldsymbol{\\alpha}) = \\frac{1}{2} \\lVert \\boldsymbol{\\beta}\\rVert^2 - \\sum_{i = 1}^n \\alpha_i \\big\\{ y_i(\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) - 1 \\big\\}\\] To solve this using the dual form, we first find the optimizer of \\(\\boldsymbol{\\beta}\\) and \\(\\beta_0\\). We take derivatives with respect to them: \\[\\begin{align} \\boldsymbol{\\beta}- \\sum_{i = 1}^n \\alpha_i y_i \\mathbf{x}_i^\\text{T}=&amp;~ 0 \\quad (\\nabla_\\boldsymbol{\\beta}{\\cal L}= 0 ) \\\\ \\sum_{i = 1}^n \\alpha_i y_i =&amp;~ 0 \\quad (\\nabla_{\\beta_0} {\\cal L}= 0 ) \\end{align}\\] Take these solution and plug them back into the Lagrangian, we have \\[{\\cal L}(\\boldsymbol{\\beta}, \\beta_0, \\boldsymbol{\\alpha}) = \\sum_{i = 1}^n \\alpha_i - \\frac{1}{2} \\sum_{i, j = 1}^n y_i y_j \\alpha_i\\alpha_j \\mathbf{x}_i^\\text{T}\\mathbf{x}_j\\] Hence, the dual optimization problem is \\[\\begin{align} \\underset{\\boldsymbol{\\alpha}}{\\max} \\quad &amp; \\sum_{i = 1}^n \\alpha_i - \\frac{1}{2} \\sum_{i, j = 1}^n y_i y_j \\alpha_i\\alpha_j \\mathbf{x}_i^\\text{T}\\mathbf{x}_j \\nonumber \\\\ \\text{subject to} \\quad &amp; \\alpha_i \\geq 0, \\,\\, i = 1, \\ldots, n. \\nonumber \\\\ &amp; \\sum_{i = 1}^n \\alpha_i y_i = 0 \\end{align}\\] Compared with the original primal form, this version has a trivial feasible solution with all \\(\\alpha_i\\)’s being 0. One can start from this solution to search for the optimizer while maintaining within the contained region. However, the primal form is difficult since there is no apparent way to satisfy the constraint. After solving the dual form, we have all the \\(\\alpha_i\\) values. The ones with \\(\\alpha_i &gt; 0\\) are called the support vectors. Based on our previous analysis, \\(\\widehat{\\boldsymbol{\\beta}} = \\sum_{i = 1}^n \\alpha_i y_i x_i^T\\), and we can also obtain \\(\\beta_0\\) by calculating the midpoint of two “closest” support vectors to the separating hyperplane: \\[\\widehat{\\beta}_0 = - \\,\\, \\frac{\\max_{i: y_i = -1} \\mathbf{x}_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} + \\min_{i: y_i = 1} \\mathbf{x}_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} }{2}\\] And the decision is \\(\\text{sign}(\\mathbf{x}_i^\\text{T}\\widehat{\\boldsymbol{\\beta}} + \\widehat{\\beta}_0)\\). An example has been demonstrated previously with the e1071 package. 14.3 Linearly Non-separable SVM with Slack Variables When we cannot have a perfect separation of the two classes, the original SVM cannot find a solution. Hence, a slack was introduce to incorporate such observations: \\[y_i (\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) \\geq (1 - \\xi_i)\\] for a positive \\(\\xi\\). Note that when \\(\\xi = 0\\), the observation is lying at the correct side, with enough margin. When \\(1 &gt; \\xi &gt; 0\\), the observation is lying at the correct side, but the margin is not sufficiently large. When \\(\\xi &gt; 1\\), the observation is lying on the wrong side of the separation hyperplane. This new optimization problem can be formulated as \\[\\begin{align} \\text{min} \\quad &amp; \\frac{1}{2}\\lVert \\boldsymbol{\\beta}\\rVert^2 + C \\sum_{i=1}^n \\xi_i \\\\ \\text{subject to} \\quad &amp; y_i (\\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) \\geq (1 - \\xi_i), \\,\\, i = 1, \\ldots, n, \\\\ \\text{and} \\quad &amp; \\xi_i \\geq 0, \\,\\, i = 1, \\ldots, n, \\end{align}\\] where \\(C\\) is a tuning parameter that controls the emphasis on the slack variable. Large \\(C\\) will be less tolerable on having positive slacks. We can again write the Lagrangian primal \\({\\cal L}(\\boldsymbol{\\beta}, \\beta_0, \\boldsymbol{\\alpha}, \\boldsymbol{\\xi})\\) as \\[\\frac{1}{2} \\lVert \\boldsymbol{\\beta}\\rVert^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i = 1}^n \\alpha_i \\big\\{ y_i(x_i^\\text{T}\\boldsymbol{\\beta}+ \\beta_0) - (1 - \\xi_i) \\big\\} - \\sum_{i = 1}^n \\gamma_i \\xi_i,\\] where \\(\\alpha_i\\)’s and \\(\\gamma_i\\)’s are all positive. We can similarly obtain the solution corresponding to \\(\\boldsymbol{\\beta}\\), \\(\\beta_0\\) and \\(\\boldsymbol{\\xi}\\): \\[\\begin{align} \\boldsymbol{\\beta}- \\sum_{i = 1}^n \\alpha_i y_i x_i =&amp;~ 0 \\quad (\\nabla_\\boldsymbol{\\beta}{\\cal L}= 0 ) \\\\ \\sum_{i = 1}^n \\alpha_i y_i =&amp;~ 0 \\quad (\\nabla_{\\beta_0} {\\cal L}= 0 ) \\\\ C - \\alpha_i - \\gamma_i =&amp;~ 0 \\quad (\\nabla_{\\xi_i} {\\cal L}= 0 ) \\end{align}\\] Substituting them back into the Lagrangian, we have the dual form: \\[\\begin{align} \\underset{\\boldsymbol{\\alpha}}{\\max} \\quad &amp; \\sum_{i = 1}^n \\alpha_i - \\frac{1}{2} \\sum_{i, j = 1}^n y_i y_j \\alpha_i\\alpha_j \\color{OrangeRed}{\\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle} \\\\ \\text{subject to} \\quad &amp; 0 \\leq \\alpha_i \\leq C, \\,\\, i = 1, \\ldots, n, \\\\ \\text{and} \\quad &amp; \\sum_{i = 1}^n \\alpha_i y_i = 0. \\end{align}\\] Here, the inner product \\(\\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle\\) is nothing but \\(\\mathbf{x}_i^\\text{T}\\mathbf{x}_j\\). The observations with \\(0 &lt; \\alpha_i &lt; C\\) are the that lie on the margin. Hence, we can obtain these observations and perform the same calculations as before to obtain \\(\\widehat{\\beta}_0\\). The following code generates some data for this situation and fit SVM. We use the default \\(C = 1\\). set.seed(70) n &lt;- 10 # number of data points for each class p &lt;- 2 # dimension # Generate the positive and negative examples xneg &lt;- matrix(rnorm(n*p,mean=0,sd=1),n,p) xpos &lt;- matrix(rnorm(n*p,mean=1.5,sd=1),n,p) x &lt;- rbind(xpos,xneg) y &lt;- matrix(as.factor(c(rep(1,n),rep(-1,n)))) # Visualize the data plot(x,col=ifelse(y&gt;0,&quot;blue&quot;,&quot;red&quot;), pch = 19, cex = 1.2, lwd = 2, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;, cex.lab = 1.5) legend(&quot;topright&quot;, c(&quot;Positive&quot;,&quot;Negative&quot;),col=c(&quot;blue&quot;,&quot;red&quot;), pch=c(19, 19),text.col=c(&quot;blue&quot;,&quot;red&quot;), cex = 1.5) svm.fit &lt;- svm(y ~ ., data = data.frame(x, y), type=&#39;C-classification&#39;, kernel=&#39;linear&#39;,scale=FALSE, cost = 1) b &lt;- t(svm.fit$coefs) %*% svm.fit$SV b0 &lt;- -svm.fit$rho points(x[svm.fit$index, ], col=&quot;black&quot;, cex=3) abline(a= -b0/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=1, lwd = 2) abline(a= (-b0-1)/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=3, lwd = 2) abline(a= (-b0+1)/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=3, lwd = 2) If we instead use a smaller \\(C\\): # Visualize the data plot(x,col=ifelse(y&gt;0,&quot;blue&quot;,&quot;red&quot;), pch = 19, cex = 1.2, lwd = 2, xlab = &quot;X1&quot;, ylab = &quot;X2&quot;, cex.lab = 1.5) legend(&quot;topright&quot;, c(&quot;Positive&quot;,&quot;Negative&quot;),col=c(&quot;blue&quot;,&quot;red&quot;), pch=c(19, 19),text.col=c(&quot;blue&quot;,&quot;red&quot;), cex = 1.5) # fit SVM with C = 10 svm.fit &lt;- svm(y ~ ., data = data.frame(x, y), type=&#39;C-classification&#39;, kernel=&#39;linear&#39;,scale=FALSE, cost = 0.1) b &lt;- t(svm.fit$coefs) %*% svm.fit$SV b0 &lt;- -svm.fit$rho points(x[svm.fit$index, ], col=&quot;black&quot;, cex=3) abline(a= -b0/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=1, lwd = 2) abline(a= (-b0-1)/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=3, lwd = 2) abline(a= (-b0+1)/b[1,2], b=-b[1,1]/b[1,2], col=&quot;black&quot;, lty=3, lwd = 2) 14.4 Example: SAheart Data If you want to use the 1071e package and perform cross-validation, you could consider using the caret package. Make sure that you specify method = \"svmLinear2\". The following code is using the SAheart as an example. library(ElemStatLearn) data(SAheart) library(caret) cost.grid = expand.grid(cost = seq(0.01, 2, length = 20)) train_control = trainControl(method=&quot;repeatedcv&quot;, number=10, repeats=3) svm2 &lt;- train(as.factor(chd) ~., data = SAheart, method = &quot;svmLinear2&quot;, trControl = train_control, tuneGrid = cost.grid) # see the fitted model svm2 ## Support Vector Machines with Linear Kernel ## ## 462 samples ## 9 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 416, 416, 416, 415, 416, 416, ... ## Resampling results across tuning parameters: ## ## cost Accuracy Kappa ## 0.0100000 0.7142923 0.2844994 ## 0.1147368 0.7200123 0.3520308 ## 0.2194737 0.7164354 0.3454492 ## 0.3242105 0.7171600 0.3467866 ## 0.4289474 0.7164354 0.3453015 ## 0.5336842 0.7164354 0.3450704 ## 0.6384211 0.7157108 0.3438517 ## 0.7431579 0.7171600 0.3472755 ## 0.8478947 0.7157108 0.3437850 ## 0.9526316 0.7157108 0.3437850 ## 1.0573684 0.7171600 0.3479914 ## 1.1621053 0.7164354 0.3459484 ## 1.2668421 0.7164354 0.3459484 ## 1.3715789 0.7178847 0.3500130 ## 1.4763158 0.7171600 0.3479914 ## 1.5810526 0.7178847 0.3500130 ## 1.6857895 0.7171600 0.3479914 ## 1.7905263 0.7171600 0.3479914 ## 1.8952632 0.7171600 0.3479914 ## 2.0000000 0.7164354 0.3459484 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cost = 0.1147368. Note that when you fit the model, there are a few things you could consider: You can consider centering and scaling the covariates. This can be done during pre-processing. Or you may specify preProcess = c(\"center\", \"scale\") in the train() function. You may want to start with a wider range of cost values, then narrow down to a smaller range, since SVM can be quite sensitive to tuning in some cases. There are many other SVM libraries, such as kernlab. This can be specified by using method = \"svmLinear\". However, kernlab uses C as the parameter name for cost. We will show an example later. 14.5 Nonlinear SVM via Kernel Trick The essential idea of kernel trick can be summarized as using the kernel function of two observations \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\) to replace the inner product between some feature mapping of the two covariate vectors. In other words, if we want to create some nonlinear features of \\(\\mathbf{x}\\), such as \\(x_1^2\\), \\(\\exp(x_2)\\), \\(\\sqrt{x_3}\\), etc., we may in general write them as \\[\\Phi : {\\cal X}\\rightarrow {\\cal F}, \\,\\,\\, \\Phi(\\mathbf{x}) = (\\phi_1(\\mathbf{x}), \\phi_2(\\mathbf{x}), \\ldots ),\\] where \\({\\cal F}\\) has either finite or infinite dimensions. Then, we can still treat this as a linear SVM by constructing the decision rule as \\[f(x) = \\langle \\Phi(\\mathbf{x}), \\boldsymbol{\\beta}\\rangle = \\Phi(\\mathbf{x})^\\text{T}\\boldsymbol{\\beta}.\\] This is why we used the \\(\\langle \\cdot, \\cdot\\rangle\\) operator in the previous example. Now, the kernel trick is essentially skipping the explicit calculation of \\(\\Phi(\\mathbf{x})\\) by utilizing the property that \\[K(\\mathbf{x}, \\mathbf{z}) = \\langle \\Phi(\\mathbf{x}), \\Phi(\\mathbf{z}) \\rangle\\] for some kernel function \\(K(\\mathbf{x}, \\mathbf{z})\\). Since \\(\\langle \\Phi(\\mathbf{x}), \\Phi(\\mathbf{z}) \\rangle\\) is all we need in the dual form, we can simply replace it by \\(K(\\mathbf{x}, \\mathbf{z})\\), which gives the kernel form: \\[\\begin{align} \\underset{\\boldsymbol{\\alpha}}{\\max} \\quad &amp; \\sum_{i = 1}^n \\alpha_i - \\frac{1}{2} \\sum_{i, j = 1}^n y_i y_j \\alpha_i\\alpha_j \\color{OrangeRed}{K(\\mathbf{x}_i, \\mathbf{x}_j)} \\\\ \\text{subject to} \\quad &amp; 0 \\leq \\alpha_i \\leq C, \\,\\, i = 1, \\ldots, n, \\\\ \\text{and} \\quad &amp; \\sum_{i = 1}^n \\alpha_i y_i = 0. \\end{align}\\] One most apparent advantage of doing this is to save computational cost. This maybe understood using the following example: Consider kernel function \\(K(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^\\text{T}\\mathbf{z})^2\\) Consider \\(\\Phi(\\mathbf{x})\\) being the basis expansion that contains all second order interactions: \\(x_k x_l\\) for \\(1 \\leq k, l \\leq p\\) We can show that the two gives equivalent results, however, the kernel version is much faster. \\(K(\\mathbf{x}, \\mathbf{z})\\) takes \\(p+1\\) operations, while \\(\\langle \\Phi(\\mathbf{x}_i), \\Phi(\\mathbf{x}_j) \\rangle\\) requires \\(3p^2\\). \\[\\begin{align} K(\\mathbf{x}, \\mathbf{z}) &amp;=~ \\left(\\sum_{k=1}^p x_k z_k\\right) \\left(\\sum_{l=1}^p x_l z_l\\right) \\\\ &amp;=~ \\sum_{k=1}^p \\sum_{l=1}^p x_k z_k x_l z_l \\\\ &amp;=~ \\sum_{k, l=1}^p (x_k x_l) (z_k z_l) \\\\ &amp;=~ \\langle \\Phi(\\mathbf{x}), \\Phi(\\mathbf{z}) \\rangle \\end{align}\\] Formally, this property is guaranteed by the Mercer’s theorem that states: The kernel matrix \\(K\\) is positive semi-definite if and only if the function \\(K(x_i ,x_j)\\) is equivalent to some inner product \\(\\langle \\Phi(\\mathbf{x}), \\Phi(\\mathbf{z}) \\rangle\\). Besides making the calculation of nonlinear functions easier, using the kernel trick also implies that if we use a proper kernel function, then it defines a space of functions \\({\\cal H}\\) (reproducing kernel Hilbert space, RKHS) that can be represented in the form of \\(f(x) = \\sum_i \\alpha_i K(x, x_i)\\) for some \\(x_i\\) in \\({\\cal X}\\) (see the Moore–Aronszajn theorem) with a proper definition of inner product. However, this space is of infinite dimension, noticing that \\(i\\) goes from 1 to infinity. However, as long as we search for the solution within \\({\\cal H}\\), and also apply a proper penalty of the estimated function \\(\\widehat{f}(\\mathbf{x})\\), then our computational job will reduce to solving the \\(\\alpha_i\\)’s that corresponds to the observed \\(n\\) data points, meaning that we only need to solve the solution within a finite space. This is guaranteed by the Representer theorem. There are numerous articles on the RKHS. Hence, we will not focus on introducing this technique. However, we will later on use this property in the penalized formulation of SVM. 14.6 Example: mixture.example Data We use the mixture.example data in the ElemStatLearn package. In addition, we use a different package kernlab. The red dotted line indicates the true decision boundary. library(ElemStatLearn) data(mixture.example) # redefine data px1 = mixture.example$px1 px2 = mixture.example$px2 x = mixture.example$x y = mixture.example$y # plot the data and true decision boundary prob &lt;- mixture.example$prob prob.bayes &lt;- matrix(prob, length(px1), length(px2)) contour(px1, px2, prob.bayes, levels=0.5, lty=2, labels=&quot;&quot;, xlab=&quot;x1&quot;,ylab=&quot;x2&quot;, main=&quot;SVM with linear kernal&quot;, col = &quot;red&quot;, lwd = 2) points(x, col=ifelse(y==1, &quot;darkorange&quot;, &quot;deepskyblue&quot;), pch = 19) # train linear SVM using the kernlab package library(kernlab) cost = 10 svm.fit &lt;- ksvm(x, y, type=&quot;C-svc&quot;, kernel=&#39;vanilladot&#39;, C=cost) ## Setting default kernel parameters # plot the SVM decision boundary # Extract the indices of the support vectors on the margin: sv.alpha&lt;-alpha(svm.fit)[[1]][which(alpha(svm.fit)[[1]]&lt;cost)] sv.index&lt;-alphaindex(svm.fit)[[1]][which(alpha(svm.fit)[[1]]&lt;cost)] sv.matrix&lt;-x[sv.index,] points(sv.matrix, pch=16, col=ifelse(y[sv.index] == 1, &quot;darkorange&quot;, &quot;deepskyblue&quot;), cex=1.5) # Plot the hyperplane and the margins: w &lt;- t(cbind(coef(svm.fit)[[1]])) %*% xmatrix(svm.fit)[[1]] b &lt;- - b(svm.fit) abline(a= -b/w[1,2], b=-w[1,1]/w[1,2], col=&quot;black&quot;, lty=1, lwd = 2) abline(a= (-b-1)/w[1,2], b=-w[1,1]/w[1,2], col=&quot;black&quot;, lty=3, lwd = 2) abline(a= (-b+1)/w[1,2], b=-w[1,1]/w[1,2], col=&quot;black&quot;, lty=3, lwd = 2) Let’s also try a nonlinear SVM, using the radial kernel. # fit SVM with radial kernel, with cost = 5 dat = data.frame(y = factor(y), x) fit = svm(y ~ ., data = dat, scale = FALSE, kernel = &quot;radial&quot;, cost = 5) # extract the prediction xgrid = expand.grid(X1 = px1, X2 = px2) func = predict(fit, xgrid, decision.values = TRUE) func = attributes(func)$decision # visualize the decision rule ygrid = predict(fit, xgrid) plot(xgrid, col = ifelse(ygrid == 1, &quot;bisque&quot;, &quot;cadetblue1&quot;), pch = 20, cex = 0.2, main=&quot;SVM with radial kernal&quot;) points(x, col=ifelse(y==1, &quot;darkorange&quot;, &quot;deepskyblue&quot;), pch = 19) # our estimated function value, cut at 0 contour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE, lwd = 2) # the true probability, cut at 0.5 contour(px1, px2, matrix(prob, 69, 99), level = 0.5, add = TRUE, col = &quot;red&quot;, lty=2, lwd = 2) You may also consider some other popular kernels. The following ones are implemented in the e1071 package, with additional tuning parameters \\(\\text{coef}_0\\) and \\(\\gamma\\). Linear: \\(K(\\mathbf{x}, \\mathbf{z}) = \\mathbf{x}^\\text{T}\\mathbf{z}\\) \\(d\\)th degree polynomial: \\(K(\\mathbf{x}, \\mathbf{z}) = (\\text{coef}_0 + \\gamma \\mathbf{x}^\\text{T}\\mathbf{z})^d\\) Radial basis: \\(K(\\mathbf{x}, \\mathbf{z}) = \\exp(- \\gamma \\lVert \\mathbf{x}- \\mathbf{z}\\lVert^2)\\) Sigmoid: \\(\\tanh(\\gamma \\mathbf{x}^\\text{T}\\mathbf{z}+ \\text{coef}_0)\\) Cross-validation can also be doing using the caret package. To specify the kernel, one must correctly specify the method parameter in the train() function. For this example, we use the method = \"svmRadial\" that uses the kernlab package to fit the model. For this choice, you need to tune just sigma and C (cost). More details are refereed to the caret documentation. svm.radial &lt;- train(y ~ ., data = dat, method = &quot;svmRadial&quot;, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid(C = c(0.01, 0.1, 0.5, 1), sigma = c(1, 2, 3)), trControl = trainControl(method = &quot;cv&quot;, number = 5)) svm.radial ## Support Vector Machines with Radial Basis Function Kernel ## ## 200 samples ## 2 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## Pre-processing: centered (2), scaled (2) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 160, 160, 160, 160, 160 ## Resampling results across tuning parameters: ## ## C sigma Accuracy Kappa ## 0.01 1 0.715 0.43 ## 0.01 2 0.760 0.52 ## 0.01 3 0.770 0.54 ## 0.10 1 0.720 0.44 ## 0.10 2 0.790 0.58 ## 0.10 3 0.800 0.60 ## 0.50 1 0.795 0.59 ## 0.50 2 0.815 0.63 ## 0.50 3 0.830 0.66 ## 1.00 1 0.795 0.59 ## 1.00 2 0.825 0.65 ## 1.00 3 0.835 0.67 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 3 and C = 1. 14.7 SVM as a Penalized Model Recall that in SVM, we need \\(y_i f(\\mathbf{x}_i)\\) to be at least \\(1 - \\xi_i\\), this implies that we would prefer \\(1 - y_i f(\\mathbf{x}_i)\\) to be negative or 0. And observation with \\(1 - y_i f(\\mathbf{x}_i)\\) should be penalized. Hence, recall that the objective function of dual form in SVM is \\(\\frac{1}{2}\\lVert \\boldsymbol{\\beta}\\rVert^2 + C \\sum_{i=1}^n \\xi_i\\), we may rewrite this as a new version: \\[\\min \\,\\, \\sum_{i=1}^n \\big[ 1 - y_i f(\\mathbf{x}_i) \\big]_{+} \\, +\\, \\lambda \\lVert \\boldsymbol{\\beta}\\rVert^2.\\] Here, we converted \\(1/(2C)\\) to \\(\\lambda\\). And this resembles a familiar form of “Loss \\(+\\) Penalty”, where the slack variables becomes the loss and the norm of \\(\\boldsymbol{\\beta}\\) is the penalty. This particular loss function is called the Hinge loss, with \\[L(y, f(\\mathbf{x})) = [1 - yf(\\mathbf{x})]_+ = \\max(0, 1 - yf(\\mathbf{x}))\\] However, the Hinge loss is not differentiable. There are some other loss functions that can be used as substitute: Logistic loss: \\[L(y, f(\\mathbf{x})) = \\log_2( 1 + e^{-y f(\\mathbf{x})})\\] Modified Huber Loss: \\[L(y, f(\\mathbf{x})) = \\begin{cases} \\max(0, 1 - yf(\\mathbf{x}))^2 &amp; \\text{for} \\quad yf(\\mathbf{x}) \\geq -1 \\\\ -4 yf(\\mathbf{x}) &amp; \\text{otherwise} \\\\ \\end{cases}\\] Here is a visualization of several different loss functions. t = seq(-2, 3, 0.01) # different loss functions hinge = pmax(0, 1 - t) zeroone = (t &lt;= 0) logistic = log2(1 + exp(-t)) modifiedhuber = ifelse(t &gt;= -1, (pmax(0, 1 - t))^2, -4*t) # plot plot(t, zeroone, type = &quot;l&quot;, lwd = 2, ylim = c(0, 4), main = &quot;Loss Functions&quot;) points(t, hinge, type = &quot;l&quot;, lwd = 2, col = &quot;red&quot;, ) points(t, logistic, type = &quot;l&quot;, lty = 2, col = &quot;darkorange&quot;, lwd = 2) points(t, modifiedhuber, type = &quot;l&quot;, lty = 2, col = &quot;deepskyblue&quot;, lwd = 2) legend(&quot;topright&quot;, c(&quot;Zero-one&quot;, &quot;Hinge&quot;, &quot;Logistic&quot;, &quot;Modified Huber&quot;), col = c(1, 2, &quot;darkorange&quot;, &quot;deepskyblue&quot;), lty = c(1, 1, 2, 2), lwd = 2, cex = 1.5) For linear decision rules, with \\(f(\\mathbf{x}) = \\beta_0 + \\mathbf{x}^\\text{T}\\boldsymbol{\\beta}\\), this should be trivial to solve. However, we also want to consider nonlinear decision functions. But the above form does not contain a kernel function to use the kernel trick. The Representer Theorem (Kimeldorf and Wahba 1970) can help us in this case. This theorem was originally developed for in the setting of Chebyshev splines, but later on generalized. The theorem ensures that if we solve the function \\(f\\) with regularization with respect to the norm in the RKHS induced from a kernel function \\(K\\), then the solution must admits a finite representation of the form (although the space \\({\\cal H}\\) we search for the solution is infinite): \\[\\widehat{f}(\\mathbf{x}) = \\sum_{i = 1}^n \\beta_i K(\\mathbf{x}, \\mathbf{x}_i).\\] This suggests that the optimization problem becomes \\[\\sum_{i=1}^n L(y_i, \\mathbf{K}_i^\\text{T}\\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}^\\text{T}\\mathbf{K}\\boldsymbol{\\beta},\\] where \\(\\mathbf{K}_{n \\times n}\\) is the kernel matrix with \\(\\mathbf{K}_{ij} = K(x_i, x_j)\\), and \\(\\mathbf{K}_i\\) is the \\(i\\) the column of \\(\\mathbf{K}\\). This is an unconstrained optimization problem that can be solved using gradient decent if \\(L\\) is differentiable. More details will be presented in the next Chapter. 14.8 Kernel and Feature Maps: Another Example We give another example about the equivalence of kernel and the inner product of feature maps, which is ensured by the Mercer’s Theorem (Mercer 1909). Consider the Gaussian kernel \\(e^{-\\gamma \\lVert \\mathbf{x}- \\mathbf{z}\\rVert}\\). We can write, using Tayler expansion, \\[\\begin{align} &amp;e^{\\gamma \\lVert \\mathbf{x}- \\mathbf{z}\\rVert} \\nonumber \\\\ =&amp; e^{-\\gamma \\lVert \\mathbf{x}\\rVert + 2 \\gamma \\mathbf{x}^\\text{T}\\mathbf{z}- \\gamma \\lVert \\mathbf{z}\\rVert} \\nonumber \\\\ =&amp; e^{-\\gamma \\lVert \\mathbf{x}\\rVert - \\gamma \\lVert \\mathbf{z}\\rVert} \\bigg[ 1 + \\frac{2 \\gamma \\mathbf{x}^\\text{T}\\mathbf{z}}{1!} + \\frac{(2 \\gamma \\mathbf{x}^\\text{T}\\mathbf{z})^2}{2!} + \\frac{(2 \\gamma \\mathbf{x}^\\text{T}\\mathbf{z})^3}{3!} + \\cdots \\bigg] \\end{align}\\] Note that \\(\\mathbf{x}^\\text{T}\\mathbf{z}\\) is the inner product of all first order feature maps. We also showed previously \\((\\mathbf{x}^\\text{T}\\mathbf{z})^2\\) is equivalent to the inner product of all second order feature maps (\\(\\Phi_2(\\mathbf{x})\\)), and \\((\\mathbf{x}^\\text{T}\\mathbf{z})^3\\) would be equivalent to the third order version (\\(\\Phi_3(\\mathbf{x})\\)), etc.. Hence, the previous equation can be written as the inner product of feature maps in the form of \\[e^{-\\gamma \\lVert \\mathbf{x}\\rVert} \\bigg[ 1, \\sqrt{\\frac{2\\gamma}{1!}} \\mathbf{x}^\\text{T}, \\sqrt{\\frac{(2\\gamma)^2}{2!}} \\Phi_2^\\text{T}(\\mathbf{x}), \\sqrt{\\frac{(2\\gamma)^3}{3!}} \\Phi_3^\\text{T}(\\mathbf{x}), \\cdots \\bigg]\\] This shows the Gaussian kernel is corresponding to all polynomials with a scaling factor of \\(e^{-\\gamma \\lVert \\mathbf{x}\\rVert}\\) Reference Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. “A Training Algorithm for Optimal Margin Classifiers.” In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144–52. Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” Machine Learning 20 (3): 273–97. Kimeldorf, George S, and Grace Wahba. 1970. “A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines.” The Annals of Mathematical Statistics 41 (2): 495–502. Mercer, James. 1909. “Xvi. Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 209 (441-458): 415–46. "],["reproducing-kernel-hilbert-space.html", "Chapter 15 Reproducing Kernel Hilbert Space 15.1 Constructing the RKHS 15.2 Properties of RKHS 15.3 The Representer Theorem", " Chapter 15 Reproducing Kernel Hilbert Space In the previous chapter of SVM, we gave an example to show that instead of using the inner product \\(\\langle \\Phi(\\mathbf{x}), \\Phi(\\mathbf{z}) \\rangle\\) between the feature maps of \\(\\mathbf{x}\\) and \\(\\mathbf{z}\\), we can instead use the kernel trick \\(K(\\mathbf{x}, \\mathbf{z})\\) to perform the exact same calculation. And also, in the penalized kernel version, we mentioned that the decision rule can be expressed in the finite sample form of \\(\\sum_{i = 1}^n \\beta_i K(\\cdot, \\mathbf{x}_i)\\) by the Representer Theorem. All of these are based on a fundamental tool of the Reproducing Kernel Hilbert Space and we will provide some basic knowledge of it. We will also prove the Representer Theorem (Kimeldorf and Wahba 1970), which is very similar to the proof of smoothing spline. 15.1 Constructing the RKHS Recall that in the smoothing spline example, we wanted to fit a regression model by solving for a function \\(f\\) in a quite complicated space, the second order Sobolev space. We could not exhaust all the candidates in this space because that would be computationally untraceable. However, the results there shows that the solution has a finite representation. In general, when we solve a regression problem using a Loss \\(+\\) Penalty form, we will also enjoy that property if we search the (regression or decision) function \\(f\\) within a RKHS. So let’s first define what this space look like. We start with the feature space \\(\\cal X\\) of \\(X\\), where \\(X\\) is just the \\(p\\) dimensional feature we often deal with. Let’s say we have a sample \\(x_1\\), then if we have a kernel function \\(k(\\cdot, \\cdot)\\), we can construct a new function called \\(K(x_1, \\cdot)\\). Keep in mind that \\(K(x_1, \\cdot)\\) is a function with argument \\(\\cdot\\) and parameter \\(x_1\\) in this case. Similarly, we can do another sample, say \\(x_2\\) and generate a function based on that sample, called \\(K(x_1, \\cdot)\\). The following plot shows three of such functions, using red, orange and blue lines, receptively. Since we can have many samples from \\(\\cal X\\), we will also have infinite such functions like \\(K(x, \\cdot)\\), and also the linear combinations of them would also be interesting to us. Let’s consider a space \\({\\cal G}\\) of all such functions \\[{\\cal G} = \\left\\{\\sum_{i}^n \\alpha_i K(x_i, \\cdot) \\mid \\alpha \\in \\mathbb{R}, n \\in \\mathbb{N}, x_i \\in {\\cal X} \\right\\} \\] The black curve in the previous plot is an example of such linear combinations. We can see that the functions within \\({\\cal G}\\) start to become more and more flexible as we consider all the linear combinations. And as one final step, we will consider the completion of this space, which leads to the RKHS. \\[\\cal H = \\bar{\\cal G}.\\] Completion here means that \\(\\cal H\\) will contain the limits of all Cauchy sequences of such functions in \\(\\cal G\\). 15.2 Properties of RKHS This space \\(\\cal H\\) enjoys several important and useful properties. First, by the Riesz representation theorem, we know that \\(\\cal H\\) is a Hilbert space with the reproducing property. For a (real valued) Hilbert space, it must satisfy symmetric: \\(\\langle K_x, K_z \\rangle = \\langle K_z, K_x \\rangle\\) linear: \\(\\langle a K_{x_1} + b K_{x_2}, K_z \\rangle = a \\langle K_{x_1}, K_z \\rangle + b \\langle K_{x_2}, K_z \\rangle\\) positive definite: \\(\\langle K_x, K_x \\rangle \\geq 0\\) and \\(\\langle K_x, K_x \\rangle = 0\\) iff \\(K_x = 0\\) Also, the reproducing property means that when we evaluate a function \\(f \\in \\cal H\\) at a point \\(x\\), it is the same as calculating the inner product between \\(f\\) and \\(K_x\\). Formally, \\[f(x) = \\langle f, K_x \\rangle_{\\cal H}\\] Now, we could simply take \\(f = K_z\\), that means, evaluating \\(K_z(x)\\) is \\[K_z(x) = \\langle K_z, K_x \\rangle_{\\cal H}\\] Note that \\(K_z(x) = K(z, x)\\), this implies that the inner product in \\(\\cal H\\) is done by the kernel: \\[\\langle K_z, K_x \\rangle_{\\cal H} = K(z, x)\\] For example, if we have \\(f(\\cdot) = \\sum_i \\alpha_i K(x_i, \\cdot)\\), then evaluating \\(f\\) at \\(x\\) is \\[\\begin{align} f(x) =&amp; \\, \\langle f, K(x, \\cdot) \\rangle_{\\cal H} \\nonumber \\\\ =&amp; \\, \\left\\langle \\sum_i \\alpha_i K(x_i, \\cdot), K(x, \\cdot) \\right\\rangle_{\\cal H} \\nonumber \\\\ =&amp; \\, \\sum_i \\alpha_i \\left\\langle K(x_i, \\cdot), K(x, \\cdot) \\right\\rangle_{\\cal H} \\nonumber \\\\ =&amp; \\, \\sum_i \\alpha_i K(x_i, x) \\end{align}\\] The Moore–Aronszajn theorem (Aronszajn 1950) ensures that a positive definite kernel \\(K(\\cdot, \\cdot)\\) on \\(\\cal X\\) would uniquely define such a RKHS, where \\(K(\\cdot, \\cdot)\\) itself is the reproducing kernel. Hence, all we need is the original \\(\\cal X\\) and a kernel function. Then the RKHS can be defined as we stated previously, with all the nice properties. Besides these, another results by Mercer interprets kernels as feature maps, which we have already see in the SVM chapter that \\(K(x, z)= \\langle \\Phi(x), \\Phi(z) \\rangle\\). Overall, we set some relationships among these three quantities in their respective spaces: original features \\(x\\) feature maps \\(\\Phi(x)\\) functions \\(K(x, \\cdot)\\) 15.3 The Representer Theorem \\(\\cal H\\) is still a very large space of functions. And it is not clear if we want to find \\(f\\) in \\(\\cal H\\) for our optimization problem, how do we computationally complete that task. It is unlikely that we can exhaust all such functions. Well, luckily, we don’t need to. This is ensured by the Representer Theorem, which states that only a finite sample presentation is needed. Theorem 15.1 (Representer Theorem) If we are given a set of data \\(\\{x_i, y_i\\}_{i=1}^n\\), and we search for the best solution in \\({\\cal H}\\) of the optimization problem \\[\\widehat f = \\underset{f \\in \\cal H}{\\arg\\min} \\,\\, {\\cal L}(\\{y_i, f(x_i)\\}_{i=1}^n) + p(\\| f \\|_{{\\cal H}}^2 ),\\] where \\({\\cal L}\\) is the loss function, \\(p\\) is a monotone penalty function, and \\({\\cal H}\\) is the RKHS with kernel \\(K\\). Then the solution must take the form \\[\\widehat f = \\sum_{i=1}^n w_i K(\\cdot, x_i)\\] The proof is quite simple. The logic is the same as the smoothing spline proof. Proof. We can first use the kernel \\(K\\) associated with \\(\\cal H\\) to define a set of functions \\[K(\\cdot, x_i), \\, K(\\cdot, x_2), \\, \\cdots, \\, K(\\cdot, x_n)\\] Then, suppose the solution is some function \\(f \\in {\\cal H}\\), we could find its projection on the space spaned by these functions. This means that we could write \\(f\\) as \\[f(\\cdot) = \\sum_{i=1}^n \\alpha_i K(\\cdot, x_i) + h(\\cdot)\\] for some \\(\\alpha_i\\) and \\(h(\\cdot)\\). Also, since \\(h(\\cdot)\\) is in the orthogonal space of all such \\(K(\\cdot, x_i)\\), we have, by the reproducing property, \\[ h(x_i) = \\langle K(x_i, \\cdot), h(\\cdot) \\rangle = 0\\] for all \\(i\\). You may recall our proof in the smoothing spline for the same construction of \\(h\\) that has \\(h(x_i) = 0\\) for all \\(i\\). By the reproducing property, we have, for any observations in the training data, \\[\\begin{align} f(x_j) =&amp; \\langle f(\\cdot), K(\\cdot, x_j) \\rangle \\nonumber \\\\ =&amp; \\left\\langle \\sum_{i=1}^n \\alpha_i K(x_i, \\cdot) + h(\\cdot), K(\\cdot, x_j) \\right\\rangle \\nonumber \\\\ =&amp; \\sum_{i=1}^n \\alpha_i K(x_i, x_j) + \\sum_{i=1}^n \\alpha_i h(x_j) \\nonumber \\\\ =&amp; \\sum_{i=1}^n \\alpha_i K(x_i, x_j) \\end{align}\\] Which means that, the evaluation of \\(f(x_j)\\) would be the same as just evaluating it on this finite represtantation. Hence the loss function would be the same regardless of whether we have \\(h\\) or not. And also the penalty term of this finite represtantation would be better since \\[\\begin{align} \\lVert f \\rVert^2 =&amp; \\lVert \\sum_{i=1}^n \\alpha_i K(\\cdot, x_i) + h(\\cdot) \\rVert^2 \\nonumber \\\\ =&amp; \\lVert \\sum_{i=1}^n \\alpha_i K(\\cdot, x_i) \\rVert^2 + \\lVert h(\\cdot) \\rVert^2 \\nonumber \\\\ \\geq&amp; \\lVert \\sum_{i=1}^n \\alpha_i K(\\cdot, x_i) \\rVert^2 \\end{align}\\] This completes the proof since this finite represetnation would be the one being prefered than \\(f\\). Reference Aronszajn, Nachman. 1950. “Theory of Reproducing Kernels.” Transactions of the American Mathematical Society 68 (3): 337–404. Kimeldorf, George S, and Grace Wahba. 1970. “A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines.” The Annals of Mathematical Statistics 41 (2): 495–502. "],["kernel-ridge-regression.html", "Chapter 16 Kernel Ridge Regression 16.1 Example: Linear Kernel and Ridge Regression 16.2 Example: Alternative View", " Chapter 16 Kernel Ridge Regression With our understandings of the RKHS and the representer theorem, we can now say that for any regression function models, if we want the solution to be more flexible, we may solve it within a RKHS. For example, consider the following regression problem: \\[\\widehat f = \\underset{f \\in {\\cal H}}{\\arg\\min} \\,\\, \\frac{1}{n} \\sum_{i=1}^n \\Big(y_i - \\widehat f(x_i) \\Big)^2 + \\lambda \\lVert f \\rVert_{\\cal H}^2\\] Since we know that the solution has to take the form \\[\\widehat f = \\sum_{i=1}^n \\alpha_i K(x_i, \\cdot),\\] we can instead solve the problem as a ridge regression type of problem: \\[\\widehat f = \\underset{f \\in {\\cal H}}{\\arg\\min} \\,\\, \\frac{1}{n} \\big\\lVert \\mathbf{y}- \\mathbf{K}\\boldsymbol{\\alpha}\\big\\rVert^2 + \\lambda \\lVert f \\rVert_{\\cal H}^2,\\] where \\(\\mathbf{K}\\) is an \\(n \\times n\\) matrix with \\(K(x_i, x_j)\\) at its \\((i,j)\\)th element. With some simple calculation, we also have \\[\\begin{align} \\lVert f \\rVert_{\\cal H}^2 =&amp; \\langle f, f \\rangle \\nonumber \\\\ =&amp; \\langle \\sum_{i=1}^n \\alpha_i K(x_i, \\cdot), \\sum_{j=1}^n \\alpha_j K(x_j, \\cdot) \\rangle \\nonumber \\\\ =&amp; \\sum_{i, j} \\alpha_i \\alpha_j \\big\\langle K(x_i, \\cdot), K(x_j, \\cdot) \\big\\rangle \\nonumber \\\\ =&amp; \\sum_{i, j} \\alpha_i \\alpha_j K(x_i, x_j) \\nonumber \\\\ =&amp; \\boldsymbol{\\alpha}^\\text{T}\\mathbf{K}\\boldsymbol{\\alpha} \\end{align}\\] Hence, the problem becomes \\[\\widehat f = \\underset{f \\in {\\cal H}}{\\arg\\min} \\,\\, \\frac{1}{n} \\big\\lVert \\mathbf{y}- \\mathbf{K}\\boldsymbol{\\alpha}\\big\\rVert^2 + \\lambda \\boldsymbol{\\alpha}^\\text{T}\\mathbf{K}\\boldsymbol{\\alpha}.\\] By taking the derivative with respect to \\(\\boldsymbol{\\alpha}\\), we have (note that \\(\\mathbf{K}\\) is symmetric), \\[\\begin{align} -\\frac{1}{n} \\mathbf{K}^\\text{T}(\\mathbf{y}- \\mathbf{K}\\boldsymbol{\\alpha}) + \\lambda \\mathbf{K}\\boldsymbol{\\alpha}\\overset{\\text{set}}{=} \\mathbf{0} \\nonumber \\\\ \\mathbf{K}(- \\mathbf{y}+ \\mathbf{K}\\boldsymbol{\\alpha}+ n\\lambda \\boldsymbol{\\alpha}) = \\mathbf{0}. \\end{align}\\] This implies \\[ \\boldsymbol{\\alpha}= (\\mathbf{K}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{y}.\\] and we obtained the solution. 16.1 Example: Linear Kernel and Ridge Regression When \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\text{T}\\mathbf{x}_j\\), we also have \\(\\mathbf{K}= \\mathbf{X}\\mathbf{X}^\\text{T}\\). We should expect this to match the original ridge regression since this is essentially a linear regression. First, plug this into our previous result, we have \\[ \\boldsymbol{\\alpha}= (\\mathbf{X}\\mathbf{X}^\\text{T}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{y}.\\] and the fitted value is \\[ \\widehat{\\mathbf{y}} = \\mathbf{K}\\boldsymbol{\\alpha}= \\mathbf{X}\\mathbf{X}^\\text{T}(\\mathbf{X}\\mathbf{X}^\\text{T}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{y}\\] Using a matrix identity \\((\\mathbf{P}\\mathbf{Q}+ \\mathbf{I})^{-1}\\mathbf{P}= \\mathbf{P}(\\mathbf{Q}\\mathbf{P}+ \\mathbf{I})^{-1}\\), and let \\(\\mathbf{Q}= \\mathbf{X}= \\mathbf{P}^\\text{T}\\), we have \\[ \\widehat{\\mathbf{y}} = \\mathbf{K}\\boldsymbol{\\alpha}= \\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}\\] and \\[ \\widehat{\\mathbf{y}} = \\mathbf{X}\\underbrace{\\big[ \\mathbf{X}^\\text{T}\\boldsymbol{\\alpha}\\big]}_{\\boldsymbol{\\beta}} = \\mathbf{X}\\underbrace{\\big[ (\\mathbf{X}^\\text{T}\\mathbf{X}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\text{T}\\mathbf{y}\\big]}_{\\boldsymbol{\\beta}}\\] which is simply the Ridge regression solution, and also the corresponding linear regression solution \\(\\widehat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\text{T}\\widehat{\\boldsymbol{\\alpha}}\\). This makes the penalty term \\(\\boldsymbol{\\alpha}^\\text{T}\\mathbf{K}\\boldsymbol{\\alpha}= \\boldsymbol{\\alpha}^\\text{T}\\mathbf{X}\\mathbf{X}^\\text{T}\\boldsymbol{\\alpha}= \\boldsymbol{\\beta}^\\text{T}\\boldsymbol{\\beta}\\), which maps every thing back to the ridge regression form. 16.2 Example: Alternative View This example is motivated from an alternative derivation provided by Prof. Max Welling on his kernel ridge regression lecture note. This understanding matches the SVM primal to dual derivation, but is performed on a linear regression. We can then again switch things to the kernel version (through kernel trick). Consider a linear regression \\[\\underset{\\boldsymbol{\\beta}}{\\text{minimize}} \\,\\, \\frac{1}{n} \\lVert \\mathbf{y}- \\mathbf{X}\\boldsymbol{\\beta}\\rVert^2 + \\lambda \\lVert \\boldsymbol{\\beta}\\rVert^2\\] Introduce a new set of variables \\[z_i = y_i - \\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta},\\] for \\(i = 1, \\ldots, n\\). Then The original problem becomes \\[\\begin{align} \\underset{\\boldsymbol{\\beta}, \\mathbf{z}}{\\text{minimize}} \\quad &amp; \\frac{1}{2n\\lambda} \\lVert \\mathbf{z}\\rVert^2 + \\frac{1}{2} \\lVert \\boldsymbol{\\beta}\\rVert^2 \\nonumber \\\\ \\text{subj. to} \\quad &amp; z_i = y_i - \\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}, \\,\\, i = 1, \\ldots, n. \\end{align}\\] If we use the same strategy from the SVM derivation, we have the Lagrangian \\[{\\cal L} = \\frac{1}{2n\\lambda} \\lVert \\mathbf{z}\\rVert^2 + \\frac{1}{2} \\lVert \\boldsymbol{\\beta}\\rVert^2 + \\sum_{i=1}^n \\alpha_i (y_i - \\mathbf{x}_i^\\text{T}\\boldsymbol{\\beta}- z_i)\\] with \\(\\alpha_i \\in \\mathbb{R}\\). Switching from primal to dual, by taking derivative w.r.t. \\(\\boldsymbol{\\beta}\\) and \\(\\mathbf{z}\\), we have \\[\\begin{align} \\frac{\\partial \\cal L}{\\partial z_i} =&amp;\\, \\frac{1}{n\\lambda}z_i - \\alpha_i = 0, \\quad \\text{for} \\,\\, i = 1, \\ldots, n, \\nonumber \\\\ \\text{and}\\,\\, \\frac{\\partial \\cal L}{\\partial \\boldsymbol{\\beta}} =&amp;\\, \\boldsymbol{\\beta}- \\sum_{i=1}^n \\alpha_i \\mathbf{x}_i = \\mathbf{0} \\end{align}\\] Hence, we have, the estimated \\(\\widehat{\\boldsymbol{\\beta}}\\) is \\(\\sum_{i=1}^n \\alpha_i \\mathbf{x}_i\\) that matches our previous understanding. Also, if we view this as a linear kernel solution, the predicted value of at \\(\\mathbf{x}\\) is \\[\\begin{align} f(\\mathbf{x}) =&amp; \\,\\, \\mathbf{x}^\\text{T}\\boldsymbol{\\beta}\\nonumber \\\\ =&amp; \\sum_{i=1}^n \\alpha_i \\mathbf{x}^\\text{T}\\mathbf{x}_i \\nonumber \\\\ =&amp; \\sum_{i=1}^n \\alpha_i K(\\mathbf{x}, \\mathbf{x}_i). \\end{align}\\] Now, to complete our dual solution, we plugin these results, and have \\[\\begin{align} \\underset{\\boldsymbol{\\alpha}}{\\max} \\underset{\\mathbf{z}, \\boldsymbol{\\beta}}{\\min} {\\cal L} =&amp; \\frac{n\\lambda}{2} \\boldsymbol{\\alpha}^\\text{T}\\boldsymbol{\\alpha}+ \\frac{1}{2} \\sum_{i, j} \\alpha_i \\alpha_j x_i^\\text{T}x_j + \\sum_{j} \\alpha_j \\big(y_j - x_j^\\text{T}\\sum_i \\alpha_i \\mathbf{x}_i - n\\lambda \\alpha_i \\big) \\nonumber \\\\ =&amp; - \\frac{n\\lambda}{2} \\boldsymbol{\\alpha}^\\text{T}\\boldsymbol{\\alpha}- \\frac{1}{2} \\sum_{i, j} \\alpha_i \\alpha_j \\langle x_i, x_j \\rangle + \\sum_{i} \\alpha_i y_i \\nonumber \\\\ =&amp; - \\frac{n\\lambda}{2} \\boldsymbol{\\alpha}^\\text{T}\\boldsymbol{\\alpha}- \\frac{1}{2} \\boldsymbol{\\alpha}^\\text{T}\\mathbf{K}\\boldsymbol{\\alpha}+ \\boldsymbol{\\alpha}^\\text{T}\\mathbf{y} \\end{align}\\] By again taking derivative w.r.t. \\(\\alpha\\), we have \\[ - n\\lambda \\mathbf{I}\\boldsymbol{\\alpha}- \\mathbf{K}\\boldsymbol{\\alpha}+ \\mathbf{y}= \\mathbf{0},\\] and the solution is the same as what we had before \\[\\boldsymbol{\\alpha}= (\\mathbf{K}+ n\\lambda \\mathbf{I})^{-1} \\mathbf{y}\\] "],["classification-and-regression-trees.html", "Chapter 17 Classification and Regression Trees 17.1 Example: Classification Tree 17.2 Splitting a Node 17.3 Regression Trees 17.4 Predicting a Target Point 17.5 Tuning a Tree Model", " Chapter 17 Classification and Regression Trees A tree model is very simple to fit and enjoys interpretability. It is also the core component of random forest and boosting. Both trees and random forests can be used for classification and regression problems, although trees are not ideal for regressions problems due to its large bias. There are two main stream of tree models, Classification and Regression Trees (CART, Breiman et al. (1984)) and C4.5 (Quinlan 1993), which is an improvement of the ID3 (Iterative Dichotomiser 3) algorithm. The main difference is to use binary or multiple splits and the criteria of the splitting rule. In fact the splitting rule criteria is probably the most essential part of a tree. 17.1 Example: Classification Tree Let’s generate a model with nonlinear classification rule. set.seed(1) n = 500 x1 = runif(n, -1, 1) x2 = runif(n, -1, 1) y = rbinom(n, size = 1, prob = ifelse(x1^2 + x2^2 &lt; 0.6, 0.9, 0.1)) par(mar=rep(2,4)) plot(x1, x2, col = ifelse(y == 1, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = 19) symbols(0, 0, circles = sqrt(0.6), add = TRUE, inches = FALSE, cex = 2) A classification tree model is recursively splitting the feature space such that eventually each region is dominated by one class. We will use rpart as an example to fit trees, which stands for recursively partitioning. set.seed(1) library(rpart) rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y)) # the tree structure par(mar=rep(0.5, 4)) plot(rpart.fit) text(rpart.fit) # if you want to peek into the tree # note that we set cp = 0.041, which is a tuning parameter # we will discuss this later rpart.fit$cptable ## CP nsplit rel error xerror xstd ## 1 0.17040359 0 1.0000000 1.0000000 0.04984280 ## 2 0.14798206 3 0.4843049 0.7264574 0.04692735 ## 3 0.01121076 4 0.3363229 0.4484305 0.04010884 ## 4 0.01000000 7 0.3004484 0.4035874 0.03852329 prune(rpart.fit, cp = 0.041) ## n= 500 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 500 223 0 (0.55400000 0.44600000) ## 2) x2&lt; -0.6444322 90 6 0 (0.93333333 0.06666667) * ## 3) x2&gt;=-0.6444322 410 193 1 (0.47073171 0.52926829) ## 6) x1&gt;=0.6941279 68 8 0 (0.88235294 0.11764706) * ## 7) x1&lt; 0.6941279 342 133 1 (0.38888889 0.61111111) ## 14) x2&gt;=0.7484327 53 7 0 (0.86792453 0.13207547) * ## 15) x2&lt; 0.7484327 289 87 1 (0.30103806 0.69896194) ## 30) x1&lt; -0.6903174 51 9 0 (0.82352941 0.17647059) * ## 31) x1&gt;=-0.6903174 238 45 1 (0.18907563 0.81092437) * The model proceed with the following steps. Note that steps 5 and 6 may not be really beneficial (consider that we know the true model). Alternatively, there are many other packages that can perform the same analysis. For example, the tree package. However, be careful that this package uses a different splitting rule by default If you want to match the result, use split = \"gini\". Note that this plot is very crowded because it will split until pretty much only one class in each terminal node. Hence, you can imaging that there will be a tuning parameter issue. We will discuss this later. library(tree) tree.fit = tree(as.factor(y)~x1+x2, data = data.frame(x1, x2, y), split = &quot;gini&quot;) plot(tree.fit) text(tree.fit) 17.2 Splitting a Node In a tree model, the splitting mechanism performs in the following way, which is just comparing all possible splits on all variables. For simplicity, we will assume that a binary splitting rule is used, i.e., we split the current node into to two child nodes, and apply the procedure recursively. At the current node, go through each variable to find the best cut-off point that splits the node. Compare all the best cut-off points across all variable and choose the best one to split the current node and then iterate. So, what error criterion should we use to compare different cut-off points? There are three of them at least: Gini impurity (CART) Shannon entropy (C4.5) Mis-classification error Gini impurity is used in CART, while ID3/C4.5 uses the Shannon entropy. These criteria have different effects than the mis-classifications error. They usually prefer more “pure” nodes, meaning that it is more likely to single out a set of pure class terminal node if we use Gini impurity and Shannon entropy. This is because their measures are nonlinear. Suppose that we have a population (or a set of observations) with \\(p_k\\) proportion of class \\(k\\), for \\(k = 1, \\ldots, K\\). Then, the Gini impurity is given by \\[ \\text{Gini} = \\sum_{k = 1}^K p_k (1 - p_k) = 1 - \\sum_{k=1}^K p_k^2.\\] The Shannon theory is defined as \\[- \\sum_{k=1}^K p_k \\log(p_k).\\] And the classification error simply adds up all mis-classified portions if we predict the population into the most prevalent one: \\[ 1 - \\underset{k = 1, \\ldots, K}{\\max} \\,\\, p_k\\] The following plot shows all three quantities as a function of \\(p\\), when there are only two classes, i.e., \\(K = 2\\). For each quantity, smaller value means that the node is more “pure”, hence, there is a higher certainty when we predict a new value. The idea of splitting a node is that, we want the two resulting child node to contain less variation. In other words, we want each child node to be as “pure” as possible. Hence, the idea is to calculate this error criterion both before and after the split and see what cut-off point gives us the best reduction of error. Of course, all of these quantities will be calculated based on the sample version, instead of the truth. For example, if we use the Gini impurity to compare different splits, we use the following quantity for an internal node \\({\\cal A}\\): \\[\\begin{align} \\text{score}(j, c) = \\text{Gini}({\\cal A}) - \\left( \\frac{N_{{\\cal A}_L}}{N_{{\\cal A}}} \\text{Gini}({\\cal A}_L) + \\frac{N_{{\\cal A}_R}}{N_{{\\cal A}}} \\text{Gini}({\\cal A}_R) \\right). \\end{align}\\] Here, \\({\\cal A}_L\\) (left child node) and \\({\\cal A}_R\\) (right child node) denote the two child nodes resulted from a potential split on the \\(j\\)th variable at a cut-off point \\(c\\), such that \\[{\\cal A}_L = \\{\\mathbf{x}: \\mathbf{x}\\in {\\cal A}, \\, x_j \\leq c\\}\\] and \\[{\\cal A}_R = \\{\\mathbf{x}: \\mathbf{x}\\in {\\cal A}, \\, x_j &gt; c\\}.\\] Then \\(N_{\\cal A}\\), \\(N_{{\\cal A}_L}\\), \\(N_{{\\cal A}_R}\\) are the number of observations in these nodes, respectively. The implication of this is quite intuitive: \\(\\text{Gini}({\\cal A})\\) calculates the uncertainty of the entire node \\({\\cal A}\\), while the second quantity is a summary of the uncertainty of the two potential child nodes. Hence a larger score indicates a better split, and we may choose the best index \\(j\\) and cut-off point \\(c\\) to proceed, \\[\\underset{j \\, , \\, c}{\\arg\\max} \\,\\, \\text{score}(j, c)\\] and then work on each child node separately using the same procedure. 17.3 Regression Trees The basic procedure for a regression tree is pretty much the same as a classification tree, except that we will use a different way to evaluate how good a potential split is. Note that the variance is a simple quantity to describe the noise within a node, we can use \\[\\begin{align} \\text{score}(j, c) = \\text{Var}({\\cal A}) - \\left( \\frac{N_{{\\cal A}_L}}{N_{{\\cal A}}} \\text{Var}({\\cal A}_L) + \\frac{N_{{\\cal A}_R}}{N_{{\\cal A}}} \\text{Var}({\\cal A}_R) \\right). \\end{align}\\] 17.4 Predicting a Target Point When we have a new target point \\(\\mathbf{x}_0\\) to predict, the basic strategy is to “drop it down the tree”. This is simply starting from the root node and following the splitting rule to see which terminal node it ends up with. Note that a fitted tree will have a collection of terminal nodes, say, \\(\\{{\\cal A}_1, {\\cal A}_2, \\ldots, {\\cal A}_M\\}\\), then suppose \\(\\mathbf{x}_0\\) falls into terminal node \\({\\cal A}_m\\), we use \\(\\bar{y}_{{\\cal A}_m}\\), the average of original training data that falls into this node, as the prediction. The final prediction can be written as \\[\\begin{align} \\widehat{f}(\\mathbf{x}_0) =&amp; \\sum_{m = 1}^M \\bar{y}_{{\\cal A}_m} \\mathbf{1}\\{\\mathbf{x}_0 \\in {\\cal A}_m\\} \\\\ =&amp; \\sum_{m = 1}^M \\frac{\\sum_{i=1}^n y_i \\mathbf{1}\\{\\mathbf{x}_i \\in {\\cal A}_m\\}}{\\sum_{i=1}^n \\mathbf{1}\\{\\mathbf{x}_i \\in {\\cal A}_m\\}} \\mathbf{1}\\{\\mathbf{x}_0 \\in {\\cal A}_m\\}. \\end{align}\\] 17.5 Tuning a Tree Model Tree tuning is essentially about when to stop splitting. Or we could look at this reversely by first fitting a very large tree, then see if we could remove some branches of a tree to make it simpler without sacrificing much accuracy. One approach is called the cost-complexity pruning. This is another penalized framework that we use the accuracy as the loss function, and use the tree-size as the penalty part for complexity. Formally, if we have any tree model \\({\\cal T}\\), consider this can be written as \\[\\begin{align} C_\\alpha({\\cal T}) =&amp;~ \\sum_{\\text{all terminal nodes $t$ in ${\\cal T}$}} N_t \\cdot \\text{Impurity}(t) + \\alpha |{\\cal T}| \\nonumber \\\\ =&amp;~ C({\\cal T}) + \\alpha |{\\cal T}| \\end{align}\\] Now, we can start with a very large tree, say, fitted until all pure terminal nodes. Call this tree as \\({\\cal T}_\\text{max}\\). We can then exhaust all its sub-trees by pruning any branches, and calculate this \\(C(\\cdot)\\) function of the sub-tree. Then the tree that gives the smallest value will be our best tree. But this can be computationally too expensive. Hence, one compromise, instead of trying all possible sub-trees, is to use the weakest-link cutting. This means that, we cut the branch (essentially a certain split) that displays the weakest banefit towards the \\(C(\\cdot)\\) function. The procedure is the following: Look at an internal node \\(t\\) of \\({\\cal T}_\\text{max}\\), and denote the entire branch starting from \\(t\\) as \\({\\cal T}_t\\) Compare: remove the entire branch (collapse \\({\\cal T}_t\\) into a single terminal node) vs. keep \\(T_t\\). To do this, calculate \\[\\alpha \\leq \\frac{C(t) - C({\\cal T}_t)}{|T_t| - 1}\\] Note that \\(|{\\cal T}_t| - 1\\) is the size difference between the two trees. Try all internal nodes \\(t\\), and cut the branch \\(t\\) that has the smallest value on the right hand side. This gives the smallest \\(\\alpha\\) value to remove some branches. Then iterate the procedure based on this reduced tree. Note that the \\(\\alpha\\) values will get larger as we move more branches. Hence this produces a solution path. Now this is very similar to the Lasso solution path idea, and we could use cross-validation to select the best tuning. By default, the rpart function uses a 10-fold cross-validation. This can be controlled using the rpart.control() function and specify the xval argument. For details, please see the documentation. The following plot using plotcp() in the rpart package gives a visualization of the relative cross-validation error. It also produces a horizontal line (the dotted line). It suggests the lowest (plus certain variation) that we could achieve. Hence, we will select the best cp value (\\(alpha\\)) that is above this line. The way that this is constructed is similar to the lambda.1se choice in glmnet. # and the tuning parameter plotcp(rpart.fit) printcp(rpart.fit) ## ## Classification tree: ## rpart(formula = as.factor(y) ~ x1 + x2, data = data.frame(x1, ## x2, y)) ## ## Variables actually used in tree construction: ## [1] x1 x2 ## ## Root node error: 223/500 = 0.446 ## ## n= 500 ## ## CP nsplit rel error xerror xstd ## 1 0.170404 0 1.00000 1.00000 0.049843 ## 2 0.147982 3 0.48430 0.72646 0.046927 ## 3 0.011211 4 0.33632 0.44843 0.040109 ## 4 0.010000 7 0.30045 0.40359 0.038523 Reference Breiman, Leo, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 1984. Classification and Regression Trees. Monterey, CA: Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software. Quinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Elsevier. "],["random-forests.html", "Chapter 18 Random Forests 18.1 Bagging Predictors 18.2 Random Forests 18.3 Kernel view of Random Forests 18.4 Variable Importance 18.5 Adaptiveness of Random Forest Kernel", " Chapter 18 Random Forests Roughly speaking, random forests (Breiman 2001) are parallelly fitted CART models with some randomness. There are several main components: Bootstrapping of data for each tree using the Bagging idea (Breiman 1996), and use the averaged result (for regression) or majority voting (for classification) of all trees as the prediction. At each internal node, we may not consider all variables. Instead, we consider a randomly selected mtry variables to search for the best split. This idea was inspired by Ho (1998). For each tree, we will not perform pruning. Instead, we simply stop when the internal node contains no more than nodesize number of observations. Later on, there were various version of random forests that attempts to improve the performance, from both computational and theoretical prospective. We will introduce them later. 18.1 Bagging Predictors CART models may be difficult when dealing with non-axis-aligned decision boundaries. This can be seen from the example below, in a two-dimensional case. The idea of Bagging is that we can fit many CART models, each from a Bootstrap sample, i.e., sample with replacement from the original \\(n\\) observations. The reason that Breiman considered bootstrap samples is because it can approximate the original distribution that generates the data. But the end result is that since each tree may be slightly different from each other, when we stack them, the decision bound can be more “smooth”. # generate some data set.seed(2) n = 1000 x1 = runif(n, -1, 1) x2 = runif(n, -1, 1) y = rbinom(n, size = 1, prob = ifelse((x1 + x2 &gt; -0.5) &amp; (x1 + x2 &lt; 0.5) , 0.8, 0.2)) xgrid = expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01)) Let’s compare the decision rule of CART and Bagging. For CART, the decision line has to be aligned to axis. For Bagging, we use a total of 200 trees, specified by nbagg in the ipred package. # fit CART library(rpart) rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y)) # we could fit a different tree using a bootstrap sample # rpart.fit = rpart(as.factor(y)~x1+x2, data = data.frame(x1, x2, y)[sample(1:n, n, replace = TRUE), ]) pred = matrix(predict(rpart.fit, xgrid, type = &quot;class&quot;) == 1, 201, 201) contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels=0.5, labels=&quot;&quot;,axes=FALSE) points(x1, x2, col = ifelse(y == 1, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = 19, yaxt=&quot;n&quot;, xaxt = &quot;n&quot;) points(xgrid, pch=&quot;.&quot;, cex=1.2, col=ifelse(pred, &quot;deepskyblue&quot;, &quot;darkorange&quot;)) box() title(&quot;CART&quot;) # fit Bagging library(ipred) bag.fit = bagging(as.factor(y)~x1+x2, data = data.frame(x1, x2, y), nbagg = 200, ns = 400) pred = matrix(predict(prune(bag.fit), xgrid) == 1, 201, 201) contour(seq(-1, 1, 0.01), seq(-1, 1, 0.01), pred, levels=0.5, labels=&quot;&quot;,axes=FALSE) points(x1, x2, col = ifelse(y == 1, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = 19, yaxt=&quot;n&quot;, xaxt = &quot;n&quot;) points(xgrid, pch=&quot;.&quot;, cex=1.2, col=ifelse(pred, &quot;deepskyblue&quot;, &quot;darkorange&quot;)) box() title(&quot;Bagging&quot;) 18.2 Random Forests Random forests are equipped with this bootstrapping strategy, but also with other randomness and mechanism. A Random forest can be controlled by several key parameters: ntree: Number of trees, many controls the stability. We typically want to fit a large number of trees. sampsize: How many samples to use when fitting each tree. In practice, we often use the training sample size if sampled with replacement. mtry: Number of randomly sampled variable to consider at each internal node. This need to be tuned for adaptiveness to sparse signal (large mtry) or highly correlated features (small mtry). nodesize: Stop splitting when the node sample size is no larger than nodesize. This works similar to \\(k\\) in a KNN model. However, its effect can be greatly affected by other tuning parameters. Using the randomForest package, we can fit the model. It is difficult to visualize this when p &gt; 2. But we can look at the testing error. # generate some data with larger p set.seed(2) n = 1000 p = 10 X = matrix(runif(n*p, -1, 1), n, p) x1 = X[, 1] x2 = X[, 2] y = rbinom(n, size = 1, prob = ifelse((x1 + x2 &gt; -0.5) &amp; (x1 + x2 &lt; 0.5), 0.8, 0.2)) xgrid = expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01)) # fit random forests with a selected tuning library(randomForest) ## randomForest 4.7-1.2 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin rf.fit = randomForest(X, as.factor(y), ntree = 1000, mtry = 7, nodesize = 10, sampsize = 800) Instead of generating a set of testing samples labels, let’s directly compare with the “true” decision rule, the Bayes rule. # the testing data Xtest = matrix(runif(n*p, -1, 1), n, p) # the Bayes rule BayesRule = ifelse((Xtest[, 1] + Xtest[, 2] &gt; -0.5) &amp; (Xtest[, 1] + Xtest[, 2] &lt; 0.5), 1, 0) mean( (predict(rf.fit, Xtest) == &quot;1&quot;) == BayesRule ) ## [1] 0.785 18.3 Kernel view of Random Forests For this part, we need to install a new package RLT. This is a package in development. But you can install the current version from GitHub (ver \\(\\geq\\) 4.2.6). Please note that the current CRAN version (ver. 3.2.5) does not work for this part. Use the following code to install the package. If you are using MacOS, then you need to follow this guild to install the package for OpenMP. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;teazrq/RLT&quot;) Similar to a tree model, random forest can also be viewed as kernel estimator. Essentially its a stacking of kernels induced from all trees. The idea has been illustrated in Scornet (2016). However, since random forest is a random algorithm, each tree can be slightly different from each other. To incorporate this, denote the randomness of a tree estimator by \\(\\eta\\), which can affect how the tree is constructed. Suppose we fit \\(B\\) trees in a random forest, with each tree denoted as \\({\\cal T_b}\\), then a random forest estimator can be expressed as \\[ \\hat{f}(x) = \\frac{1}{B} \\sum_{b = 1}^B \\frac{\\sum_i K_{\\cal T_b}(x, x_i; \\eta_b) y_i}{ \\sum_i K_{\\cal T_b}(x, x_i; \\eta_b) }. \\] Note that in this expression, the denominators in each tree estimator are different since we may randomly end-up with some sample size smaller than the nodesize after a split. However, we can still think each terminal nodes as having roughly the same size. Hence, we could also consider an alternative kernel induced from random forest, which aligns with traditional kernel estimators. \\[ \\hat{f}(x) = \\frac{ \\sum_i y_i \\sum_{b = 1}^B K_{\\cal T_b}(x, x_i) }{ \\sum_i \\sum_{b = 1}^B K_{\\cal T_b}(x, x_i) }. \\] In this case, the kernel function is \\[ K_\\text{RF}(x, x_i) = \\sum_{b = 1}^B K_{\\cal T_b}(x, x_i), \\] which counts how many times \\(x\\) falls into the same terminal node as observation \\(x_i\\). Hence, this kernel representation can be incorporated into many machine learning algorithms. For example, if we are interested in a supervised clustering setting, we can first fit a random forest model and perform spectral clustering using the induced kernel matrix. We can also use the kernel ridge regression with the kernel induced. Let’s generate a new dataset with 5 continuous variables. The true model depends on just the first two variables. # generate data n = 1000; p = 5 X = matrix(runif(n*p), n, p) y = X[, 1] + X[, 2] + rnorm(n) If we fit just one tree, there could be different variations based on the randomness. Please note that the name of these parameters can be different in the RLT package. library(RLT) ## RLT and Random Forests v4.2.6 ## pre-release at github.com/teazrq/RLT par(mfrow=c(2, 3)) for (i in 1:6) { # fit a model with one tree RLTfit &lt;- RLT(X, y, ntrees = 1, nmin = 30, mtry = 5, split.gen = &quot;best&quot;, resample.prob = 1, resample.replace = TRUE, param.control = list(&quot;resample.track&quot; = TRUE)) # target point 1 newX = matrix(c(0.25, 0.75, 0.5, 0.5, 0.5), 1, 5) KernelW = forest.kernel(RLTfit, X1 = newX, X2 = X, vs.train = TRUE)$Kernel par(mar = c(2, 2, 2, 2)) plot(X[, 1], X[, 2], col = &quot;deepskyblue&quot;, pch = 19, cex = 0.5) points(X[, 1], X[, 2], col = &quot;darkorange&quot;, pch = 19, cex = KernelW&gt;0, lwd = 2) points(newX[1], newX[2], col = &quot;black&quot;, pch = 4, cex = 3, lwd = 5) } If we stack all of them, we obtain a random forest kernel. RLTfit &lt;- RLT(X, y, ntrees = 1000, nmin = 10, mtry = 5, split.gen = &quot;best&quot;, resample.prob = 1, resample.replace = TRUE, param.control = list(&quot;resample.track&quot; = TRUE)) par(mfrow=c(1, 2)) # target point 1 newX = matrix(c(0.25, 0.75, 0.5, 0.5, 0.5), 1, 5) KernelW = forest.kernel(RLTfit, X1 = newX, X2 = X, vs.train = TRUE)$Kernel par(mar = c(2, 2, 2, 2)) plot(X[, 1], X[, 2], col = &quot;deepskyblue&quot;, pch = 19, cex = 0.5) points(X[, 1], X[, 2], col = &quot;darkorange&quot;, cex = 10*KernelW/1000, lwd = 2) points(newX[1], newX[2], col = &quot;black&quot;, pch = 4, cex = 4, lwd = 5) legend(&quot;bottomright&quot;, &quot;Target Point&quot;, pch = 4, col = &quot;black&quot;, lwd = 5, lty = NA, cex = 1.5) # target point 2 newX = matrix(c(0.5, 0.3, 0.5, 0.5, 0.5), 1, 5) KernelW = forest.kernel(RLTfit, X1 = newX, X2 = X, vs.train = TRUE)$Kernel par(mar = c(2, 2, 2, 2)) plot(X[, 1], X[, 2], col = &quot;deepskyblue&quot;, pch = 19, cex = 0.5) points(X[, 1], X[, 2], col = &quot;darkorange&quot;, cex = 10*KernelW/1000, lwd = 2) points(newX[1], newX[2], col = &quot;black&quot;, pch = 4, cex = 4, lwd = 5) legend(&quot;bottomright&quot;, &quot;Target Point&quot;, pch = 4, col = &quot;black&quot;, lwd = 5, lty = NA, cex = 1.5) 18.4 Variable Importance The idea of variable importance is to identify which features (or variables) are most influential in predicting the outcome based on the fitted model. Variable importance is typically assessed through techniques like mean decrease accuracy, which measures the decrease in model accuracy when a variable’s values are permuted across the out-of-bag samples, thereby disrupting the relationship between that variable and the target. Alternatively, it can also be measured using the mean decrease impurity, which calculates the total reduction in the criterion (Gini impurity, entropy, or mean squared error) that each variable provides when used in trees, averaged over all trees in the forest. The calculation can be summarized by the following steps: Train the Random Forest: Fit a random forest model to your data using all available variables. Out-of-Bag Evaluation: For each tree in the forest, predict the outcome for the out-of-bag (OOB) samples—these are the samples not used in the construction of that particular tree. Compute the OOB accuracy (or another relevant metric like AUC for classification, MSE for regression) for these predictions. Permute Variable &amp; Re-evaluate: For each variable of interest, randomly permute its values among the OOB samples. Then, use the same tree to make predictions on these “shuffled” data and compute the accuracy (or other metrics) again. Calculate Decrease in Accuracy: Compare the accuracy obtained from the permuted data to the original OOB accuracy for each tree. The difference is a measure of the importance of the variable for that specific tree. Average Over All Trees: Aggregate these importance measures across all trees in the forest to get a single importance score for each variable. rf.fit = randomForest(x = X, y = y, ntree = 1000, nodesize = 10, mtry = 20, importance = TRUE) # variable importance for %IncMSE (1st column) rf.fit$importance 18.5 Adaptiveness of Random Forest Kernel However, random forest can adapt pretty well in a high-dimensional, especially sparse setting. This is because of the greedy splitting rule selection. The adaptiveness works in a way that, it tends to ignore covariates that are not effective on explaining the variation of \\(Y\\). Hence, making the model similar to a kernel method on a low-dimensional space. The following example illustrate this effect in a two-dimensional case. We can see that the outcome is only related to the first dimension. Hence, when setting mtry = 2, we will almost always prefer to split on the first variable, making its neighbors very close to the target prediction point \\((0, 0)^T\\). # generate some data set.seed(2) n = 400 x1 = runif(n, -1, 1) x2 = runif(n, -1, 1) y = 2*x1 + 0.2*rnorm(n) xgrid = expand.grid(x1 = seq(-1, 1, 0.01), x2 = seq(-1, 1, 0.01)) # fit forest rf.fit = RLT(x = data.frame(x1, x2), y = y, model = &quot;regression&quot;, mtry = 2, nmin = 40, param.control = list(resample.track = TRUE)) # calculate kernel rf.kernel = forest.kernel(rf.fit, X1 = data.frame(&quot;x1&quot; = 0, &quot;x2&quot; = 0), X2 = data.frame(x1, x2), vs.train = TRUE)$Kernel # kernel weights plot(x1, x2, pch = 19, yaxt=&quot;n&quot;, xaxt = &quot;n&quot;, cex = (rf.kernel + 1)^0.15 - 0.7) points(0, 0, col = &quot;red&quot;, pch = 18, cex = 3) The tuning parameter mtry has a very strong effect on controlling this greediness. When mtry is large, we will be very greedy on selecting the true signal variable to split. In a high-dimensional setting, we may only use a few variables before reaching a terminal node, making the model only rely a few dimensions. When we use a very small mtry, the model behaves similarly to a regular kernel estimator with good smoothing (small variance) property. However, since it is effectively randomly selecting a dimension to split, the bandwidth on each dimension would also be similar but large since we can only afford a few splits before the node size becomes too small. This can be seen from the following example, with mtry = 1. # fit forest rf.fit = RLT(x = data.frame(x1, x2), y = y, model = &quot;regression&quot;, mtry = 1, nmin = 40, param.control = list(resample.track = TRUE)) # calculate kernel rf.kernel = forest.kernel(rf.fit, X1 = data.frame(&quot;x1&quot; = 0, &quot;x2&quot; = 0), X2 = data.frame(x1, x2), vs.train = TRUE)$Kernel # kernel weights plot(x1, x2, pch = 19, yaxt=&quot;n&quot;, xaxt = &quot;n&quot;, cex = (rf.kernel + 1)^0.15 - 0.7) points(0, 0, col = &quot;red&quot;, pch = 18, cex = 3) Reference Breiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40. ———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. Ho, Tin Kam. 1998. “The Random Subspace Method for Constructing Decision Forests.” IEEE Transactions on Pattern Analysis and Machine Intelligence 20 (8): 832–44. Scornet, Erwan. 2016. “Random Forests and Kernel Methods.” IEEE Transactions on Information Theory 62 (3): 1485–1500. "],["boosting.html", "Chapter 19 Boosting 19.1 AdaBoost 19.2 Training Error of AdaBoost 19.3 Tuning the Number of Trees 19.4 Gradient Boosting 19.5 Gradient Boosting with Logistic Link", " Chapter 19 Boosting Boosting is another ensemble model, created in the form of \\[F_T(x) = \\sum_{t = 1}^T \\alpha_t f_t(x)\\] However, it is different from random forest, in which each \\(f_t(x)\\) is learned parallelly. These \\(f_t(x)\\)’s are called weak learners and are constructed sequentially, with coefficients \\(\\alpha_t\\)’s to represent their weights. The most classical model, AdaBoost was proposed by Freund and Schapire (1997) for classification problems, and a more statically view of this type of model called gradient boosting machines (J. H. Friedman 2001) can handle any loss function we commonly use. We will first introduce AdaBoost and then discuss gradient boosting. 19.1 AdaBoost Following our common notation, we observe a set of data \\(\\{\\mathbf{x}_i, y_i\\}_{i=1}^n\\). Similar to SVM, we code \\(y_i\\)s as \\(-1\\) or \\(1\\). The AdaBoost works by creating \\(F_T(x)\\) sequentially and use \\(\\text{sign}(F_T(x))\\) as the classification rule. The algorithm is given in the following: Initiate weights \\(w_i^{(1)} = 1/n\\), for \\(i = 1, \\ldots, n\\) For \\(t = 1, \\ldots, T\\), do Fit a classifier \\(f_t(x)\\) to the training data with subject weights \\(w_i^{(t)}\\)’s. Compute the weighed error rate \\[\\epsilon_t = \\sum_{i=1}^n w_i^{(t)} \\mathbf{1}\\{y_i \\neq f_t(x_i) \\}\\] Compute \\[\\alpha_t = \\frac{1}{2} \\log \\frac{1 - \\epsilon_t}{\\epsilon_t}\\] Update subject weights \\[w_i^{(t + 1)} = \\frac{1}{Z_t} w_i^{(t)} \\exp\\big\\{ - \\alpha_t y_i f_t(x_i) \\big\\}\\] where \\(Z_t\\) is a normalizing constant make \\(w_i^{(t + 1)}\\)’s sum up to 1: \\[Z_t = \\sum_{i=1}^n w_i^{(t)} \\exp\\big\\{ - \\alpha_t y_i f_t(x_i) \\big\\}\\] Output the final model \\[F_T(x) = \\sum_{t = 1}^T \\alpha_t f_t(x)\\] and the decision rule is \\(\\text{sign}(F_T(x))\\). An important mechanism in AdaBoost is the weight update step. We can notice that the weight is increased if \\(\\exp\\big\\{ - \\alpha_t y_i f_t(x_i) \\big\\}\\) is larger than 1. This is simply when \\(y_i f_t(x_i)\\) is negative, i.e., subject \\(i\\) got mis-classified by \\(f_t\\) at this iteration. Hence, during the next iteration \\(t+1\\), the model \\(f_{(t+1)}\\) will more likely to address this subject. Here, \\(f_t\\) can be any classification model, for example, we could use a tree model. The following figures demonstrate this idea of updating weights and aggregate the learners. x1 = seq(0.1, 1, 0.1) x2 = c(0.5, 0.3, 0.1, 0.6, 0.7, 0.8, 0.5, 0.7, 0.8, 0.2) # the data y = c(1, 1, -1, -1, 1, 1, -1, 1, -1, -1) X = cbind(&quot;x1&quot; = x1, &quot;x2&quot; = x2) xgrid = expand.grid(&quot;x1&quot; = seq(0, 1.1, 0.01), &quot;x2&quot; = seq(0, 0.9, 0.01)) # plot data plot(X[, 1], X[, 2], col = ifelse(y &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = ifelse(y &gt; 0, 4, 1), xlim = c(0, 1.1), lwd = 3, ylim = c(0, 0.9), cex = 3) # fit gbm with 3 trees library(gbm) ## Loaded gbm 2.2.2 ## This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3 gbm.fit = gbm(y ~., data.frame(x1, x2, y= as.numeric(y == 1)), distribution=&quot;adaboost&quot;, interaction.depth = 1, n.minobsinnode = 1, n.trees = 3, shrinkage = 1, bag.fraction = 1) # you may peek into each tree pretty.gbm.tree(gbm.fit, i.tree = 1) ## SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight Prediction ## 0 0 0.25 1 2 3 2.5 10 0.00 ## 1 -1 1.00 -1 -1 -1 0.0 2 1.00 ## 2 -1 -0.25 -1 -1 -1 0.0 8 -0.25 ## 3 -1 0.00 -1 -1 -1 0.0 10 0.00 # we can view the predicted decision rule plot(X[, 1], X[, 2], col = ifelse(y &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = ifelse(y &gt; 0, 4, 1), xlim = c(0, 1.1), lwd = 3, ylim = c(0, 0.9), cex = 3) pred = predict(gbm.fit, xgrid) ## Using 3 trees... points(xgrid, col = ifelse(pred &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), cex = 0.2) Here is a rundown of the algorithm. Let’s initialize all weights as \\(1/n\\). We only used trees with a single split as weak learners. The first tree is splitting at \\(X_1 = 0.25\\). After the first split, we need to adjust the weights. w1 = rep(1/10, 10) f1 &lt;- function(x) ifelse(x[, 1] &lt; 0.25, 1, -1) e1 = sum(w1*(f1(X) != y)) a1 = 0.5*log((1-e1)/e1) w2 = w1*exp(- a1*y*f1(X)) w2 = w2/sum(w2) # the first tree plot(X[, 1], X[, 2], col = ifelse(y &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = ifelse(y &gt; 0, 4, 1), xlim = c(0, 1.1), lwd = 3, ylim = c(0, 0.9), cex = 3) pred = f1(xgrid) points(xgrid, col = ifelse(pred &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), cex = 0.2) # weights after the first tree plot(X[, 1], X[, 2], col = ifelse(y &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = ifelse(y &gt; 0, 4, 1), xlim = c(0, 1.1), lwd = 3, ylim = c(0, 0.9), cex = 30*w2) We can notice that the observations got correctly classified will decrease their weights while those mis-classified will increase the weights. f2 &lt;- function(x) ifelse(x[, 2] &gt; 0.65, 1, -1) e2 = sum(w2*(f2(X) != y)) a2 = 0.5*log((1-e2)/e2) w3 = w2*exp(- a2*y*f2(X)) w3 = w3/sum(w3) # the second tree plot(X[, 1], X[, 2], col = ifelse(y &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = ifelse(y &gt; 0, 4, 1), xlim = c(0, 1.1), lwd = 3, ylim = c(0, 0.9), cex = 30*w2) pred = f2(xgrid) points(xgrid, col = ifelse(pred &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), cex = 0.2) # weights after the second tree plot(X[, 1], X[, 2], col = ifelse(y &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = ifelse(y &gt; 0, 4, 1), xlim = c(0, 1.1), lwd = 3, ylim = c(0, 0.9), cex = 30*w3) And then we have the third step. Combining all three steps and their decision function, we have the final classifier \\[\\begin{align} F_3(x) =&amp; \\sum_{t=1}^3 \\alpha_t f_t(x) \\nonumber \\\\ =&amp; 0.4236 \\cdot f_1(x) + 0.6496 \\cdot f_2(x) + 0.9229 \\cdot f_3(x) \\end{align}\\] f3 &lt;- function(x) ifelse(x[, 1] &lt; 0.85, 1, -1) e3 = sum(w3*(f3(X) != y)) a3 = 0.5*log((1-e3)/e3) # the third tree plot(X[, 1], X[, 2], col = ifelse(y &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = ifelse(y &gt; 0, 4, 1), xlim = c(0, 1.1), lwd = 3, ylim = c(0, 0.9), cex = 30*w3) pred = f3(xgrid) points(xgrid, col = ifelse(pred &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), cex = 0.2) # the final decision rule plot(X[, 1], X[, 2], col = ifelse(y &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), pch = ifelse(y &gt; 0, 4, 1), xlim = c(0, 1.1), lwd = 3, ylim = c(0, 0.9), cex = 3) pred = a1*f1(xgrid) + a2*f2(xgrid) + a3*f3(xgrid) points(xgrid, col = ifelse(pred &gt; 0, &quot;deepskyblue&quot;, &quot;darkorange&quot;), cex = 0.2) abline(v = 0.25) # f1 abline(h = 0.65) # f2 abline(v = 0.85) # f3 19.2 Training Error of AdaBoost There is an interesting property about the boosting algorithm that if we can always find a classifier that performs better than random guessing at each iteration \\(t\\), then the training error will eventually converge to zero. This works by analyzing the weight after the last iteration \\(T\\): \\[\\begin{align} w_i^{(T+1)} =&amp; \\frac{1}{Z_T} w_i^{(T)} \\exp\\big\\{ - \\alpha_t y_i f_t(x_i) \\big\\} \\nonumber \\\\ =&amp; \\frac{1}{Z_1\\cdots Z_T} w_i^{(1)} \\prod_{t = 1}^T \\exp\\big\\{ - \\alpha_t y_i f_t(x_i) \\big\\} \\nonumber \\\\ =&amp; \\frac{1}{Z_1\\cdots Z_T} \\frac{1}{n} \\exp\\Big\\{ - y_i \\sum_{t = 1}^T \\alpha_t f_t(x_i) \\Big\\} \\end{align}\\] Since \\(\\sum_{t = 1}^T \\alpha_t f_t(x_i)\\) is just the model at the \\(T\\)-th iteration, we can write it as \\(F_T(x_i)\\). Noticing that they sum up to 1, we have \\[1 = \\sum_{i = 1}^n w_i^{(T+1)} = \\frac{1}{Z_1\\cdots Z_T} \\frac{1}{n} \\sum_{i = 1}^n \\exp\\big\\{ - y_i F_T(x_i) \\big\\}\\] and \\[Z_1\\cdots Z_T = \\frac{1}{n} \\sum_{i = 1}^n \\exp\\big\\{ - y_i F_T(x_i) \\big\\}\\] On the right-hand-side, this is the exponential loss after we fit the model. In fact, this quantity would bound above the 0/1 loss, since the exponential loss is \\(\\exp[ - y f(x) ]\\), For correctly classified subjects, \\(y f(x) &gt; 0\\), and \\(\\exp[ - y f(x) ] &gt; 0\\) For incorrectly classified subjects, \\(y f(x) &lt; 0\\) the exponential loss is larger than 1 This means that \\[Z_1\\cdots Z_T &gt; \\frac{1}{n} \\sum_{i = 1}^n \\mathbf{1} \\big\\{ y_i \\neq \\text{sign}(F_T(x_i)) \\big\\}\\] Hence, if we want the final model to have low training error, we should bound above the \\(Z_t\\)’s. Recall that \\(Z_t\\) is used to normalize the weights, we have \\[Z_t = \\sum_i^{n} w_i^{(t)} \\exp[ - \\alpha_t y_i f_t(x_i) ].\\] We have two cases at this iteration, \\(y_i f(x_i) = 1\\) for correct subjects, and \\(y_i f(x_i) = -1\\) for the incorrect ones, hence, By our definition, \\(\\epsilon_t = \\sum_i w_i^{(t)} \\mathbf{1} \\big\\{ y_i \\neq f_t(x_i) \\big\\}\\) is the proportion of weights for mis-classified samples. \\[\\begin{align} Z_t =&amp; \\,\\,\\sum_{i=1}^n w_i^{(t)} \\exp[ - \\alpha_t y_i f_t(x_i)] \\nonumber\\\\ =&amp;\\,\\,\\sum_{y_i = f_t(x_i)} w_i^{(t)} \\exp[ - \\alpha_t ] + \\sum_{y_i \\neq f_t(x_i)} w_i^{(t)} \\exp[ \\alpha_t ] \\nonumber\\\\ =&amp; \\,\\, \\exp[ - \\alpha_t ] \\sum_{y_i = f_t(x_i)} w_i^{(t)} + \\exp[ \\alpha_t ] \\sum_{y_i \\neq f_t(x_i)} w_i^{(t)} \\end{align}\\] So we have \\[ Z_t = (1 - \\epsilon_t) \\exp[ - \\alpha_t ] + \\epsilon_t \\exp[ \\alpha_t ].\\] If we want to minimize the product of all \\(Z_t\\)’s, we can consider minimizing each of them. Let’s consider this as a function of \\(\\alpha_t\\), then by taking a derivative with respect to \\(\\alpha_t\\), we have \\[ - (1 - \\epsilon_t) \\exp[ - \\alpha_t ] + \\epsilon_t \\exp[ \\alpha_t ] = 0\\] and \\[\\alpha_t = \\frac{1}{2} \\log \\frac{1 - \\epsilon_t}{\\epsilon_t}.\\] Plugging this back into \\(Z_t\\), we have \\[Z_t = 2 \\sqrt{\\epsilon_t(1-\\epsilon_t)}\\] Since \\(\\epsilon_t(1-\\epsilon_t)\\) can only attain maximum of \\(1/4\\), \\(Z_t\\) must be smaller than 1. This makes the product \\(Z_1 \\cdots Z_T\\) converging to 0. If we look at this more closely, by defining \\(\\gamma_t = \\frac{1}{2} - \\epsilon_t\\) as the improvement from a random model (with error \\(1/2\\)), then \\[\\begin{align} Z_t =&amp; 2 \\sqrt{\\epsilon_t(1-\\epsilon_t)} \\nonumber \\\\ =&amp; \\sqrt{1 - 4 \\gamma_t^2} \\nonumber \\\\ \\leq&amp; \\exp\\big[ - 2 \\gamma_t^2 \\big] \\end{align}\\] The last equation is because by Taylor expansion, \\(\\exp\\big[ - 4 \\gamma_t^2 \\big] \\geq 1 - 4 \\gamma_t^2\\). Then, we can finally put all \\(Z_t\\)’s together: \\[\\begin{align} \\text{Training Error} =&amp; \\sum_{i = 1}^n \\mathbf{1} \\big\\{ y_i \\neq \\text{sign}(F_T(x_i)) \\big\\} \\nonumber \\\\ =&amp; \\sum_{i = 1}^n \\exp \\big[ - y_i \\neq F_T(x_i) \\big] \\nonumber \\\\ =&amp; Z_1 \\cdots Z_T \\nonumber \\\\ \\leq&amp; \\exp \\big[ - 2 \\sum_{t=1}^T \\gamma_t^2 \\big], \\end{align}\\] which converges to 0 as long as \\(\\sum_{t=1}^T \\gamma_t^2\\) accumulates up to infinite. But of course, in practice, it would increasing difficult find \\(f_t(x)\\) that reduces the training error greatly. 19.3 Tuning the Number of Trees Although we can get really low training classification error, this is subject to overfitting. The following code demonstrates what an overfitted looks like. # One-dimensional classification example n = 1000; set.seed(1) x = cbind(seq(0, 1, length.out = n), runif(n)) py = (sin(4*pi*x[, 1]) + 1)/2 y = rbinom(n, 1, py) plot(x[, 1], y + runif(n, -0.05, 0.05), pch = 19, ylim = c(-0.05, 1.05), cex = 0.5, col = ifelse(y==1,&quot;darkorange&quot;, &quot;deepskyblue&quot;), xlab = &quot;x&quot;, ylab = &quot;P(Y=1 | X=x)&quot;) points(x[, 1], py, type = &quot;l&quot;, lwd = 3) # fit AdaBoost with bootstrapping, I am using a large shrinkage factor gbm.fit = gbm(y~., data.frame(x, y), distribution=&quot;adaboost&quot;, n.minobsinnode = 2, n.trees=200, shrinkage = 1, bag.fraction=0.8, cv.folds = 10) # plot the decision function (Fx, not sign(Fx)) size=c(1, 5, 10, 20, 50, 100) for(i in 1:6) { par(mar=c(2,2,3,1)) plot(x[, 1], py, type = &quot;l&quot;, lwd = 3, ylab = &quot;P(Y=1 | X=x)&quot;, col = &quot;gray&quot;) points(x[, 1], y + runif(n, -0.05, 0.05), pch = 19, cex = 0.5, ylim =c(-0.05, 1.05), col = ifelse(y==1, &quot;darkorange&quot;, &quot;deepskyblue&quot;)) Fx = predict(gbm.fit, n.trees=size[i]) # this returns the fitted function, but not class lines(x[, 1], 1/(1+exp(-2*Fx)), lwd = 1) title(paste(&quot;# of Iterations = &quot;, size[i])) } Hence, selecting trees is necessary. For this purpose, we can use either the out-of-bag error to estimate the exponential upper bound, or simply do cross-validation. # get the best number of trees from cross-validation (or oob if no cv is used) gbm.perf(gbm.fit) ## [1] 39 19.4 Gradient Boosting Let’s take an alternative view of this problem, we use an additive structure to fit models \\[F_T(x) = \\sum_{t = 1}^T \\alpha_t f(x; \\boldsymbol{\\theta}_t)\\] by minimizing a loss function \\[\\underset{\\{\\alpha_t, \\boldsymbol{\\theta}_t\\}_{t=1}^T}{\\min} \\sum_{i=1}^n L\\big(y_i, F_T(x_i)\\big)\\] In this framework, we may choose a loss function \\(L\\) that is suitable for the problem, and also choose the base learner \\(f(x; \\boldsymbol{\\theta})\\) with parameter \\(\\boldsymbol{\\theta}\\). Examples of this include linear function, spline, tree, etc.. While it maybe difficult to minimize over all parameters \\(\\{\\alpha_t, \\boldsymbol{\\theta}_t\\}_{t=1}^T\\), we may consider doing this in a stage-wise fashion. The algorithm could work in the following way: Set \\(F_0(x) = 0\\) For \\(t = 1, \\ldots, T\\) Choose \\((\\alpha_t, \\boldsymbol{\\theta}_t)\\) to minimize the loss \\[\\underset{\\alpha, \\boldsymbol{\\theta}}{\\min} \\,\\, \\sum_{i=1}^n L\\big(y_i, F_{t-1}(x_i) + \\alpha f(x_i; \\boldsymbol{\\theta})\\big)\\] Update \\(F_t(x) = F_{t-1}(x) + \\alpha_t f(x; \\boldsymbol{\\theta}_t)\\) Output \\(F_T(x)\\) as the final model The previous AdaBoost example is using exponential loss function. Also, it doesn’t pick an optimal \\(f(x; \\boldsymbol{\\theta})\\) at each step. We just need a model that is better than random. The step size \\(\\alpha_t\\) is optimized at each \\(t\\) given the fitted \\(f(x; \\boldsymbol{\\theta}_t)\\). Another example is the forward stage-wise linear regression. In this case, we fit a single variable linear model at each step \\(t\\): \\[f(x, j) = \\text{sign}\\big(\\text{Cor}(X_j, \\mathbf{r})\\big) X_j\\] * \\(\\mathbf{r}\\) is the residual, as \\(r_i = y_i - F_{t-1}(x_i)\\) * \\(j\\) is the index that has the largest absolute correlation with \\(\\mathbf{r}\\) Then we give a very small step size \\(\\alpha_t\\), say, \\(\\alpha_t = 10^{-5}\\), and with sign equal to the correlation between \\(X_j\\). In this case, \\(F_t(x)\\) is almost equivalent to the Lasso solution path, as \\(t\\) increases. We may notice that \\(r_i\\) is in fact the negative gradient of the squared-error loss, as a function of the fitted function: \\[r_{it} = - \\left[ \\frac{\\partial \\, \\big(y_i - F(x_i)\\big)^2 }{\\partial \\, F(x_i)} \\right]_{F(x_i) = F_{t-1}(x_i)}\\] and we are essentially fitting a weak leaner \\(f_t(x)\\) to the residuals and update the fitted model \\(F_t(x)\\). The following example shows the result of using a tree leaner as \\(f_t(x)\\): library(gbm) # a simple regression problem p = 1 x = seq(0, 1, 0.001) fx &lt;- function(x) 2*sin(3*pi*x) y = fx(x) + rnorm(length(x)) plot(x, y, pch = 19, ylab = &quot;y&quot;, col = &quot;gray&quot;, cex = 0.5) # plot the true regression line lines(x, fx(x), lwd = 2, col = &quot;deepskyblue&quot;) We can see that the fitted model progressively approaximates the true function. # fit regression boosting # I use a very large shrinkage value for demonstrating the functions # in practice you should use 0.1 or even smaller values for stability gbm.fit = gbm(y~x, data = data.frame(x, y), distribution = &quot;gaussian&quot;, n.trees=300, shrinkage=0.5, bag.fraction=0.8) # somehow, cross-validation for 1 dimensional problem creates error # gbm(y ~ ., data = data.frame(x, y), cv.folds = 3) # this produces an error # plot the fitted regression function at several iterations par(mfrow=c(2,3)) size=c(1,5,10,50,100,300) for(i in 1:6) { par(mar=c(2,2,3,1)) plot(x, y, pch = 19, ylab = &quot;y&quot;, col = &quot;gray&quot;, cex = 0.5) lines(x, fx(x), lwd = 2, col = &quot;deepskyblue&quot;) # this returns the fitted function, but not class Fx = predict(gbm.fit, n.trees=size[i]) lines(x, Fx, lwd = 3, col = &quot;darkorange&quot;) title(paste(&quot;# of Iterations = &quot;, size[i])) } This idea can be generalized to any loss function \\(L\\). This is the gradient boosting model: At each iteration \\(t\\), calculate ``pseudo-residuals’’, i.e., the negative gradient for each observation \\[g_{it} = - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x_i) = F_{t-1}(x_i)}\\] Fit \\(f_t(x, \\boldsymbol{\\theta}_t)\\) to pseudo-residual \\(g_{it}\\)’s Search for the best \\[\\alpha_t = \\underset{\\alpha}{\\arg\\min} \\sum_{i=1}^n L\\big(y_i, F_{t-1}(x_i) + \\alpha f(x_i; \\boldsymbol{\\theta}_t)\\big)\\] Update \\(F_t(x) = F_{t-1}(x) + \\alpha_t f(x; \\boldsymbol{\\theta}_t)\\) Hence, the only change when modeling different outcomes is to choose the loss function \\(L\\), and derive the pseudo-residuals For regression, the loss is \\(\\frac{1}{2} (y - f(x))^2\\), and the pseudo-residual is \\(y_i - f(x_i)\\) For quantile regression to model median, the loss is \\(|y - f(x)|\\), and the pseudo-residual is sign\\((y_i - f(x_i))\\) For classification, we can use the negative log likelihood of a single observation \\(- [ y\\log(p) + (1-y)\\log(1-p) ]\\), and express \\(p\\) as the log-odds of a scale predictor, i.e., \\(f = \\log(p/(1-p))\\). Then the pseudo-residual is \\(y_i - p(x_i)\\) 19.5 Gradient Boosting with Logistic Link To see how the pseudo-residual of a classification model is derived, let’s use the logistic link function the predicted probability \\(p\\) defined as: \\[ p = \\frac{e^{F(x)}}{1 + e^{F(x)}} = \\frac{1}{1 + e^{-F(x)}} \\] The negative log-likelihood for the Bernoulli distribution for a single instance is given by: \\[ L(y, p) = - [y \\log(p) + (1 - y) \\log(1 - p)] \\] Let’s first find the partial derivative of this loss function with respect to \\(p\\): \\[ \\frac{\\partial L(y, p)}{\\partial p} = -\\left[ y \\frac{1}{p} - (1-y) \\frac{1}{1-p} \\right] \\] The derivative of \\(p\\) with respect to \\(F(x)\\) is: \\[ \\frac{\\partial p}{\\partial F(x)} = \\frac{e^{-F(x)}}{(1 + e^{-F(x)})^2} = p(1-p) \\] Hence, the partial derivative of the loss function with respect to \\(F\\) is: \\[\\begin{align*} \\frac{\\partial L(y, p)}{\\partial F(x)} &amp;= \\frac{\\partial L(y, p)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial F(x)}\\\\ &amp;= -\\left[ y \\frac{1}{p} - (1-y) \\frac{1}{1-p} \\right] \\cdot p(1-p)\\\\ &amp;= -(y - y p + p - y p)\\\\ &amp;= -(y - p) \\end{align*}\\] Note that we should move \\(F(x)\\) to the negative gradient, then \\(y_i - p_i\\) is the pseudo-residual that we use in the boosting algorithm to fit the next tree / linear booster. The sign mainly influences the direction of the adjustment but is accounted for in the optimization process. Reference Freund, Yoav, and Robert E Schapire. 1997. “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.” Journal of Computer and System Sciences 55 (1): 119–39. Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics, 1189–1232. "],["k-means.html", "Chapter 20 K-Means 20.1 Basic Concepts 20.2 Example 1: iris data 20.3 Example 2: clustering of image pixels", " Chapter 20 K-Means 20.1 Basic Concepts The \\(k\\)-means clustering algorithm attempts to solve the following optimization problem: \\[ \\underset{C, \\, \\{m_k\\}_{k=1}^K}\\min \\sum_{k=1}^K \\sum_{C(i) = k} \\lVert x_i - m_k \\rVert^2, \\] where \\(C(\\cdot): \\{1, \\ldots, n\\} \\rightarrow \\{1, \\ldots, K\\}\\) is a cluster assignment function, and \\(m_k\\)’s are the cluster means. To solve this problem, \\(k\\)-means uses an iterative approach that updates \\(C(\\cdot)\\) and \\(m_k\\)’s alternatively. Suppose we have a set of six observations. We first randomly assign them into two clusters (initiate a random \\(C\\) function). Based on this cluster assignment, we can calculate the corresponding cluster mean \\(m_k\\)’s. Then we will assign each observation to the closest cluster mean. In this example, only the blue point on the top will be moved to a new cluster. Then the cluster means can then be recalculated. When there is nothing to move anymore, the algorithm stops. Keep in mind that we started with a random cluster assignment, and this objective function is not convex. Hence we may obtain different results if started with different values. The solution is to try different starting points and use the best final results. This can be tuned using the nstart parameter in the kmeans() function. # some random data set.seed(1) mat = matrix(rnorm(1000), 50, 20) # if we use only one starting point kmeans(mat, centers = 3, nstart = 1)$tot.withinss ## [1] 885.8913 # if we use multiple starting point and pick the best one kmeans(mat, centers = 3, nstart = 100)$tot.withinss ## [1] 883.8241 20.2 Example 1: iris data We use the classical iris data as an example. This dataset contains three different classes, but the goal here is to learn the clusters without knowing the class labels. # plot the original data using two variables head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa library(ggplot2) ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() The last two variables in the iris data carry more information on separating the three classes. Hence we will only use the Petal.Length and Petal.Width. library(colorspace) par(mar = c(3, 2, 4, 2), xpd = TRUE) MASS::parcoord(iris[, -5], col = rainbow_hcl(3)[iris$Species], var.label = TRUE, lwd = 2) legend(x = 1.2, y = 1.3, cex = 1, legend = as.character(levels(iris$Species)), fill = rainbow_hcl(3), horiz = TRUE) Let’s perform the \\(k\\)-means clustering set.seed(1) # k mean clustering iris.kmean &lt;- kmeans(iris[, 3:4], centers = 3, nstart = 20) # the center of each class iris.kmean$centers ## Petal.Length Petal.Width ## 1 1.462000 0.246000 ## 2 5.595833 2.037500 ## 3 4.269231 1.342308 # the within cluster variation iris.kmean$withinss ## [1] 2.02200 16.29167 13.05769 # the between cluster variation iris.kmean$betweenss ## [1] 519.524 # plot the fitted clusters vs. the truth iris.kmean$cluster &lt;- as.factor(iris.kmean$cluster) ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + # true cluster geom_point(alpha = 0.3, size = 3.5) + scale_color_manual(values = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;)) + geom_point(col = c(&quot;blue&quot;, &quot;green&quot;, &quot;red&quot;)[iris.kmean$cluster]) # fitted cluster 20.3 Example 2: clustering of image pixels Let’s first load and plot an image of Leo. library(jpeg) img&lt;-readJPEG(&quot;data/leo.jpg&quot;) # generate a blank image par(mar=rep(0.2, 4)) plot(c(0, 400), c(0, 500), xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, bty = &#39;n&#39;, pch = &#39;&#39;, ylab = &#39;&#39;, xlab = &#39;&#39;) rasterImage(img, 0, 0, 400, 500) For a jpg file, each pixel is stored as a vector with 3 elements — representing red, green and blue intensities. However, by the way, that this objective img being constructed, it is stored as a 3d array. The first two dimensions are the height and width of the figure. We need to vectorize them and treat each pixel as an observation. dim(img) ## [1] 500 400 3 # this apply function applies vecterization to each layer (r/g/b) of the image. img_expand = apply(img, 3, c) # and now we have the desired data matrix dim(img_expand) ## [1] 200000 3 Before performing the \\(k\\)-mean clustering, let’s have a quick peek at the data in a 3d view. Since there are too many observations, we randomly sample a few. library(scatterplot3d) set.seed(1) sub_pixels = sample(1:nrow(img_expand), 1000) sub_img_expand = img_expand[sub_pixels, ] scatterplot3d(sub_img_expand, pch = 19, xlab = &quot;red&quot;, ylab = &quot;green&quot;, zlab = &quot;blue&quot;, color = rgb(sub_img_expand[,1], sub_img_expand[,2], sub_img_expand[,3])) The next step is to perform the \\(k\\)-mean and obtain the cluster label. For example, let’s try 5 clusters. kmeanfit &lt;- kmeans(img_expand, 5) # to produce the new graph, we simply replicate the cluster mean # for all observations in the same cluster new_img_expand = kmeanfit$centers[kmeanfit$cluster, ] # now we need to convert this back to the array that can be plotted as an image. # this is a lazy way to do it, but get the job done new_img = img new_img[, , 1] = matrix(new_img_expand[,1], 500, 400) new_img[, , 2] = matrix(new_img_expand[,2], 500, 400) new_img[, , 3] = matrix(new_img_expand[,3], 500, 400) # plot the new image plot(c(0, 400), c(0, 500), xaxt = &#39;n&#39;, yaxt = &#39;n&#39;, bty = &#39;n&#39;, pch = &#39;&#39;, ylab = &#39;&#39;, xlab = &#39;&#39;) rasterImage(new_img, 0, 0, 400, 500) With this technique, we can easily reproduce results with different \\(k\\) values. Apparently, as \\(k\\) increases, we get better resolution. \\(k = 30\\) seems to recover the original image fairly well. ## Warning: did not converge in 10 iterations "],["hierarchical-clustering.html", "Chapter 21 Hierarchical Clustering 21.1 Basic Concepts 21.2 Example 1: iris data 21.3 Example 2: RNA Expression Data", " Chapter 21 Hierarchical Clustering 21.1 Basic Concepts Suppose we have a set of six observations: The goal is to progressively group them together until there is only one group. During this process, we will always choose the closest two groups (some may be individuals) to merge. If we evaluate the distance between two observations, that would be very easy. For example, the Euclidean distance and Hamming distance can be used. But what about the distance between two groups? Suppose we have two groups of observations \\(G\\) and \\(H\\), then several distance metric can be considered: Complete linkage: the furthest pair \\[d(G, H) = \\underset{i \\in G, \\, j \\in G}{\\max} d(x_i, x_j)\\] Single linkage: the closest pair \\[d(G, H) = \\underset{i \\in G, \\, j \\in G}{\\min} d(x_i, x_j)\\] Average linkage: average distance \\[d(G, H) = \\frac{1}{n_G n_H} \\sum_{i \\in G} \\sum_{i \\in H} d(x_i, x_j)\\] The R function hclust() uses the complete linkage as default. To perform a hierarchical clustering, we need to know all the pair-wise distances, i.e., \\(d(x_i, x_j)\\). Let’s consider the Euclidean distance. # the Euclidean distance can be computed using dist() as.matrix(dist(x)) ## 1 2 3 4 5 6 ## 1 0.000000 1.2294164 1.7864196 1.1971565 1.4246185 1.5698349 ## 2 1.229416 0.0000000 2.3996575 0.8727261 1.9243764 2.2708670 ## 3 1.786420 2.3996575 0.0000000 2.8586738 0.4782442 0.2448835 ## 4 1.197156 0.8727261 2.8586738 0.0000000 2.4219048 2.6741260 ## 5 1.424618 1.9243764 0.4782442 2.4219048 0.0000000 0.4204479 ## 6 1.569835 2.2708670 0.2448835 2.6741260 0.4204479 0.0000000 We use this distance matrix in the hierarchical clustering algorithm hclust(). The plot() function will display the merging process. This should be exactly the same as we demonstrated previously. The height of each split represents how separated the two subsets are (the distance when they are merged). Selecting the number of clusters is still a tricky problem. Usually, we pick a cutoff where the height of the next split is short. Hence, the above example fits well with two clusters. 21.2 Example 1: iris data The iris data contains three clusters and four variables. We use all variables in the distance calculation and use the default complete linkage. iris_hc &lt;- hclust(dist(iris[, 3:4])) plot(iris_hc) This does not seem to perform very well, considering that we know the true number of classes is three. This shows that, in practice, the detected clusters can heavily depend on the variables you use. Let’s try some other linkage functions. iris_hc &lt;- hclust(dist(iris[, 3:4]), method = &quot;average&quot;) plot(iris_hc, hang = -1) This looks better, at least more consistent with the truth. Now we can also consider using other package to plot this result. For example, the ape package provides some interesting choices. library(ape) plot(as.phylo(iris_hc), type = &quot;unrooted&quot;, cex = 0.6, no.margin = TRUE) We can also add the true class colors to the plot. This plot is motivated by the dendextend package vignettes. Of course in a realistic situation, we wouldn’t know what the true class is. 21.3 Example 2: RNA Expression Data We use a tissue gene expression dataset from the tissuesGeneExpression library, available from bioconductor. I prepared the data to include only 100 genes. You can download the data from the course website. In this first step, we simply plot the data using a heatmap. By default, a heatmap uses red to denote higher values, and yellow for lower values. Note that we first plot the data without organizing the columns or rows. The data is also standardized based on columns (genes). load(&quot;data/tissue.Rda&quot;) dim(expression) ## [1] 189 100 table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver placenta ## 38 34 15 31 39 26 6 head(expression[, 1:3]) ## 211298_s_at 203540_at 211357_s_at ## GSM11805.CEL.gz 7.710426 5.856596 12.618471 ## GSM11814.CEL.gz 4.741010 5.813841 5.116707 ## GSM11823.CEL.gz 11.730652 5.986338 13.206078 ## GSM11830.CEL.gz 5.061337 6.316815 9.780614 ## GSM12067.CEL.gz 4.955245 6.561705 8.589003 ## GSM12075.CEL.gz 10.469501 5.880740 13.050554 heatmap(scale(expression), Rowv = NA, Colv = NA) Hierarchical clustering may help us discover interesting patterns. If we reorganize the columns and rows based on the clusters, then it may reveal underlying subclasses of issues, or subgroups of genes. heatmap(scale(expression)) Note that there are many other R packages that produce more interesting plots. For example, you can try the heatmaply package. "],["principal-component-analysis.html", "Chapter 22 Principal Component Analysis 22.1 Basic Concepts 22.2 Example 1: iris Data 22.3 Example 2: Handwritten Digits", " Chapter 22 Principal Component Analysis 22.1 Basic Concepts Principal Component Analysis (PCA) is arguably the most commonly used approach for dimension reduction and visualization. The idea is to capture major signals of variation in a dataset. A nice demonstration of the search of direction is provided at this r-bloggers site: Let’s look at a two-dimensional case, we are trying to find a line (direction) on this plain, such that if all points are projected onto this line, their coordinates have the largest variance, compared with any other line. The following code is used to generate a set of observations. # generate some random data from a 2-dimensional normal distribution. library(MASS) set.seed(1) n = 100 Sigma = matrix(c(0.5, -0.65, -0.65, 1), 2, 2) x_org = mvrnorm(n, c(1, 2), Sigma) x = scale(x_org, scale = FALSE, center = TRUE) plot(x, xlim = c(-3, 3), ylim= c(-3, 3), pch = 19, cex = 0.75) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) Let’ start with finding a direction to project all the observations onto. And we want this projection to have the largest variation. Of course the direction that goes along the spread of the data would be the best choice for the purpose of large variance. plot(x, xlim = c(-3, 3), ylim= c(-3, 3), pch = 19, cex = 0.75) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) # This line is obtained from performing PCA pc1 = princomp(x)$loadings[,1] abline(a = 0, b = pc1[2]/pc1[1], col = &quot;deepskyblue&quot;, lwd = 4) # The direction pc1 ## [1] 0.5659608 -0.8244322 Once we have the first direction, we can also remove the projection from the original covariates, and search for a direction \\(\\mathbf{v}_2\\) that is orthogonal to \\(\\mathbf{v}_1\\), with \\(\\mathbf{v}_1^\\text{T}\\mathbf{v}_2 = 0\\), such that it contains large variation of \\(\\mathbf{X}\\). par(mar=c(2, 2, 0.3, 0.3)) plot(x, xlim = c(-3.5, 3.5), ylim= c(-3.5, 3.5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) # largest PC pc1 = princomp(x)$loadings[,1] abline(a = 0, b = pc1[2]/pc1[1], col = &quot;deepskyblue&quot;, lwd = 4) # second largest PC pc2 = princomp(x)$loadings[,2] abline(a = 0, b = pc2[2]/pc2[1], col = &quot;darkorange&quot;, lwd = 3) pc2 ## [1] 0.8244322 0.5659608 t(pc1) %*% pc2 ## [,1] ## [1,] 0 We can also see how much variation these two directions accounts for in the original data. The following shows the corresponding standard deviation. princomp(x)$sdev ## Comp.1 Comp.2 ## 1.0748243 0.2206133 Formally, we can generalized this to \\(\\mathbf{X}\\) with any dimensions. And the key tool is to perform the singular value decomposition (SVD): \\[\\mathbf{X}= \\mathbf{U}\\mathbf{D}\\mathbf{V}^\\text{T}\\] The \\(\\mathbf{V}\\) matrix here corresponds to the directions we found. Hence, \\(\\mathbf{v}_1\\) is its first column, \\(\\mathbf{v}_1\\) is its second column, etc.. \\(\\mathbf{D}\\) is a diagonal matrix ordered from the largest to the smallest values, correspond to the standard deviation of the spreads. And \\(\\mathbf{U}\\) represents the coordinates once we project \\(\\mathbf{X}\\) onto those directions. An alternative way to understand this is by matrix approximation, if we want to find a rank-1 matrix that best approximate \\(\\mathbf{X}\\) with the Frobenius norm, we optimize \\[\\text{minimize} \\quad \\lVert \\mathbf{X}- \\mathbf{u}_1 d_1 \\mathbf{v}_1^\\text{T}\\rVert_2^2\\] This can be generalized into any dimensional problem. Another alternative formulation is to use eigen-decomposition of \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\), which can be written as \\[\\mathbf{X}^\\text{T}\\mathbf{X}= \\mathbf{V}\\mathbf{D}\\mathbf{U}^\\text{T}\\mathbf{U}\\mathbf{D}\\mathbf{V}^\\text{T}= \\mathbf{V}\\mathbf{D}^2 \\mathbf{V}^\\text{T}\\] But one thing we usually need to take care of is the centering issue. This is why we used scale() function at the beginning. However, we only center, but not scale the data. If we do not center, then the first principal component (PC) could be a direction that points to the center of the data. Note that when \\(\\mathbf{X}\\) is already centered, \\(\\mathbf{X}^\\text{T}\\mathbf{X}\\) is the covariance matrix. Hence PCA is also performing eigen-decomposition to the covariance matrix. plot(x_org, main = &quot;Before Centering&quot;, xlim = c(-5, 5), ylim= c(-5, 5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) par(mar=c(2, 2, 2, 0.3)) plot(x, main = &quot;After Centering&quot;, xlim = c(-5, 5), ylim= c(-5, 5), pch = 19, cex = 0.5) abline(h = 0, col = &quot;red&quot;) abline(v = 0, col = &quot;red&quot;) Finally, for any dimensional data \\(\\mathbf{X}\\), we usually visualize them in the first two directions, or three. Note that the coordinates on the PC’s can be obtained using either the scores (\\(\\mathbf{U}\\)) in the fitted object of princomp, or simply multiply the original data matrix by the loading matrix \\(\\mathbf{V}\\). pcafit &lt;- princomp(x) # the new coordinates on PC&#39;s head(pcafit$scores) ## Comp.1 Comp.2 ## [1,] 0.88434533 0.13503607 ## [2,] -0.08990294 -0.01851954 ## [3,] 1.13589963 0.20234582 ## [4,] -1.78763374 -0.04571218 ## [5,] -0.26536435 0.14271170 ## [6,] 1.11779894 -0.41759594 # direct calculation based on projection head(x %*% pcafit$loadings) ## Comp.1 Comp.2 ## [1,] 0.88434533 0.13503607 ## [2,] -0.08990294 -0.01851954 ## [3,] 1.13589963 0.20234582 ## [4,] -1.78763374 -0.04571218 ## [5,] -0.26536435 0.14271170 ## [6,] 1.11779894 -0.41759594 # visualize the data on the PCs # Note that the both axies are scaled par(mar=c(4, 4.2, 0.3, 0.3)) plot(pcafit$scores[,1], pcafit$scores[,2], xlab = &quot;First PC&quot;, ylab = &quot;Second PC&quot;, pch = 19, cex.lab = 1.5) abline(h = 0, col = &quot;deepskyblue&quot;, lwd = 4) abline(v = 0, col = &quot;darkorange&quot;, lwd = 4) There are many different functions in R that performs PCA. princomp and prcomp are the most popular ones. 22.1.1 Note: Scaling You should always center the variables when performing PCA, however, whether to use scaling (force each variable to have a standard deviation of 1) depends on the particular application. When you have variables that are extremely disproportionate, e.g., age vs. RNA expression, scaling should be used. This is to prevent some variables from dominating the PC loadings due to their large scales. When all the variables are of the similar type, e.g., color intensities of pixels in a figure, it is better to use the original scale. This is because the variables with larger variations may carry more signal. Scaling may lose that information. 22.2 Example 1: iris Data We use the iris data again. All four variables are considered in this analysis. We plot the first and second PC directions. iris_pc &lt;- prcomp(iris[, 1:4]) library(ggplot2) ggplot(data = data.frame(iris_pc$x), aes(x=PC1, y=PC2)) + geom_point(color=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[iris$Species], size = 3) One may be interested in plotting all pair-wise direction to see if lower PC’s provide useful information. pairs(iris_pc$x, col=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[iris$Species], pch = 19) However, usually, the lower PC’s are less informative. This can also be speculated from the eigenvalue plot, which shows how influential each PC is. plot(iris_pc, type = &quot;l&quot;, pch = 19, main = &quot;Iris PCA Variance&quot;) Feature contributions to the PC can be accessed through the magnitude of the loadings. This table shows that Petal.Length is the most influential variable on the first PC, with loading \\(\\approx 0.8567\\). iris_pc$rotation ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.36138659 -0.65658877 0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 -0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 -0.54583143 0.7536574 We can further visualize this on a plot. This can be helpful when the number of variables is large. features = row.names(iris_pc$rotation) ggplot(data = data.frame(iris_pc$rotation), aes(x=PC1, y=PC2, label=features,color=features)) + geom_point(size = 3) + geom_text(size=3) 22.3 Example 2: Handwritten Digits The handwritten zip code digits data contains 7291 training data and 2007 testing data. Each image is a \\(16 \\times 16\\)-pixel gray-scale image. Hence they are converted to a vector of 256 variables. library(ElemStatLearn) # Handwritten Digit Recognition Data # the first column is the true digit dim(zip.train) ## [1] 7291 257 Here is a sample of some images: Let’s do a simpler task, using just three letters: 1, 4 and 8. zip.sub = zip.train[zip.train[,1] %in% c(1,4,8), -1] zip.sub.truth = as.factor(zip.train[zip.train[,1] %in% c(1,4,8), 1]) dim(zip.sub) ## [1] 2199 256 zip_pc = prcomp(zip.sub) plot(zip_pc, type = &quot;l&quot;, pch = 19, main = &quot;Digits 1, 4, and 8: PCA Variance&quot;) The eigenvalue results suggest that the first two principal components are much more influential than the rest. A pair-wise PC plot of the first four PC’s may further confirm that speculation. pairs(zip_pc$x[, 1:4], col=c(&quot;chartreuse4&quot;, &quot;darkorange&quot;, &quot;deepskyblue&quot;)[zip.sub.truth], pch = 19) Let’s look at the first two PCs more closely. Even without knowing the true class (no colors) we can still vaguely see 3 clusters. library(ggplot2) ggplot(data = data.frame(zip_pc$x), aes(x=PC1, y=PC2)) + geom_point(size = 2) Finally, let’s briefly look at the results of PCA for all 10 different digits. Of course, more PC’s are needed for this task. You can also plot other PC’s to get more information. library(colorspace) zip_pc &lt;- prcomp(zip.train) plot(zip_pc, type = &quot;l&quot;, pch = 19, main = &quot;All Digits: PCA Variance&quot;) ggplot(data = data.frame(prcomp(zip.train)$x), aes(x=PC1, y=PC2)) + geom_point(color = rainbow_hcl(10)[zip.train[,1]+1], size = 1) "],["self-organizing-map.html", "Chapter 23 Self-Organizing Map 23.1 Basic Concepts", " Chapter 23 Self-Organizing Map 23.1 Basic Concepts I found the best demonstration of the Self-Organizing Map algorithm is the following graph that displays it over iterations. It is available at this website: Let’s understand this by pairing it with the algorithm. There are several different algorithms available, but one of the most popular ones is proposed by Kohonen (1990). Here, we present a SOM with a 2-dimensional output. The following are the inputs: \\(\\{x_i\\}_{i=1}^n\\) is a set of \\(n\\) observations, with dimension \\(p\\) (the yellow and green dots in the figure). \\(w_{ij}\\), \\(i = 1, \\ldots p\\), \\(j = 1, \\ldots q\\) are a grid of centers (the connected black dots). They are similar to the centers in a k-mean algorithm. However, they also preserve some geometric relationships among \\(w_{ij}\\)’s, meaning that \\(w_{ij}\\)’s are closer if their indices \\({i, j}\\) are closer (connected in the figure). \\(\\alpha\\) this is a learning rate between \\([0, 1]\\). This controls how fast the \\(w_{ij}\\)’s are updated. \\(r\\) is also a tuning parameter. This controls how many \\(w_{ij}\\)’s will be updated at each iteration Now, we look at the algorithm. This is different from \\(k\\)-means because we do not use all the observations immediately. The algorithm works by stream-in the observations one-by-one. Whenever a new observation \\(x_k\\), \\(k = 1, \\ldots, n\\) comes in, we will update the centers \\(w_{ij}\\)’s by the following: For all \\(w_{ij}\\), calculate the distance between each \\(w_{ij}\\) and \\(x_k\\). Let \\(d_{ij} = \\lVert x_k - w_{ij} \\rVert\\). By default, we use Euclidean distance. Select the closest \\(w_{ij}\\), denoted as \\(w_{\\ast}\\) Update each \\(w_{ij}\\) based on the fomular \\(w_{ij} = w_{ij} + \\alpha \\, h(w_\\ast, w_{ij}, r) \\, \\lVert x_k - w_{ij} \\rVert\\) After each iteration (updating with one more observation), we will decrease the value of \\(\\alpha\\) and \\(r\\). In the kohonen package, the \\(\\alpha\\) starts at 0.05, and gradually decreases to 0.01, while \\(r\\) is chosen to be 2/3 of all cluster means at the first iteration. Using the kohonen package, we perform a SOM on the Handwritten Digit Recognition Data. The heatmap shows how each \\(w_{ij}\\) is away from it’s neighboring \\(w_{ij}\\)’s. The extreme bright one means that the center is quite isolated by itself. library(kohonen) ## ## Attaching package: &#39;kohonen&#39; ## The following object is masked from &#39;package:class&#39;: ## ## somgrid # Handwritten Digit Recognition Data library(ElemStatLearn) # the first column is the true digit dim(zip.train) ## [1] 7291 257 # for speed concern, I only use a few variables (pixels) zip.SOM &lt;- som(zip.train[, seq(2, 257, length.out = 10)], grid = somgrid(10, 10, &quot;rectangular&quot;)) plot(zip.SOM, type = &quot;dist.neighbours&quot;) plot(zip.SOM, type = &quot;mapping&quot;, pchs = 20, main = &quot;Mapping Type SOM&quot;) # plot(zip.SOM, main = &quot;Default SOM Plot&quot;) # you can try using all the pixels # zip.SOM &lt;- som(zip.train[, 2:257], # grid = somgrid(10, 10, &quot;rectangular&quot;)) # plot(zip.SOM, type = &quot;dist.neighbours&quot;) We could also look at the class labels (digits) coming out of the SOM. Particularly the plot on the right-hand side shows the proportion of subjects with each label for the subjects in each cluster (using a pie chart). set.seed(1) zip.SOM2 &lt;- xyf(zip.train[, seq(2, 257, length.out = 10)], classvec2classmat(zip.train[, 1]), grid = somgrid(10, 10, &quot;hexagonal&quot;), rlen = 300) par(mfrow = c(1, 2)) plot(zip.SOM2, type = &quot;codes&quot;, main = c(&quot;Codes X&quot;, &quot;Codes Y&quot;)) zip.SOM2.hc &lt;- cutree(hclust(dist(zip.SOM2$codes[[2]])), 10) add.cluster.boundaries(zip.SOM2, zip.SOM2.hc) Reference Kohonen, Teuvo. 1990. “The Self-Organizing Map.” Proceedings of the IEEE 78 (9): 1464–80. "],["spectral-clustering.html", "Chapter 24 Spectral Clustering 24.1 An Example 24.2 Adjacency Matrix 24.3 Laplacian Matrix 24.4 Derivation of the Feature Embedding 24.5 Feature Embedding 24.6 Clustering with Embedded Features 24.7 Normalized Graph Laplacian 24.8 Using a Different Adjacency Matrix", " Chapter 24 Spectral Clustering Spectral clustering aims at clustering observations based on their proximity information. It essentially consists of two steps. The first step is a feature embedding, or dimension reduction. We first construct the graph Laplacian \\(\\mathbf{L}\\) (or normalized version), which represent the proximity information, and perform eigen-decomposition of the matrix. This allows us to use a lower dimensional matrix to represent the proximity information of the original data. Once we have the low-dimensional data, we can perform the regular clustering algorithm, i.e., \\(k\\)-means on the new dataset. 24.1 An Example Let’s look at an example that regular \\(k\\)-means would fail. set.seed(1) n = 200 r = c(rep(1, n), rep(2, n), rep(3, n)) + runif(n*3, -0.1, 0.1) theta = runif(n) * 2 * pi x1 = r * cos(theta) x2 = r * sin(theta) X = cbind(x1, x2) plot(X) kmeanfit = kmeans(X, centers = 3) plot(X, col = kmeanfit$cluster + 1, pch = 19) Since \\(k\\)-means use Euclidean distance, it is not appropriate for such problems. 24.2 Adjacency Matrix Maybe we should use a nonlinear way to describe the distance/closeness between subjects. For example, let’s define two sample to be close if they are within the k-nearest neighbors of each other. We use \\(k = 10\\), and create an adjacency matrix \\(\\mathbf{W}\\). library(FNN) ## ## Attaching package: &#39;FNN&#39; ## The following objects are masked from &#39;package:class&#39;: ## ## knn, knn.cv W = matrix(0, 3*n, 3*n) # get neighbor index for each observation nn = get.knn(X, k=10) # write into W for (i in 1:(3*n)) W[i, nn$nn.index[i, ]] = 1 # W is not necessary symmetric W = 0.5*(W + t(W)) # we may also use # W = pmax(W, t(W)) Let’s use a heatmap to display what the adjacency information look like. Please note that our data are ordered with clusters 1, 2 and 3. # plot the adjacency matrix heatmap(W, Rowv = NA, Colv=NA, symm = TRUE, revC = TRUE) 24.3 Laplacian Matrix The next step is to calculate the Laplacian \\[\\mathbf{L} = \\mathbf{D} - \\mathbf{W},\\] where \\(\\mathbf{D}\\) is a diagonal matrix with its elements equation to the row sums (or column sums) of \\(\\mathbf{W}\\). We will then perform eigen-decomposition on \\(\\mathbf{L}\\) to extract/define some underlying features. # compute the degree of each vertex d = colSums(W) # the laplacian matrix L = diag(d) - W 24.4 Derivation of the Feature Embedding Assuming that we want to create artificial embedded features that reflects the underlying structure of the data, especially the graph structure. This can be done through an eigen-decomposition of the Laplacian matrix. To see this, let’s assume that the new artificial embedded feature would assign a value \\(f_i\\) to each observation \\(i\\). Hence, we want \\(f_i\\) to be close to \\(f_j\\) if \\(i\\) and \\(j\\) are close in the graph, i.e., when \\(w_{ij}\\) is large. This can be achieved by \\[\\underset{f}{\\min} \\sum_{i,j} w_{ij} (f_i - f_j)^2.\\] To solve this problem, we start with the following matrix form \\[\\begin{align*} f^\\text{T}\\mathbf{L} f &amp;= f^\\text{T}\\mathbf{D} f - f^\\text{T}\\mathbf{W} f \\\\ &amp;= \\sum_{i} d_{i} f_i^2 - \\sum_{i,j} w_{ij} f_i f_j \\\\ &amp;= \\frac{1}{2} \\bigg\\{ \\sum_{ij} w_{ij} f_i^2 - 2 \\sum_{i,j} \\mathbf{w}_{ij} f_i f_j + \\sum_{ij} w_{ij} f_j^2 \\bigg\\}\\\\ &amp;= \\frac{1}{2} \\sum_{ij} w_{ij} (f_i - f_j)^2. \\end{align*}\\] Hence, finding the embedded feature simply becomes an eigen-decomposition problem, i.e., getting the smallest eigen-values of \\(\\mathbf{L}\\). However, it should be noted that when all \\(f_i\\)’s are identical, \\(f^\\text{T}\\mathbf{L} f\\) would be zero. Hence, we should be using the second smallest eigen-values. # eigen-decomposition f = eigen(L, symmetric = TRUE) # plot the smallest eigen-values # there are three zero eigen-values, why? plot(rev(f$values)[1:10], pch = 19, ylab = &quot;eigen-values&quot;, col = c(rep(&quot;red&quot;, 3), rep(&quot;blue&quot;, 17))) 24.5 Feature Embedding In fact the smallest eigen-value will always be zero. However, we can use the eigen-vectors associated with the second and third smallest eigen-values. They define some feature embedding or dimension reduction to represent the original data. Since we know the underlying model, two dimensions are enough (to separate three clusters). However, based on the eigen-value plot, there is a big gap between the third and the fourth one. Hence, we only need three. Further removing the smallest one, only two are needed. par(mfrow=c(1,2)) plot(f$vectors[, length(f$values)-1], type = &quot;l&quot;, ylab = &quot;eigen-values&quot;, ylim = c(-0.1, 0.1)) plot(f$vectors[, length(f$values)-2], type = &quot;l&quot;, ylab = &quot;eigen-values&quot;, ylim = c(-0.1, 0.1)) 24.6 Clustering with Embedded Features We can then perform \\(k\\)-means on these two new features. And it will give us the correct clustering. par(mfrow=c(1,1)) scfit = kmeans(f$vectors[, (length(f$values)-2) : (length(f$values)-1) ], centers = 3, nstart = 20) plot(X, col = scfit$cluster + 1, pch = 19) 24.7 Normalized Graph Laplacian There are other choices of the Laplacian matrix. For example, a normalized graph Laplacian is defined as \\[\\mathbf{L}_\\text{sym} = \\mathbf{I} - \\mathbf{D^{-1/2} \\mathbf{W} D^{-1/2}}\\] For our problem, it achieves pretty much the same effect. # the normed laplacian matrix L = diag(nrow(W)) - diag(1/sqrt(d)) %*% W %*% diag(1/sqrt(d)) # eigen-decomposition f = eigen(L, symmetric = TRUE) # perform clustering scfit = kmeans(f$vectors[, (length(f$values)-2) : (length(f$values)-1) ], centers = 3, nstart = 20) plot(X, col = scfit$cluster + 1, pch = 19) 24.8 Using a Different Adjacency Matrix Let’s also try to use the Gaussian kernel to define the adjacency matrix. Here, tuning (the bandwidth parameter in the kernel function) becomes important for the performance. # the Gaussian kernel W = exp(-as.matrix(dist(X/0.2))^2) # view the adjacency matrix heatmap(W, Rowv = NA, Colv=NA, symm = TRUE, revC = TRUE) # the normed laplacian matrix L = diag(colSums(W)) - W # eigen-decomposition f = eigen(L, symmetric = TRUE) # perform clustering scfit = kmeans(f$vectors[, (length(f$values)-2) : (length(f$values)-1) ], centers = 3, nstart = 20) plot(X, col = scfit$cluster + 1, pch = 19) "],["uniform-manifold-approximation-and-projection.html", "Chapter 25 Uniform Manifold Approximation and Projection 25.1 An Example 25.2 Tuning 25.3 Another Example", " Chapter 25 Uniform Manifold Approximation and Projection Uniform Manifold Approximation and Projection (UMAP, McInnes, Healy, and Melville (2018)) becomes a very popular feature embedding / dimension reduction algorithm. If you have finished the spectral clustering section, then these concepts shouldn’t be new. In fact, PCA is also a similar approach, but its just linear in terms of the original features. There are two methods that are worth to mention here, t-SNE (Van der Maaten and Hinton 2008) and spectral embedding (the embedding step in spectral clustering). All of these methods are graph-based, meaning that they are trying to learn an embedding space such that the pair-wise geometric distances among subjects in this embedding space is “similar” to the graph defined in the original data. Here, an example of the graph is the k-nearest neighbor graph (for spectral clustering), which counts 1 if two subjects are within each others neighbors. But for t-SNE, the graph values are proportional to the kernel density function between two points with a t-distribution density function. The difference among these method is mainly on how do they define “similar”. In UMAP, this similarity is defined by a type of cross-entropy, while for spectral clustering, its the eigen-values, meaning the matrix approximation, and for t-SNE, its based on the Kullback-Leibler divergence. 25.1 An Example Let’s consider our example from the spectral clustering lecture. set.seed(1) n = 200 r = c(rep(1, n), rep(2, n), rep(3, n)) + runif(n*3, -0.1, 0.1) theta = runif(n) * 2 * pi x1 = r * cos(theta) x2 = r * sin(theta) X = cbind(x1, x2) plot(X) circle.labels = c(rep(1, n), rep(2, n), rep(3, n)) We can perform UMAP using the default tuning. This will create a two-dimensional embedding. library(umap) circles.umap = umap(X) circles.umap ## umap embedding of 600 items in 2 dimensions ## object components: layout, data, knn, config plot(circles.umap$layout, col = circle.labels) We can see that UMAP learns these new features, which groups similar observations together. Its reasonable to expect that if we perform any clustering algorithm on these new embedded features, we will recover the truth. 25.2 Tuning UMAP involves a lot of tuning parameters and the most significant one concerns about how we create the (KNN) graph in the first step. You can see the summary of all tuning parameters: umap.defaults ## umap configuration parameters ## n_neighbors: 15 ## n_components: 2 ## metric: euclidean ## n_epochs: 200 ## input: data ## init: spectral ## min_dist: 0.1 ## set_op_mix_ratio: 1 ## local_connectivity: 1 ## bandwidth: 1 ## alpha: 1 ## gamma: 1 ## negative_sample_rate: 5 ## a: NA ## b: NA ## spread: 1 ## random_state: NA ## transform_state: NA ## knn: NA ## knn_repeats: 1 ## verbose: FALSE ## umap_learn_args: NA To change the default value, we can do the following myumap.tuning = umap.defaults umap.defaults$n_neighbors = 5 circles.umap = umap(X, umap.defaults) plot(circles.umap$layout, col = circle.labels) You can see that the result is not as perfect as we wanted. It seems that there are more groups, although each group only involves one type of data. There are other parameter you may consider tuning. For example n_components controls how many dimensions you reduced data should have. Usually we don’t use values larger than three, but this is very problem specific. 25.3 Another Example Let’s use UMAP on a larger data, the hand written digit data. We will perform clustering on just the pixels. We can also predict the embedding feature values for future observations. We can see that both recovers the true labels pretty well. library(ElemStatLearn) dim(zip.train) ## [1] 7291 257 dim(zip.test) ## [1] 2007 257 zip.umap = umap(zip.train[, -1]) zip.pred = predict(zip.umap, zip.test[, -1]) plot(zip.umap$layout, col = zip.train[, 1]+1, pch = 19, cex = 0.2) plot(zip.pred, col = zip.test[, 1]+1, pch = 19, cex = 0.2) Reference McInnes, Leland, John Healy, and James Melville. 2018. “Umap: Uniform Manifold Approximation and Projection for Dimension Reduction.” arXiv Preprint arXiv:1802.03426. Van der Maaten, Laurens, and Geoffrey Hinton. 2008. “Visualizing Data Using t-SNE.” Journal of Machine Learning Research 9 (11). "],["reference.html", "Chapter 26 Reference", " Chapter 26 Reference Aronszajn, Nachman. 1950. “Theory of Reproducing Kernels.” Transactions of the American Mathematical Society 68 (3): 337–404. Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992. “A Training Algorithm for Optimal Margin Classifiers.” In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144–52. Boyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge university press. Breiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40. ———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32. Breiman, Leo, Jerome H Friedman, Richard A Olshen, and Charles J Stone. 1984. Classification and Regression Trees. Monterey, CA: Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software. Cayton, Lawrence. 2005. “Algorithms for Manifold Learning.” Univ. Of California at San Diego Tech. Rep 12 (1-17): 1. Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” Machine Learning 20 (3): 273–97. Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. “Least Angle Regression.” The Annals of Statistics 32 (2): 407–99. Freund, Yoav, and Robert E Schapire. 1997. “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.” Journal of Computer and System Sciences 55 (1): 119–39. Friedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics, 1189–1232. Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33 (1): 1. Golub, Gene H, Michael Heath, and Grace Wahba. 1979. “Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter.” Technometrics 21 (2): 215–23. Ho, Tin Kam. 1998. “The Random Subspace Method for Constructing Decision Forests.” IEEE Transactions on Pattern Analysis and Machine Intelligence 20 (8): 832–44. Hoerl, Arthur E, and Robert W Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67. Kimeldorf, George S, and Grace Wahba. 1970. “A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines.” The Annals of Mathematical Statistics 41 (2): 495–502. Kohonen, Teuvo. 1990. “The Self-Organizing Map.” Proceedings of the IEEE 78 (9): 1464–80. Li, Ker-Chau. 1991. “Sliced Inverse Regression for Dimension Reduction.” Journal of the American Statistical Association 86 (414): 316–27. McInnes, Leland, John Healy, and James Melville. 2018. “Umap: Uniform Manifold Approximation and Projection for Dimension Reduction.” arXiv Preprint arXiv:1802.03426. Mercer, James. 1909. “Xvi. Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 209 (441-458): 415–46. Nocedal, Jorge, and Stephen Wright. 2006. Numerical Optimization. Springer Science &amp; Business Media. Quinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Elsevier. Scornet, Erwan. 2016. “Random Forests and Kernel Methods.” IEEE Transactions on Information Theory 62 (3): 1485–1500. Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. Van der Maaten, Laurens, and Geoffrey Hinton. 2008. “Visualizing Data Using t-SNE.” Journal of Machine Learning Research 9 (11). Yeh, I-Cheng, and Tzu-Kuang Hsu. 2018. “Building Real Estate Valuation Models with Comparative Approach Through Case-Based Reasoning.” Applied Soft Computing 65: 260–71. Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
