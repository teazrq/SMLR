<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Logistic Regression | Statistical Machine Learning with R</title>
  <meta name="description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Logistic Regression | Statistical Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  <meta name="github-repo" content="teazrq/SMLR" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Logistic Regression | Statistical Machine Learning with R" />
  
  <meta name="twitter:description" content="A Textbook for Statistical Machine Learning Courses at UIUC" />
  

<meta name="author" content="Ruoqing Zhu, PhD" />


<meta name="date" content="2025-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="spline.html"/>
<link rel="next" href="discriminant-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.33/datatables.js"></script>
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize-0.12.0/selectize.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<script src="libs/plotly-binding-4.11.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Machine Learning with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#target-audience"><i class="fa fa-check"></i>Target Audience</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#whats-covered"><i class="fa fa-check"></i>What’s Covered?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="part"><span><b>I Basics Knowledge</b></span></li>
<li class="chapter" data-level="1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html"><i class="fa fa-check"></i><b>1</b> R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#installing-r-and-rstudio"><i class="fa fa-check"></i><b>1.1</b> Installing R and RStudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#r-basic"><i class="fa fa-check"></i><b>1.2</b> Resources and Guides</a></li>
<li class="chapter" data-level="1.3" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#basic-mathematical-operations"><i class="fa fa-check"></i><b>1.3</b> Basic Mathematical Operations</a></li>
<li class="chapter" data-level="1.4" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#data-objects"><i class="fa fa-check"></i><b>1.4</b> Data Objects</a></li>
<li class="chapter" data-level="1.5" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#readin-and-save-data"><i class="fa fa-check"></i><b>1.5</b> Readin and save data</a></li>
<li class="chapter" data-level="1.6" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-and-defining-functions"><i class="fa fa-check"></i><b>1.6</b> Using and defining functions</a></li>
<li class="chapter" data-level="1.7" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#distribution-and-random-numbers"><i class="fa fa-check"></i><b>1.7</b> Distribution and random numbers</a></li>
<li class="chapter" data-level="1.8" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#using-packages-and-other-resources"><i class="fa fa-check"></i><b>1.8</b> Using packages and other resources</a></li>
<li class="chapter" data-level="1.9" data-path="r-and-rstudio.html"><a href="r-and-rstudio.html#practice-questions"><i class="fa fa-check"></i><b>1.9</b> Practice questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>2</b> RMarkdown</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rmarkdown.html"><a href="rmarkdown.html#basics-and-resources"><i class="fa fa-check"></i><b>2.1</b> Basics and Resources</a></li>
<li class="chapter" data-level="2.2" data-path="rmarkdown.html"><a href="rmarkdown.html#formatting-text"><i class="fa fa-check"></i><b>2.2</b> Formatting Text</a></li>
<li class="chapter" data-level="2.3" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-r-code"><i class="fa fa-check"></i><b>2.3</b> Adding <code>R</code> Code</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rmarkdown.html"><a href="rmarkdown.html#r-chunks"><i class="fa fa-check"></i><b>2.3.1</b> <code>R</code> Chunks</a></li>
<li class="chapter" data-level="2.3.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-r"><i class="fa fa-check"></i><b>2.3.2</b> Inline <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rmarkdown.html"><a href="rmarkdown.html#importing-data"><i class="fa fa-check"></i><b>2.4</b> Importing Data</a></li>
<li class="chapter" data-level="2.5" data-path="rmarkdown.html"><a href="rmarkdown.html#working-directory"><i class="fa fa-check"></i><b>2.5</b> Working Directory</a></li>
<li class="chapter" data-level="2.6" data-path="rmarkdown.html"><a href="rmarkdown.html#plotting"><i class="fa fa-check"></i><b>2.6</b> Plotting</a></li>
<li class="chapter" data-level="2.7" data-path="rmarkdown.html"><a href="rmarkdown.html#chunk-options"><i class="fa fa-check"></i><b>2.7</b> Chunk Options</a></li>
<li class="chapter" data-level="2.8" data-path="rmarkdown.html"><a href="rmarkdown.html#adding-math-with-latex"><i class="fa fa-check"></i><b>2.8</b> Adding Math with LaTeX</a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="rmarkdown.html"><a href="rmarkdown.html#displaystyle-latex"><i class="fa fa-check"></i><b>2.8.1</b> Displaystyle LaTeX</a></li>
<li class="chapter" data-level="2.8.2" data-path="rmarkdown.html"><a href="rmarkdown.html#inline-latex"><i class="fa fa-check"></i><b>2.8.2</b> Inline LaTex</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="rmarkdown.html"><a href="rmarkdown.html#output-options"><i class="fa fa-check"></i><b>2.9</b> Output Options</a></li>
<li class="chapter" data-level="2.10" data-path="rmarkdown.html"><a href="rmarkdown.html#try-it"><i class="fa fa-check"></i><b>2.10</b> Try It!</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="visual-studio-code.html"><a href="visual-studio-code.html"><i class="fa fa-check"></i><b>3</b> Visual Studio Code</a>
<ul>
<li class="chapter" data-level="3.1" data-path="visual-studio-code.html"><a href="visual-studio-code.html#basics-and-resources-1"><i class="fa fa-check"></i><b>3.1</b> Basics and Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra Basics</a>
<ul>
<li class="chapter" data-level="4.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#definition"><i class="fa fa-check"></i><b>4.1</b> Definition</a></li>
<li class="chapter" data-level="4.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linear-regression"><i class="fa fa-check"></i><b>4.2</b> Linear Regression</a></li>
<li class="chapter" data-level="4.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#matrix-inversion"><i class="fa fa-check"></i><b>4.3</b> Matrix Inversion</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#linearalgebra-SM"><i class="fa fa-check"></i><b>4.3.1</b> Rank-one Update</a></li>
<li class="chapter" data-level="4.3.2" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#rank-k-update"><i class="fa fa-check"></i><b>4.3.2</b> Rank-<span class="math inline">\(k\)</span> Update</a></li>
<li class="chapter" data-level="4.3.3" data-path="linear-algebra-basics.html"><a href="linear-algebra-basics.html#times-2-block-matrix-inversion"><i class="fa fa-check"></i><b>4.3.3</b> 2 <span class="math inline">\(\times\)</span> 2 Block Matrix Inversion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="optimization-basics.html"><a href="optimization-basics.html"><i class="fa fa-check"></i><b>5</b> Optimization Basics</a>
<ul>
<li class="chapter" data-level="5.1" data-path="optimization-basics.html"><a href="optimization-basics.html#basic-concept"><i class="fa fa-check"></i><b>5.1</b> Basic Concept</a></li>
<li class="chapter" data-level="5.2" data-path="optimization-basics.html"><a href="optimization-basics.html#global_local"><i class="fa fa-check"></i><b>5.2</b> Global vs. Local Optima</a></li>
<li class="chapter" data-level="5.3" data-path="optimization-basics.html"><a href="optimization-basics.html#example-linear-regression-using-optim"><i class="fa fa-check"></i><b>5.3</b> Example: Linear Regression using <code>optim()</code></a></li>
<li class="chapter" data-level="5.4" data-path="optimization-basics.html"><a href="optimization-basics.html#first-and-second-order-properties"><i class="fa fa-check"></i><b>5.4</b> First and Second Order Properties</a></li>
<li class="chapter" data-level="5.5" data-path="optimization-basics.html"><a href="optimization-basics.html#algorithm"><i class="fa fa-check"></i><b>5.5</b> Algorithm</a></li>
<li class="chapter" data-level="5.6" data-path="optimization-basics.html"><a href="optimization-basics.html#second-order-methods"><i class="fa fa-check"></i><b>5.6</b> Second-order Methods</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="optimization-basics.html"><a href="optimization-basics.html#newtons-method"><i class="fa fa-check"></i><b>5.6.1</b> Newton’s Method</a></li>
<li class="chapter" data-level="5.6.2" data-path="optimization-basics.html"><a href="optimization-basics.html#quasi-newton-methods"><i class="fa fa-check"></i><b>5.6.2</b> Quasi-Newton Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="optimization-basics.html"><a href="optimization-basics.html#first-order-methods"><i class="fa fa-check"></i><b>5.7</b> First-order Methods</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent"><i class="fa fa-check"></i><b>5.7.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="5.7.2" data-path="optimization-basics.html"><a href="optimization-basics.html#gradient-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.7.2</b> Gradient Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate"><i class="fa fa-check"></i><b>5.8</b> Coordinate Descent</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="optimization-basics.html"><a href="optimization-basics.html#coordinate-descent-example-linear-regression"><i class="fa fa-check"></i><b>5.8.1</b> Coordinate Descent Example: Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="optimization-basics.html"><a href="optimization-basics.html#stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9</b> Stocastic Gradient Descent</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="optimization-basics.html"><a href="optimization-basics.html#mini-batch-stocastic-gradient-descent"><i class="fa fa-check"></i><b>5.9.1</b> Mini-batch Stocastic Gradient Descent</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="optimization-basics.html"><a href="optimization-basics.html#lagrangian-multiplier-for-constrained-problems"><i class="fa fa-check"></i><b>5.10</b> Lagrangian Multiplier for Constrained Problems</a></li>
</ul></li>
<li class="part"><span><b>II Linear and Penalized Linear Models</b></span></li>
<li class="chapter" data-level="6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html"><i class="fa fa-check"></i><b>6</b> Linear Regression and Model Selection</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#example-real-estate-data"><i class="fa fa-check"></i><b>6.1</b> Example: real estate data</a></li>
<li class="chapter" data-level="6.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#notation-and-basic-properties"><i class="fa fa-check"></i><b>6.2</b> Notation and Basic Properties</a></li>
<li class="chapter" data-level="6.3" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-the-lm-function"><i class="fa fa-check"></i><b>6.3</b> Using the <code>lm()</code> Function</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#adding-covariates"><i class="fa fa-check"></i><b>6.3.1</b> Adding Covariates</a></li>
<li class="chapter" data-level="6.3.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#categorical-variables"><i class="fa fa-check"></i><b>6.3.2</b> Categorical Variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-criteria"><i class="fa fa-check"></i><b>6.4</b> Model Selection Criteria</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-marrows-c_p"><i class="fa fa-check"></i><b>6.4.1</b> Using Marrows’ <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="6.4.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#using-aic-and-bic"><i class="fa fa-check"></i><b>6.4.2</b> Using AIC and BIC</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#model-selection-algorithms"><i class="fa fa-check"></i><b>6.5</b> Model Selection Algorithms</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#best-subset-selection-with-leaps"><i class="fa fa-check"></i><b>6.5.1</b> Best Subset Selection with <code>leaps</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#step-wise-regression-using-step"><i class="fa fa-check"></i><b>6.5.2</b> Step-wise regression using <code>step()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="linear-regression-and-model-selection.html"><a href="linear-regression-and-model-selection.html#marrows-cp"><i class="fa fa-check"></i><b>6.6</b> Derivation of Marrows’ <span class="math inline">\(C_p\)</span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>7</b> Ridge Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#motivation-correlated-variables-and-convexity"><i class="fa fa-check"></i><b>7.1</b> Motivation: Correlated Variables and Convexity</a></li>
<li class="chapter" data-level="7.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-penalty-and-the-reduced-variation"><i class="fa fa-check"></i><b>7.2</b> Ridge Penalty and the Reduced Variation</a></li>
<li class="chapter" data-level="7.3" data-path="ridge-regression.html"><a href="ridge-regression.html#bias-and-variance-of-ridge-regression"><i class="fa fa-check"></i><b>7.3</b> Bias and Variance of Ridge Regression</a></li>
<li class="chapter" data-level="7.4" data-path="ridge-regression.html"><a href="ridge-regression.html#degrees-of-freedom"><i class="fa fa-check"></i><b>7.4</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="7.5" data-path="ridge-regression.html"><a href="ridge-regression.html#using-the-lm.ridge-function"><i class="fa fa-check"></i><b>7.5</b> Using the <code>lm.ridge()</code> function</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue"><i class="fa fa-check"></i><b>7.5.1</b> Scaling Issue</a></li>
<li class="chapter" data-level="7.5.2" data-path="ridge-regression.html"><a href="ridge-regression.html#multiple-lambda-values"><i class="fa fa-check"></i><b>7.5.2</b> Multiple <span class="math inline">\(\lambda\)</span> values</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ridge-regression.html"><a href="ridge-regression.html#cross-validation"><i class="fa fa-check"></i><b>7.6</b> Cross-validation</a></li>
<li class="chapter" data-level="7.7" data-path="ridge-regression.html"><a href="ridge-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.7</b> Leave-one-out cross-validation</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="ridge-regression.html"><a href="ridge-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>7.7.1</b> Generalized cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ridge-regression.html"><a href="ridge-regression.html#the-glmnet-package"><i class="fa fa-check"></i><b>7.8</b> The <code>glmnet</code> package</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="ridge-regression.html"><a href="ridge-regression.html#scaling-issue-1"><i class="fa fa-check"></i><b>7.8.1</b> Scaling Issue</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>8</b> Lasso</a>
<ul>
<li class="chapter" data-level="8.1" data-path="lasso.html"><a href="lasso.html#one-variable-lasso-and-shrinkage"><i class="fa fa-check"></i><b>8.1</b> One-Variable Lasso and Shrinkage</a></li>
<li class="chapter" data-level="8.2" data-path="lasso.html"><a href="lasso.html#constrained-optimization-view"><i class="fa fa-check"></i><b>8.2</b> Constrained Optimization View</a></li>
<li class="chapter" data-level="8.3" data-path="lasso.html"><a href="lasso.html#the-solution-path"><i class="fa fa-check"></i><b>8.3</b> The Solution Path</a></li>
<li class="chapter" data-level="8.4" data-path="lasso.html"><a href="lasso.html#path-wise-coordinate-descent"><i class="fa fa-check"></i><b>8.4</b> Path-wise Coordinate Descent</a></li>
<li class="chapter" data-level="8.5" data-path="lasso.html"><a href="lasso.html#using-the-glmnet-package"><i class="fa fa-check"></i><b>8.5</b> Using the <code>glmnet</code> package</a></li>
<li class="chapter" data-level="8.6" data-path="lasso.html"><a href="lasso.html#elastic-net"><i class="fa fa-check"></i><b>8.6</b> Elastic-Net</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="spline.html"><a href="spline.html"><i class="fa fa-check"></i><b>9</b> Spline</a>
<ul>
<li class="chapter" data-level="9.1" data-path="spline.html"><a href="spline.html#using-linear-models-for-nonlinear-trends"><i class="fa fa-check"></i><b>9.1</b> Using Linear models for Nonlinear Trends</a></li>
<li class="chapter" data-level="9.2" data-path="spline.html"><a href="spline.html#a-motivating-example-and-polynomials"><i class="fa fa-check"></i><b>9.2</b> A Motivating Example and Polynomials</a></li>
<li class="chapter" data-level="9.3" data-path="spline.html"><a href="spline.html#piecewise-polynomials"><i class="fa fa-check"></i><b>9.3</b> Piecewise Polynomials</a></li>
<li class="chapter" data-level="9.4" data-path="spline.html"><a href="spline.html#splines"><i class="fa fa-check"></i><b>9.4</b> Splines</a></li>
<li class="chapter" data-level="9.5" data-path="spline.html"><a href="spline.html#spline-basis"><i class="fa fa-check"></i><b>9.5</b> Spline Basis</a></li>
<li class="chapter" data-level="9.6" data-path="spline.html"><a href="spline.html#natural-cubic-spline"><i class="fa fa-check"></i><b>9.6</b> Natural Cubic Spline</a></li>
<li class="chapter" data-level="9.7" data-path="spline.html"><a href="spline.html#smoothing-spline"><i class="fa fa-check"></i><b>9.7</b> Smoothing Spline</a></li>
<li class="chapter" data-level="9.8" data-path="spline.html"><a href="spline.html#fitting-smoothing-splines"><i class="fa fa-check"></i><b>9.8</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="9.9" data-path="spline.html"><a href="spline.html#extending-splines-to-multiple-varibles"><i class="fa fa-check"></i><b>9.9</b> Extending Splines to Multiple Varibles</a></li>
</ul></li>
<li class="part"><span><b>III Linear Classification Models</b></span></li>
<li class="chapter" data-level="10" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>10</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="logistic-regression.html"><a href="logistic-regression.html#modeling-binary-outcomes"><i class="fa fa-check"></i><b>10.1</b> Modeling Binary Outcomes</a></li>
<li class="chapter" data-level="10.2" data-path="logistic-regression.html"><a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data"><i class="fa fa-check"></i><b>10.2</b> Example: Cleveland Clinic Heart Disease Data</a></li>
<li class="chapter" data-level="10.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>10.3</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="10.4" data-path="logistic-regression.html"><a href="logistic-regression.html#solving-a-logistic-regression"><i class="fa fa-check"></i><b>10.4</b> Solving a Logistic Regression</a></li>
<li class="chapter" data-level="10.5" data-path="logistic-regression.html"><a href="logistic-regression.html#example-south-africa-heart-data"><i class="fa fa-check"></i><b>10.5</b> Example: South Africa Heart Data</a></li>
<li class="chapter" data-level="10.6" data-path="logistic-regression.html"><a href="logistic-regression.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.6</b> Penalized Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>11</b> Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="11.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#bayes-rule"><i class="fa fa-check"></i><b>11.1</b> Bayes Rule</a></li>
<li class="chapter" data-level="11.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>11.2</b> Example: Linear Discriminant Analysis (LDA)</a></li>
<li class="chapter" data-level="11.3" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>11.3</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="11.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>11.4</b> Example: Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="11.5" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>11.5</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#example-the-hand-written-digit-data"><i class="fa fa-check"></i><b>11.6</b> Example: the Hand Written Digit Data</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Models</b></span></li>
<li class="chapter" data-level="12" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html"><i class="fa fa-check"></i><b>12</b> K-Neariest Neighber</a>
<ul>
<li class="chapter" data-level="12.1" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-k"><i class="fa fa-check"></i><b>12.2</b> Tuning <span class="math inline">\(k\)</span></a></li>
<li class="chapter" data-level="12.3" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>12.3</b> The Bias-variance Trade-off</a></li>
<li class="chapter" data-level="12.4" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#knn-for-classification"><i class="fa fa-check"></i><b>12.4</b> KNN for Classification</a></li>
<li class="chapter" data-level="12.5" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-1-an-artificial-data"><i class="fa fa-check"></i><b>12.5</b> Example 1: An artificial data</a></li>
<li class="chapter" data-level="12.6" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#degrees-of-freedom-1"><i class="fa fa-check"></i><b>12.6</b> Degrees of Freedom</a></li>
<li class="chapter" data-level="12.7" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#tuning-with-the-caret-package"><i class="fa fa-check"></i><b>12.7</b> Tuning with the <code>caret</code> Package</a></li>
<li class="chapter" data-level="12.8" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#distance-measures"><i class="fa fa-check"></i><b>12.8</b> Distance Measures</a></li>
<li class="chapter" data-level="12.9" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#nn-error-bound"><i class="fa fa-check"></i><b>12.9</b> 1NN Error Bound</a></li>
<li class="chapter" data-level="12.10" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#example-2-handwritten-digit-data"><i class="fa fa-check"></i><b>12.10</b> Example 2: Handwritten Digit Data</a></li>
<li class="chapter" data-level="12.11" data-path="k-neariest-neighber.html"><a href="k-neariest-neighber.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>12.11</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html"><i class="fa fa-check"></i><b>13</b> Kernel Smoothing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#knn-vs.-kernel"><i class="fa fa-check"></i><b>13.1</b> KNN vs. Kernel</a></li>
<li class="chapter" data-level="13.2" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#kernel-density-estimations"><i class="fa fa-check"></i><b>13.2</b> Kernel Density Estimations</a></li>
<li class="chapter" data-level="13.3" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#expectation-of-the-parzen-estimator"><i class="fa fa-check"></i><b>13.3</b> Expectation of the Parzen estimator</a></li>
<li class="chapter" data-level="13.4" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#gaussian-kernel-regression"><i class="fa fa-check"></i><b>13.4</b> Gaussian Kernel Regression</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>13.4.1</b> Bias-variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#choice-of-kernel-functions"><i class="fa fa-check"></i><b>13.5</b> Choice of Kernel Functions</a></li>
<li class="chapter" data-level="13.6" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-linear-regression"><i class="fa fa-check"></i><b>13.6</b> Local Linear Regression</a></li>
<li class="chapter" data-level="13.7" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#local-polynomial-regression"><i class="fa fa-check"></i><b>13.7</b> Local Polynomial Regression</a></li>
<li class="chapter" data-level="13.8" data-path="kernel-smoothing.html"><a href="kernel-smoothing.html#r-implementations"><i class="fa fa-check"></i><b>13.8</b> R Implementations</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="nonpara.html"><a href="nonpara.html"><i class="fa fa-check"></i><b>14</b> Nonparemetric Estimation Rates</a>
<ul>
<li class="chapter" data-level="14.1" data-path="nonpara.html"><a href="nonpara.html#kernel-density-estimation"><i class="fa fa-check"></i><b>14.1</b> Kernel Density Estimation</a></li>
<li class="chapter" data-level="14.2" data-path="nonpara.html"><a href="nonpara.html#the-effect-of-smoothness"><i class="fa fa-check"></i><b>14.2</b> The Effect of Smoothness</a></li>
<li class="chapter" data-level="14.3" data-path="nonpara.html"><a href="nonpara.html#the-effect-of-dimensionality"><i class="fa fa-check"></i><b>14.3</b> The Effect of Dimensionality</a></li>
<li class="chapter" data-level="14.4" data-path="nonpara.html"><a href="nonpara.html#nadaraya-watson-regression-estimator"><i class="fa fa-check"></i><b>14.4</b> Nadaraya-Watson Regression Estimator</a></li>
</ul></li>
<li class="part"><span><b>V Kernel Machines</b></span></li>
<li class="chapter" data-level="15" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html"><i class="fa fa-check"></i><b>15</b> Reproducing Kernel Hilbert Space</a>
<ul>
<li class="chapter" data-level="15.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-motivation"><i class="fa fa-check"></i><b>15.1</b> The Motivation</a></li>
<li class="chapter" data-level="15.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#hilbert-space-preliminaries"><i class="fa fa-check"></i><b>15.2</b> Hilbert Space Preliminaries</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-space-of-square-integrable-functions"><i class="fa fa-check"></i><b>15.2.1</b> The Space of Square-Integrable Functions</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-kernel-function"><i class="fa fa-check"></i><b>15.3</b> A Kernel Function</a></li>
<li class="chapter" data-level="15.4" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#a-space-of-functions"><i class="fa fa-check"></i><b>15.4</b> A Space of Functions</a></li>
<li class="chapter" data-level="15.5" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-inner-product"><i class="fa fa-check"></i><b>15.5</b> The Inner Product</a></li>
<li class="chapter" data-level="15.6" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-rkhs"><i class="fa fa-check"></i><b>15.6</b> The RKHS</a></li>
<li class="chapter" data-level="15.7" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-reproducing-property"><i class="fa fa-check"></i><b>15.7</b> The Reproducing Property</a></li>
<li class="chapter" data-level="15.8" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#smoothness"><i class="fa fa-check"></i><b>15.8</b> Smoothness</a></li>
<li class="chapter" data-level="15.9" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#the-moorearonszajn-theorem"><i class="fa fa-check"></i><b>15.9</b> The Moore–Aronszajn Theorem</a></li>
<li class="chapter" data-level="15.10" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#examples"><i class="fa fa-check"></i><b>15.10</b> Examples</a>
<ul>
<li class="chapter" data-level="15.10.1" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#brownian-motion-kernel"><i class="fa fa-check"></i><b>15.10.1</b> Brownian Motion Kernel</a></li>
<li class="chapter" data-level="15.10.2" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#non-positive-definite-kernel"><i class="fa fa-check"></i><b>15.10.2</b> Non-positive Definite Kernel</a></li>
<li class="chapter" data-level="15.10.3" data-path="reproducing-kernel-hilbert-space.html"><a href="reproducing-kernel-hilbert-space.html#defining-new-kernels"><i class="fa fa-check"></i><b>15.10.3</b> Defining New Kernels</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html"><i class="fa fa-check"></i><b>16</b> Kernel Ridge Regression</a>
<ul>
<li class="chapter" data-level="16.1" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#linear-regression-as-a-constraint-optimization"><i class="fa fa-check"></i><b>16.1</b> Linear Regression as a Constraint Optimization</a></li>
<li class="chapter" data-level="16.2" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#the-kernel-ridge-regression"><i class="fa fa-check"></i><b>16.2</b> The Kernel Ridge Regression</a></li>
<li class="chapter" data-level="16.3" data-path="kernel-ridge-regression.html"><a href="kernel-ridge-regression.html#ridge-regression-as-a-linear-kernel-model"><i class="fa fa-check"></i><b>16.3</b> Ridge Regression as a Linear Kernel Model</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>17</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="17.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>17.1</b> Maximum-margin Classifier</a></li>
<li class="chapter" data-level="17.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-separable-svm"><i class="fa fa-check"></i><b>17.2</b> Linearly Separable SVM</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#from-primal-to-dual"><i class="fa fa-check"></i><b>17.2.1</b> From Primal to Dual</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#linearly-non-separable-svm-with-slack-variables"><i class="fa fa-check"></i><b>17.3</b> Linearly Non-separable SVM with Slack Variables</a></li>
<li class="chapter" data-level="17.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-saheart-data"><i class="fa fa-check"></i><b>17.4</b> Example: <code>SAheart</code> Data</a></li>
<li class="chapter" data-level="17.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-svm-via-kernel-trick"><i class="fa fa-check"></i><b>17.5</b> Nonlinear SVM via Kernel Trick</a></li>
<li class="chapter" data-level="17.6" data-path="support-vector-machines.html"><a href="support-vector-machines.html#example-mixture.example-data"><i class="fa fa-check"></i><b>17.6</b> Example: <code>mixture.example</code> Data</a></li>
<li class="chapter" data-level="17.7" data-path="support-vector-machines.html"><a href="support-vector-machines.html#svm-as-a-penalized-model"><i class="fa fa-check"></i><b>17.7</b> SVM as a Penalized Model</a></li>
<li class="chapter" data-level="17.8" data-path="support-vector-machines.html"><a href="support-vector-machines.html#kernel-and-feature-maps-another-example"><i class="fa fa-check"></i><b>17.8</b> Kernel and Feature Maps: Another Example</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html"><i class="fa fa-check"></i><b>18</b> The Representer Theorem</a>
<ul>
<li class="chapter" data-level="18.1" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#the-representer-theorem-1"><i class="fa fa-check"></i><b>18.1</b> The Representer Theorem</a></li>
<li class="chapter" data-level="18.2" data-path="the-representer-theorem.html"><a href="the-representer-theorem.html#notes-on-application"><i class="fa fa-check"></i><b>18.2</b> Notes on Application</a></li>
</ul></li>
<li class="part"><span><b>VI Trees and Ensembles</b></span></li>
<li class="chapter" data-level="19" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html"><i class="fa fa-check"></i><b>19</b> Classification and Regression Trees</a>
<ul>
<li class="chapter" data-level="19.1" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#example-classification-tree"><i class="fa fa-check"></i><b>19.1</b> Example: Classification Tree</a></li>
<li class="chapter" data-level="19.2" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#splitting-a-node"><i class="fa fa-check"></i><b>19.2</b> Splitting a Node</a></li>
<li class="chapter" data-level="19.3" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#regression-trees"><i class="fa fa-check"></i><b>19.3</b> Regression Trees</a></li>
<li class="chapter" data-level="19.4" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#predicting-a-target-point"><i class="fa fa-check"></i><b>19.4</b> Predicting a Target Point</a></li>
<li class="chapter" data-level="19.5" data-path="classification-and-regression-trees.html"><a href="classification-and-regression-trees.html#tuning-a-tree-model"><i class="fa fa-check"></i><b>19.5</b> Tuning a Tree Model</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="random-forests.html"><a href="random-forests.html"><i class="fa fa-check"></i><b>20</b> Random Forests</a>
<ul>
<li class="chapter" data-level="20.1" data-path="random-forests.html"><a href="random-forests.html#bagging-predictors"><i class="fa fa-check"></i><b>20.1</b> Bagging Predictors</a></li>
<li class="chapter" data-level="20.2" data-path="random-forests.html"><a href="random-forests.html#random-forests-1"><i class="fa fa-check"></i><b>20.2</b> Random Forests</a></li>
<li class="chapter" data-level="20.3" data-path="random-forests.html"><a href="random-forests.html#kernel-view-of-random-forests"><i class="fa fa-check"></i><b>20.3</b> Kernel view of Random Forests</a></li>
<li class="chapter" data-level="20.4" data-path="random-forests.html"><a href="random-forests.html#variable-importance"><i class="fa fa-check"></i><b>20.4</b> Variable Importance</a></li>
<li class="chapter" data-level="20.5" data-path="random-forests.html"><a href="random-forests.html#adaptiveness-of-random-forest-kernel"><i class="fa fa-check"></i><b>20.5</b> Adaptiveness of Random Forest Kernel</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>21</b> Boosting</a>
<ul>
<li class="chapter" data-level="21.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>21.1</b> AdaBoost</a></li>
<li class="chapter" data-level="21.2" data-path="boosting.html"><a href="boosting.html#training-error-of-adaboost"><i class="fa fa-check"></i><b>21.2</b> Training Error of AdaBoost</a></li>
<li class="chapter" data-level="21.3" data-path="boosting.html"><a href="boosting.html#tuning-the-number-of-trees"><i class="fa fa-check"></i><b>21.3</b> Tuning the Number of Trees</a></li>
<li class="chapter" data-level="21.4" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>21.4</b> Gradient Boosting</a></li>
<li class="chapter" data-level="21.5" data-path="boosting.html"><a href="boosting.html#gradient-boosting-with-logistic-link"><i class="fa fa-check"></i><b>21.5</b> Gradient Boosting with Logistic Link</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Learning</b></span></li>
<li class="chapter" data-level="22" data-path="k-means.html"><a href="k-means.html"><i class="fa fa-check"></i><b>22</b> K-Means</a>
<ul>
<li class="chapter" data-level="22.1" data-path="k-means.html"><a href="k-means.html#basic-concepts"><i class="fa fa-check"></i><b>22.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="22.2" data-path="k-means.html"><a href="k-means.html#example-1-iris-data"><i class="fa fa-check"></i><b>22.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="22.3" data-path="k-means.html"><a href="k-means.html#example-2-clustering-of-image-pixels"><i class="fa fa-check"></i><b>22.3</b> Example 2: clustering of image pixels</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>23</b> Hierarchical Clustering</a>
<ul>
<li class="chapter" data-level="23.1" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#basic-concepts-1"><i class="fa fa-check"></i><b>23.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="23.2" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-1-iris-data-1"><i class="fa fa-check"></i><b>23.2</b> Example 1: <code>iris</code> data</a></li>
<li class="chapter" data-level="23.3" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html#example-2-rna-expression-data"><i class="fa fa-check"></i><b>23.3</b> Example 2: RNA Expression Data</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>24</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="24.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#basic-concepts-2"><i class="fa fa-check"></i><b>24.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#note-scaling"><i class="fa fa-check"></i><b>24.1.1</b> Note: Scaling</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-1-iris-data-2"><i class="fa fa-check"></i><b>24.2</b> Example 1: <code>iris</code> Data</a></li>
<li class="chapter" data-level="24.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#example-2-handwritten-digits"><i class="fa fa-check"></i><b>24.3</b> Example 2: Handwritten Digits</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="self-organizing-map.html"><a href="self-organizing-map.html"><i class="fa fa-check"></i><b>25</b> Self-Organizing Map</a>
<ul>
<li class="chapter" data-level="25.1" data-path="self-organizing-map.html"><a href="self-organizing-map.html#basic-concepts-3"><i class="fa fa-check"></i><b>25.1</b> Basic Concepts</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="spectral-clustering.html"><a href="spectral-clustering.html"><i class="fa fa-check"></i><b>26</b> Spectral Clustering</a>
<ul>
<li class="chapter" data-level="26.1" data-path="spectral-clustering.html"><a href="spectral-clustering.html#an-example"><i class="fa fa-check"></i><b>26.1</b> An Example</a></li>
<li class="chapter" data-level="26.2" data-path="spectral-clustering.html"><a href="spectral-clustering.html#adjacency-matrix"><i class="fa fa-check"></i><b>26.2</b> Adjacency Matrix</a></li>
<li class="chapter" data-level="26.3" data-path="spectral-clustering.html"><a href="spectral-clustering.html#laplacian-matrix"><i class="fa fa-check"></i><b>26.3</b> Laplacian Matrix</a></li>
<li class="chapter" data-level="26.4" data-path="spectral-clustering.html"><a href="spectral-clustering.html#derivation-of-the-feature-embedding"><i class="fa fa-check"></i><b>26.4</b> Derivation of the Feature Embedding</a></li>
<li class="chapter" data-level="26.5" data-path="spectral-clustering.html"><a href="spectral-clustering.html#feature-embedding"><i class="fa fa-check"></i><b>26.5</b> Feature Embedding</a></li>
<li class="chapter" data-level="26.6" data-path="spectral-clustering.html"><a href="spectral-clustering.html#clustering-with-embedded-features"><i class="fa fa-check"></i><b>26.6</b> Clustering with Embedded Features</a></li>
<li class="chapter" data-level="26.7" data-path="spectral-clustering.html"><a href="spectral-clustering.html#normalized-graph-laplacian"><i class="fa fa-check"></i><b>26.7</b> Normalized Graph Laplacian</a></li>
<li class="chapter" data-level="26.8" data-path="spectral-clustering.html"><a href="spectral-clustering.html#using-a-different-adjacency-matrix"><i class="fa fa-check"></i><b>26.8</b> Using a Different Adjacency Matrix</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html"><i class="fa fa-check"></i><b>27</b> Uniform Manifold Approximation and Projection</a>
<ul>
<li class="chapter" data-level="27.1" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#an-example-1"><i class="fa fa-check"></i><b>27.1</b> An Example</a></li>
<li class="chapter" data-level="27.2" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#tuning"><i class="fa fa-check"></i><b>27.2</b> Tuning</a></li>
<li class="chapter" data-level="27.3" data-path="uniform-manifold-approximation-and-projection.html"><a href="uniform-manifold-approximation-and-projection.html#another-example"><i class="fa fa-check"></i><b>27.3</b> Another Example</a></li>
</ul></li>
<li class="part"><span><b>VIII Reference</b></span></li>
<li class="chapter" data-level="28" data-path="reference.html"><a href="reference.html"><i class="fa fa-check"></i><b>28</b> Reference</a></li>
<li class="divider"></li>
<li><a href="https://github.com/teazrq/SMLR" target="blank">&copy; 2023 Ruoqing Zhu</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Logistic Regression<a href="logistic-regression.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="modeling-binary-outcomes" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Modeling Binary Outcomes<a href="logistic-regression.html#modeling-binary-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To model binary outcomes using a logistic regression, we will use the 0/1 coding of <span class="math inline">\(Y\)</span>. We need to set its connection with covariates. Recall in a linear regression, the outcome is continuous, and we set</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon\]</span>
However, this does not work for classification since <span class="math inline">\(Y\)</span> can only be 0 or 1. Hence we turn to consider modeling the probability <span class="math inline">\(P(Y = 1 | X = \mathbf{x})\)</span>. Hence, <span class="math inline">\(Y\)</span> is a Bernoulli random variable given <span class="math inline">\(X\)</span>, and this is modeled by a function of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[ P(Y = 1 | X = \mathbf{x}) = \frac{\exp(\mathbf{x}^\text{T}\boldsymbol{\beta})}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol{\beta})}\]</span>
Note that although <span class="math inline">\(\mathbf{x}^\text{T}\boldsymbol{\beta}\)</span> may ranges from 0 to infinity as <span class="math inline">\(X\)</span> changes, the probability will still be bounded between 0 and 1. This is an example of <strong>Generalized Linear Models</strong>. The relationship is still represented using a linear function of <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\mathbf{x}^\text{T}\boldsymbol{\beta}\)</span>. This is called a <strong>logit link</strong> function (a function to connect the conditional expectation of <span class="math inline">\(Y\)</span> with <span class="math inline">\(\boldsymbol{\beta}^\text{T}\mathbf{x}\)</span>):</p>
<p><span class="math display">\[\eta(a) = \frac{\exp(a)}{1 + \exp(a)}\]</span>
Hence, <span class="math inline">\(P(Y = 1 | X = \mathbf{x}) = \eta(\mathbf{x}^\text{T}\boldsymbol{\beta})\)</span>. We can reversely solve this and get</p>
<span class="math display">\[\begin{aligned}
P(Y = 1 | X = \mathbf{x}) = \eta(\mathbf{x}^\text{T}\boldsymbol{\beta}) &amp;= \frac{\exp(\mathbf{x}^\text{T}\boldsymbol{\beta})}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol{\beta})}\\
1 - \eta(\mathbf{x}^\text{T}\boldsymbol{\beta}) &amp;= \frac{1}{1 + \exp(\mathbf{x}^\text{T}\boldsymbol{\beta})} \\
\text{Odds} = \frac{\eta(\mathbf{x}^\text{T}\boldsymbol{\beta})}{1-\eta(\mathbf{x}^\text{T}\boldsymbol{\beta})} &amp;= \exp(\mathbf{x}^\text{T}\boldsymbol{\beta})\\
\log(\text{Odds}) = \mathbf{x}^\text{T}\boldsymbol{\beta}
\end{aligned}\]</span>
<p>Hence, the parameters in a logistic regression is explained as <strong>log odds</strong>. Let’s look at a concrete example.</p>
</div>
<div id="example-cleveland-clinic-heart-disease-data" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Example: Cleveland Clinic Heart Disease Data<a href="logistic-regression.html#example-cleveland-clinic-heart-disease-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use use the <a href="https://www.kaggle.com/aavigan/cleveland-clinic-heart-disease-dataset">Cleveland clinic heart disease dataset</a>. The goal is to model and predict a class label of whether the patient has a hearth disease or not. This is indicated by whether the <code>num</code> variable is <span class="math inline">\(0\)</span> (no presence) or <span class="math inline">\(&gt;0\)</span> (presence).</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="logistic-regression.html#cb107-1" tabindex="-1"></a>  heart <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/processed_cleveland.csv&quot;</span>)</span>
<span id="cb107-2"><a href="logistic-regression.html#cb107-2" tabindex="-1"></a>  heart<span class="sc">$</span>Y <span class="ot">=</span> <span class="fu">as.factor</span>(heart<span class="sc">$</span>num <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb107-3"><a href="logistic-regression.html#cb107-3" tabindex="-1"></a>  <span class="fu">table</span>(heart<span class="sc">$</span>Y)</span>
<span id="cb107-4"><a href="logistic-regression.html#cb107-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb107-5"><a href="logistic-regression.html#cb107-5" tabindex="-1"></a><span class="do">## FALSE  TRUE </span></span>
<span id="cb107-6"><a href="logistic-regression.html#cb107-6" tabindex="-1"></a><span class="do">##   164   139</span></span></code></pre></div>
<p>Let’s model the probability of heart disease using the <code>Age</code> variable. This can be done using the <code>glm()</code> function, which stands for the Generalized Linear Model. The syntax of <code>glm()</code> is almost the same as a linear model. Note that it is important to use <code>family = binomial</code> to specify the logistic regression.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="logistic-regression.html#cb108-1" tabindex="-1"></a>  logistic.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y<span class="sc">~</span>age, <span class="at">data =</span> heart, <span class="at">family =</span> binomial)</span>
<span id="cb108-2"><a href="logistic-regression.html#cb108-2" tabindex="-1"></a>  <span class="fu">summary</span>(logistic.fit)</span>
<span id="cb108-3"><a href="logistic-regression.html#cb108-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb108-4"><a href="logistic-regression.html#cb108-4" tabindex="-1"></a><span class="do">## Call:</span></span>
<span id="cb108-5"><a href="logistic-regression.html#cb108-5" tabindex="-1"></a><span class="do">## glm(formula = Y ~ age, family = binomial, data = heart)</span></span>
<span id="cb108-6"><a href="logistic-regression.html#cb108-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb108-7"><a href="logistic-regression.html#cb108-7" tabindex="-1"></a><span class="do">## Coefficients:</span></span>
<span id="cb108-8"><a href="logistic-regression.html#cb108-8" tabindex="-1"></a><span class="do">##             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb108-9"><a href="logistic-regression.html#cb108-9" tabindex="-1"></a><span class="do">## (Intercept) -3.00591    0.75913  -3.960  7.5e-05 ***</span></span>
<span id="cb108-10"><a href="logistic-regression.html#cb108-10" tabindex="-1"></a><span class="do">## age          0.05199    0.01367   3.803 0.000143 ***</span></span>
<span id="cb108-11"><a href="logistic-regression.html#cb108-11" tabindex="-1"></a><span class="do">## ---</span></span>
<span id="cb108-12"><a href="logistic-regression.html#cb108-12" tabindex="-1"></a><span class="do">## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb108-13"><a href="logistic-regression.html#cb108-13" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb108-14"><a href="logistic-regression.html#cb108-14" tabindex="-1"></a><span class="do">## (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb108-15"><a href="logistic-regression.html#cb108-15" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb108-16"><a href="logistic-regression.html#cb108-16" tabindex="-1"></a><span class="do">##     Null deviance: 417.98  on 302  degrees of freedom</span></span>
<span id="cb108-17"><a href="logistic-regression.html#cb108-17" tabindex="-1"></a><span class="do">## Residual deviance: 402.54  on 301  degrees of freedom</span></span>
<span id="cb108-18"><a href="logistic-regression.html#cb108-18" tabindex="-1"></a><span class="do">## AIC: 406.54</span></span>
<span id="cb108-19"><a href="logistic-regression.html#cb108-19" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb108-20"><a href="logistic-regression.html#cb108-20" tabindex="-1"></a><span class="do">## Number of Fisher Scoring iterations: 4</span></span></code></pre></div>
<p>The result is similar to a linear regression, with some differences. The parameter estimate of age is 0.05199. It is positive, meaning that increasing age would increase the change of having heart disease. However, this does not mean that 1 year older would increase the change by 0.05. Since, by our previous formula, the probably is not directly expressed as <span class="math inline">\(\mathbf{x}^\text{T}\boldsymbol{\beta}\)</span>.</p>
<p>This calculation can be realized when predicting a new target point. Let’s consider a new subject with <code>Age = 55</code>. What is the predicted probability of heart disease? Based on our formula, we have</p>
<p><span class="math display">\[\beta_0 + \beta_1 X = -3.00591 + 0.05199 \times 55 = -0.14646\]</span>
And the estimated probability is</p>
<p><span class="math display">\[ P(Y = 1 | X) = \frac{\exp(\beta_0 + \beta_1 X)}{1 + \exp(\beta_0 + \beta_1 X)} = \frac{\exp(-0.14646)}{1 + \exp(-0.14646)} = 0.4634503\]</span>
Hence, the estimated probability for this subject is 46.3%. This can be done using R code. Please note that if you want to predict the probability, you need to specify <code>type = "response"</code>. Otherwise, only <span class="math inline">\(\beta_0 + \beta_1 X\)</span> is provided.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="logistic-regression.html#cb109-1" tabindex="-1"></a>  testdata <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="st">&quot;age&quot;</span> <span class="ot">=</span> <span class="dv">55</span>)</span>
<span id="cb109-2"><a href="logistic-regression.html#cb109-2" tabindex="-1"></a>  <span class="fu">predict</span>(logistic.fit, <span class="at">newdata =</span> testdata)</span>
<span id="cb109-3"><a href="logistic-regression.html#cb109-3" tabindex="-1"></a><span class="do">##          1 </span></span>
<span id="cb109-4"><a href="logistic-regression.html#cb109-4" tabindex="-1"></a><span class="do">## -0.1466722</span></span>
<span id="cb109-5"><a href="logistic-regression.html#cb109-5" tabindex="-1"></a>  <span class="fu">predict</span>(logistic.fit, <span class="at">newdata =</span> testdata, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb109-6"><a href="logistic-regression.html#cb109-6" tabindex="-1"></a><span class="do">##         1 </span></span>
<span id="cb109-7"><a href="logistic-regression.html#cb109-7" tabindex="-1"></a><span class="do">## 0.4633975</span></span></code></pre></div>
<p>If we need to make a 0/1 decision about this subject, a natural idea is to see if the predicted probability is greater than 0.5. In this case, we would predict this subject as 0.</p>
</div>
<div id="interpretation-of-the-parameters" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Interpretation of the Parameters<a href="logistic-regression.html#interpretation-of-the-parameters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that <span class="math inline">\(\mathbf{x}^\text{T}\boldsymbol{\beta}\)</span> is the log odds, we can further interpret the effect of a single variable. Let’s define the following two, with an arbitrary age value <span class="math inline">\(a\)</span>:</p>
<ul>
<li>A subject with <code>age</code> <span class="math inline">\(= a\)</span></li>
<li>A subject with <code>age</code> <span class="math inline">\(= a + 1\)</span></li>
</ul>
<p>Then, if we look at the <strong>odds ratio</strong> corresponding to these two target points, we have</p>
<span class="math display">\[\begin{aligned}
\text{Odds Ratio} &amp;= \frac{\text{Odds in Group 2}}{\text{Odds in Group 1}}\\
&amp;= \frac{\exp(\beta_0 + \beta_1 (a+1))}{\exp(\beta_0 + \beta_1 a)}\\
&amp;= \frac{\exp(\beta_0 + \beta_1 a) \times \exp(\beta_1)}{\exp(\beta_0 + \beta_1 a)}\\
&amp;= \exp(\beta_1)
\end{aligned}\]</span>
<p>Taking <span class="math inline">\(\log\)</span> on both sides, we have</p>
<p><span class="math display">\[\log(\text{Odds Ratio}) = \beta_1\]</span></p>
<p>Hence, the odds ratio between these two subjects (<strong>they differ only with one unit of <code>age</code></strong>) can be directly interpreted as the exponential of the parameter of <code>age</code>. After taking the log, we can also say that</p>
<blockquote>
<p>The parameter <span class="math inline">\(\beta\)</span> of a varaible in a logistic regression represents the <strong>log of odds ratio</strong> associated with one-unit increase of this variable.</p>
</blockquote>
<p>Please note that we usually do not be explicit about what this odds ratio is about (what two subject we are comparing). Because the interpretation of the parameter does not change regardless of the value <span class="math inline">\(a\)</span>, as long as the two subjects differ in one unit.</p>
<p>And also note that this conclusion is regardless of the values of other covaraites. When we have a multivariate model, as long as all other covariates are held the same, the previous derivation will remain the same.</p>
</div>
<div id="solving-a-logistic-regression" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Solving a Logistic Regression<a href="logistic-regression.html#solving-a-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The logistic regression is solved by maximizing the log-likelihood function. Note that the log-likelihood is given by</p>
<p><span class="math display">\[\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \log \, p(y_i | x_i, \boldsymbol{\beta}).\]</span>
Using the probabilities of Bernoulli distribution, we have</p>
<p><span class="math display">\[\begin{align}
\ell(\boldsymbol{\beta}) =&amp; \sum_{i=1}^n \log \left\{ \eta(\mathbf{x}_i)^{y_i} [1-\eta(\mathbf{x}_i)]^{1-y_i} \right\}\\
    =&amp; \sum_{i=1}^n y_i \log \frac{\eta(\mathbf{x}_i)}{1-\eta(\mathbf{x}_i)} + \log [1-\eta(\mathbf{x}_i)] \\
    =&amp; \sum_{i=1}^n y_i \mathbf{x}_i^\text{T}\boldsymbol{\beta}- \log [ 1 + \exp(\mathbf{x}_i^\text{T}\boldsymbol{\beta})]
\end{align}\]</span></p>
<p>Since this objective function is relatively simple, we can use Newton’s method to update. The gradient is given by</p>
<p><span class="math display">\[\frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} =~ \sum_{i=1}^n y_i \mathbf{x}_i^\text{T}- \sum_{i=1}^n \frac{\exp(\mathbf{x}_i^\text{T}\boldsymbol{\beta}) \mathbf{x}_i^\text{T}}{1 + \exp(\mathbf{x}_i^\text{T}\boldsymbol{\beta})},\]</span></p>
<p>and the Hessian matrix is given by</p>
<p><span class="math display">\[\frac{\partial^2 \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}\partial \boldsymbol{\beta}^\text{T}} =~ - \sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i^\text{T}\eta(\mathbf{x}_i) [1- \eta(\mathbf{x}_i)].\]</span>
This leads to the update</p>
<p><span class="math display">\[\boldsymbol{\beta}^{\,\text{new}} = \boldsymbol{\beta}^{\,\text{old}} - \left[\frac{\partial^2 \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}\partial \boldsymbol{\beta}^\text{T}}\right]^{-1} \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}\]</span></p>
</div>
<div id="example-south-africa-heart-data" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Example: South Africa Heart Data<a href="logistic-regression.html#example-south-africa-heart-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use the South Africa heart data as a demonstration. The goal is to estimate the probability of <code>chd</code>, the indicator of coronary heart disease.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="logistic-regression.html#cb110-1" tabindex="-1"></a>    <span class="fu">library</span>(ElemStatLearn)</span>
<span id="cb110-2"><a href="logistic-regression.html#cb110-2" tabindex="-1"></a>    <span class="fu">data</span>(SAheart)</span>
<span id="cb110-3"><a href="logistic-regression.html#cb110-3" tabindex="-1"></a>    </span>
<span id="cb110-4"><a href="logistic-regression.html#cb110-4" tabindex="-1"></a>    heart <span class="ot">=</span> SAheart</span>
<span id="cb110-5"><a href="logistic-regression.html#cb110-5" tabindex="-1"></a>    heart<span class="sc">$</span>famhist <span class="ot">=</span> <span class="fu">as.numeric</span>(heart<span class="sc">$</span>famhist)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb110-6"><a href="logistic-regression.html#cb110-6" tabindex="-1"></a>    n <span class="ot">=</span> <span class="fu">nrow</span>(heart)</span>
<span id="cb110-7"><a href="logistic-regression.html#cb110-7" tabindex="-1"></a>    p <span class="ot">=</span> <span class="fu">ncol</span>(heart)</span>
<span id="cb110-8"><a href="logistic-regression.html#cb110-8" tabindex="-1"></a>    </span>
<span id="cb110-9"><a href="logistic-regression.html#cb110-9" tabindex="-1"></a>    heart.full <span class="ot">=</span> <span class="fu">glm</span>(chd<span class="sc">~</span>., <span class="at">data=</span>heart, <span class="at">family=</span>binomial)</span>
<span id="cb110-10"><a href="logistic-regression.html#cb110-10" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">summary</span>(heart.full)<span class="sc">$</span>coef, <span class="at">dig=</span><span class="dv">3</span>)</span>
<span id="cb110-11"><a href="logistic-regression.html#cb110-11" tabindex="-1"></a><span class="do">##             Estimate Std. Error z value Pr(&gt;|z|)</span></span>
<span id="cb110-12"><a href="logistic-regression.html#cb110-12" tabindex="-1"></a><span class="do">## (Intercept)   -6.151      1.308  -4.701    0.000</span></span>
<span id="cb110-13"><a href="logistic-regression.html#cb110-13" tabindex="-1"></a><span class="do">## sbp            0.007      0.006   1.135    0.256</span></span>
<span id="cb110-14"><a href="logistic-regression.html#cb110-14" tabindex="-1"></a><span class="do">## tobacco        0.079      0.027   2.984    0.003</span></span>
<span id="cb110-15"><a href="logistic-regression.html#cb110-15" tabindex="-1"></a><span class="do">## ldl            0.174      0.060   2.915    0.004</span></span>
<span id="cb110-16"><a href="logistic-regression.html#cb110-16" tabindex="-1"></a><span class="do">## adiposity      0.019      0.029   0.635    0.526</span></span>
<span id="cb110-17"><a href="logistic-regression.html#cb110-17" tabindex="-1"></a><span class="do">## famhist        0.925      0.228   4.061    0.000</span></span>
<span id="cb110-18"><a href="logistic-regression.html#cb110-18" tabindex="-1"></a><span class="do">## typea          0.040      0.012   3.214    0.001</span></span>
<span id="cb110-19"><a href="logistic-regression.html#cb110-19" tabindex="-1"></a><span class="do">## obesity       -0.063      0.044  -1.422    0.155</span></span>
<span id="cb110-20"><a href="logistic-regression.html#cb110-20" tabindex="-1"></a><span class="do">## alcohol        0.000      0.004   0.027    0.978</span></span>
<span id="cb110-21"><a href="logistic-regression.html#cb110-21" tabindex="-1"></a><span class="do">## age            0.045      0.012   3.728    0.000</span></span>
<span id="cb110-22"><a href="logistic-regression.html#cb110-22" tabindex="-1"></a>    </span>
<span id="cb110-23"><a href="logistic-regression.html#cb110-23" tabindex="-1"></a>    <span class="co"># fitted value </span></span>
<span id="cb110-24"><a href="logistic-regression.html#cb110-24" tabindex="-1"></a>    yhat <span class="ot">=</span> (heart.full<span class="sc">$</span>fitted.values<span class="sc">&gt;</span><span class="fl">0.5</span>)</span>
<span id="cb110-25"><a href="logistic-regression.html#cb110-25" tabindex="-1"></a>    <span class="fu">table</span>(yhat, SAheart<span class="sc">$</span>chd)</span>
<span id="cb110-26"><a href="logistic-regression.html#cb110-26" tabindex="-1"></a><span class="do">##        </span></span>
<span id="cb110-27"><a href="logistic-regression.html#cb110-27" tabindex="-1"></a><span class="do">## yhat      0   1</span></span>
<span id="cb110-28"><a href="logistic-regression.html#cb110-28" tabindex="-1"></a><span class="do">##   FALSE 256  77</span></span>
<span id="cb110-29"><a href="logistic-regression.html#cb110-29" tabindex="-1"></a><span class="do">##   TRUE   46  83</span></span></code></pre></div>
<p>Based on what we learned in class, we can solve this problem ourselves using numerical optimization. Here we will demonstrate an approach that uses general solver <code>optim()</code>. First, write the objective function of the logistic regression, for any value of <span class="math inline">\(\boldsymbol \beta\)</span>, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span>.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="logistic-regression.html#cb111-1" tabindex="-1"></a>    <span class="co"># the negative log-likelihood function of logistic regression </span></span>
<span id="cb111-2"><a href="logistic-regression.html#cb111-2" tabindex="-1"></a>    my.loglik <span class="ot">&lt;-</span> <span class="cf">function</span>(b, x, y)</span>
<span id="cb111-3"><a href="logistic-regression.html#cb111-3" tabindex="-1"></a>    {</span>
<span id="cb111-4"><a href="logistic-regression.html#cb111-4" tabindex="-1"></a>        bm <span class="ot">=</span> <span class="fu">as.matrix</span>(b)</span>
<span id="cb111-5"><a href="logistic-regression.html#cb111-5" tabindex="-1"></a>        xb <span class="ot">=</span>  x <span class="sc">%*%</span> bm</span>
<span id="cb111-6"><a href="logistic-regression.html#cb111-6" tabindex="-1"></a>        <span class="co"># this returns the negative loglikelihood</span></span>
<span id="cb111-7"><a href="logistic-regression.html#cb111-7" tabindex="-1"></a>        <span class="fu">return</span>(<span class="fu">sum</span>(y<span class="sc">*</span>xb) <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">log</span>(<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(xb))))</span>
<span id="cb111-8"><a href="logistic-regression.html#cb111-8" tabindex="-1"></a>    }</span>
<span id="cb111-9"><a href="logistic-regression.html#cb111-9" tabindex="-1"></a></span>
<span id="cb111-10"><a href="logistic-regression.html#cb111-10" tabindex="-1"></a>    <span class="co"># Gradient</span></span>
<span id="cb111-11"><a href="logistic-regression.html#cb111-11" tabindex="-1"></a>    my.gradient <span class="ot">&lt;-</span> <span class="cf">function</span>(b, x, y)</span>
<span id="cb111-12"><a href="logistic-regression.html#cb111-12" tabindex="-1"></a>    {</span>
<span id="cb111-13"><a href="logistic-regression.html#cb111-13" tabindex="-1"></a>        bm <span class="ot">=</span> <span class="fu">as.matrix</span>(b) </span>
<span id="cb111-14"><a href="logistic-regression.html#cb111-14" tabindex="-1"></a>        expxb <span class="ot">=</span>  <span class="fu">exp</span>(x <span class="sc">%*%</span> bm)</span>
<span id="cb111-15"><a href="logistic-regression.html#cb111-15" tabindex="-1"></a>        <span class="fu">return</span>(<span class="fu">t</span>(x) <span class="sc">%*%</span> (y <span class="sc">-</span> expxb<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>expxb)))</span>
<span id="cb111-16"><a href="logistic-regression.html#cb111-16" tabindex="-1"></a>    }</span></code></pre></div>
<p>Let’s check the result of this function for some arbitrary <span class="math inline">\(\boldsymbol \beta\)</span> value.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="logistic-regression.html#cb112-1" tabindex="-1"></a>    <span class="co"># prepare the data matrix, I am adding a column of 1 for intercept</span></span>
<span id="cb112-2"><a href="logistic-regression.html#cb112-2" tabindex="-1"></a>    </span>
<span id="cb112-3"><a href="logistic-regression.html#cb112-3" tabindex="-1"></a>    x <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(<span class="st">&quot;intercept&quot;</span> <span class="ot">=</span> <span class="dv">1</span>, heart[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>]))</span>
<span id="cb112-4"><a href="logistic-regression.html#cb112-4" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">as.matrix</span>(heart[,<span class="dv">10</span>])</span>
<span id="cb112-5"><a href="logistic-regression.html#cb112-5" tabindex="-1"></a>    </span>
<span id="cb112-6"><a href="logistic-regression.html#cb112-6" tabindex="-1"></a>    <span class="co"># check my function</span></span>
<span id="cb112-7"><a href="logistic-regression.html#cb112-7" tabindex="-1"></a>    b <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">ncol</span>(x))</span>
<span id="cb112-8"><a href="logistic-regression.html#cb112-8" tabindex="-1"></a>    <span class="fu">my.loglik</span>(b, x, y) <span class="co"># scalar</span></span>
<span id="cb112-9"><a href="logistic-regression.html#cb112-9" tabindex="-1"></a><span class="do">## [1] -320.234</span></span>
<span id="cb112-10"><a href="logistic-regression.html#cb112-10" tabindex="-1"></a>    </span>
<span id="cb112-11"><a href="logistic-regression.html#cb112-11" tabindex="-1"></a>    <span class="co"># check the optimal value and the likelihood</span></span>
<span id="cb112-12"><a href="logistic-regression.html#cb112-12" tabindex="-1"></a>    <span class="fu">my.loglik</span>(heart.full<span class="sc">$</span>coefficients, x, y)</span>
<span id="cb112-13"><a href="logistic-regression.html#cb112-13" tabindex="-1"></a><span class="do">## [1] -236.07</span></span></code></pre></div>
<p>Then we optimize this objective function</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="logistic-regression.html#cb113-1" tabindex="-1"></a>    <span class="co"># Use a general solver to get the optimal value</span></span>
<span id="cb113-2"><a href="logistic-regression.html#cb113-2" tabindex="-1"></a>    <span class="co"># Note that we are doing maximization instead of minimization, </span></span>
<span id="cb113-3"><a href="logistic-regression.html#cb113-3" tabindex="-1"></a>    <span class="co"># we need to specify &quot;fnscale&quot; = -1</span></span>
<span id="cb113-4"><a href="logistic-regression.html#cb113-4" tabindex="-1"></a>    <span class="fu">optim</span>(b, <span class="at">fn =</span> my.loglik, <span class="at">gr =</span> my.gradient, </span>
<span id="cb113-5"><a href="logistic-regression.html#cb113-5" tabindex="-1"></a>          <span class="at">method =</span> <span class="st">&quot;BFGS&quot;</span>, <span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">control =</span> <span class="fu">list</span>(<span class="st">&quot;fnscale&quot;</span> <span class="ot">=</span> <span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb113-6"><a href="logistic-regression.html#cb113-6" tabindex="-1"></a><span class="do">## $par</span></span>
<span id="cb113-7"><a href="logistic-regression.html#cb113-7" tabindex="-1"></a><span class="do">##  [1] -6.150733305  0.006504017  0.079376464  0.173923988  0.018586578  0.925372019  0.039595096</span></span>
<span id="cb113-8"><a href="logistic-regression.html#cb113-8" tabindex="-1"></a><span class="do">##  [8] -0.062909867  0.000121675  0.045225500</span></span>
<span id="cb113-9"><a href="logistic-regression.html#cb113-9" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb113-10"><a href="logistic-regression.html#cb113-10" tabindex="-1"></a><span class="do">## $value</span></span>
<span id="cb113-11"><a href="logistic-regression.html#cb113-11" tabindex="-1"></a><span class="do">## [1] -236.07</span></span>
<span id="cb113-12"><a href="logistic-regression.html#cb113-12" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb113-13"><a href="logistic-regression.html#cb113-13" tabindex="-1"></a><span class="do">## $counts</span></span>
<span id="cb113-14"><a href="logistic-regression.html#cb113-14" tabindex="-1"></a><span class="do">## function gradient </span></span>
<span id="cb113-15"><a href="logistic-regression.html#cb113-15" tabindex="-1"></a><span class="do">##       74       16 </span></span>
<span id="cb113-16"><a href="logistic-regression.html#cb113-16" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb113-17"><a href="logistic-regression.html#cb113-17" tabindex="-1"></a><span class="do">## $convergence</span></span>
<span id="cb113-18"><a href="logistic-regression.html#cb113-18" tabindex="-1"></a><span class="do">## [1] 0</span></span>
<span id="cb113-19"><a href="logistic-regression.html#cb113-19" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb113-20"><a href="logistic-regression.html#cb113-20" tabindex="-1"></a><span class="do">## $message</span></span>
<span id="cb113-21"><a href="logistic-regression.html#cb113-21" tabindex="-1"></a><span class="do">## NULL</span></span></code></pre></div>
<p>This matches our <code>glm()</code> solution. Now, if we do not have a general solver, we should consider using the Newton-Raphson. You need to write a function to calculate the Hessian matrix and proceed with an optimization update.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="logistic-regression.html#cb114-1" tabindex="-1"></a>    <span class="co"># my Newton-Raphson method</span></span>
<span id="cb114-2"><a href="logistic-regression.html#cb114-2" tabindex="-1"></a>    <span class="co"># set up an initial value</span></span>
<span id="cb114-3"><a href="logistic-regression.html#cb114-3" tabindex="-1"></a>    <span class="co"># this is sometimes crucial...</span></span>
<span id="cb114-4"><a href="logistic-regression.html#cb114-4" tabindex="-1"></a>    </span>
<span id="cb114-5"><a href="logistic-regression.html#cb114-5" tabindex="-1"></a>    b <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">ncol</span>(x))</span>
<span id="cb114-6"><a href="logistic-regression.html#cb114-6" tabindex="-1"></a>    </span>
<span id="cb114-7"><a href="logistic-regression.html#cb114-7" tabindex="-1"></a>    mybeta <span class="ot">=</span> <span class="fu">my.logistic</span>(b, x, y, <span class="at">tol =</span> <span class="fl">1e-10</span>, <span class="at">maxitr =</span> <span class="dv">20</span>, </span>
<span id="cb114-8"><a href="logistic-regression.html#cb114-8" tabindex="-1"></a>                         <span class="at">gr =</span> my.gradient, <span class="at">hess =</span> my.hessian, <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb114-9"><a href="logistic-regression.html#cb114-9" tabindex="-1"></a><span class="do">## at iteration 1, current beta is </span></span>
<span id="cb114-10"><a href="logistic-regression.html#cb114-10" tabindex="-1"></a><span class="do">## -4.032 0.005 0.066 0.133 0.009 0.694 0.024 -0.045 -0.001 0.027</span></span>
<span id="cb114-11"><a href="logistic-regression.html#cb114-11" tabindex="-1"></a><span class="do">## at iteration 2, current beta is </span></span>
<span id="cb114-12"><a href="logistic-regression.html#cb114-12" tabindex="-1"></a><span class="do">## -5.684 0.006 0.077 0.167 0.017 0.884 0.037 -0.061 0 0.041</span></span>
<span id="cb114-13"><a href="logistic-regression.html#cb114-13" tabindex="-1"></a><span class="do">## at iteration 3, current beta is </span></span>
<span id="cb114-14"><a href="logistic-regression.html#cb114-14" tabindex="-1"></a><span class="do">## -6.127 0.007 0.079 0.174 0.019 0.924 0.039 -0.063 0 0.045</span></span>
<span id="cb114-15"><a href="logistic-regression.html#cb114-15" tabindex="-1"></a><span class="do">## at iteration 4, current beta is </span></span>
<span id="cb114-16"><a href="logistic-regression.html#cb114-16" tabindex="-1"></a><span class="do">## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045</span></span>
<span id="cb114-17"><a href="logistic-regression.html#cb114-17" tabindex="-1"></a><span class="do">## at iteration 5, current beta is </span></span>
<span id="cb114-18"><a href="logistic-regression.html#cb114-18" tabindex="-1"></a><span class="do">## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045</span></span>
<span id="cb114-19"><a href="logistic-regression.html#cb114-19" tabindex="-1"></a><span class="do">## at iteration 6, current beta is </span></span>
<span id="cb114-20"><a href="logistic-regression.html#cb114-20" tabindex="-1"></a><span class="do">## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045</span></span>
<span id="cb114-21"><a href="logistic-regression.html#cb114-21" tabindex="-1"></a><span class="do">## at iteration 7, current beta is </span></span>
<span id="cb114-22"><a href="logistic-regression.html#cb114-22" tabindex="-1"></a><span class="do">## -6.151 0.007 0.079 0.174 0.019 0.925 0.04 -0.063 0 0.045</span></span>
<span id="cb114-23"><a href="logistic-regression.html#cb114-23" tabindex="-1"></a>    </span>
<span id="cb114-24"><a href="logistic-regression.html#cb114-24" tabindex="-1"></a>    <span class="co"># the parameter value</span></span>
<span id="cb114-25"><a href="logistic-regression.html#cb114-25" tabindex="-1"></a>    mybeta</span>
<span id="cb114-26"><a href="logistic-regression.html#cb114-26" tabindex="-1"></a><span class="do">##                    [,1]</span></span>
<span id="cb114-27"><a href="logistic-regression.html#cb114-27" tabindex="-1"></a><span class="do">## intercept -6.1507208650</span></span>
<span id="cb114-28"><a href="logistic-regression.html#cb114-28" tabindex="-1"></a><span class="do">## sbp        0.0065040171</span></span>
<span id="cb114-29"><a href="logistic-regression.html#cb114-29" tabindex="-1"></a><span class="do">## tobacco    0.0793764457</span></span>
<span id="cb114-30"><a href="logistic-regression.html#cb114-30" tabindex="-1"></a><span class="do">## ldl        0.1739238981</span></span>
<span id="cb114-31"><a href="logistic-regression.html#cb114-31" tabindex="-1"></a><span class="do">## adiposity  0.0185865682</span></span>
<span id="cb114-32"><a href="logistic-regression.html#cb114-32" tabindex="-1"></a><span class="do">## famhist    0.9253704194</span></span>
<span id="cb114-33"><a href="logistic-regression.html#cb114-33" tabindex="-1"></a><span class="do">## typea      0.0395950250</span></span>
<span id="cb114-34"><a href="logistic-regression.html#cb114-34" tabindex="-1"></a><span class="do">## obesity   -0.0629098693</span></span>
<span id="cb114-35"><a href="logistic-regression.html#cb114-35" tabindex="-1"></a><span class="do">## alcohol    0.0001216624</span></span>
<span id="cb114-36"><a href="logistic-regression.html#cb114-36" tabindex="-1"></a><span class="do">## age        0.0452253496</span></span>
<span id="cb114-37"><a href="logistic-regression.html#cb114-37" tabindex="-1"></a>    <span class="co"># get the standard error estimation </span></span>
<span id="cb114-38"><a href="logistic-regression.html#cb114-38" tabindex="-1"></a>    mysd <span class="ot">=</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">solve</span>(<span class="sc">-</span><span class="fu">my.hessian</span>(mybeta, x, y))))    </span></code></pre></div>
<p>With this solution, I can then get the standard errors and the p-value. You can check them with the <code>glm()</code> function solution.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="logistic-regression.html#cb115-1" tabindex="-1"></a>    <span class="co"># my summary matrix</span></span>
<span id="cb115-2"><a href="logistic-regression.html#cb115-2" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">data.frame</span>(<span class="st">&quot;beta&quot;</span> <span class="ot">=</span> mybeta, <span class="st">&quot;sd&quot;</span> <span class="ot">=</span> mysd, <span class="st">&quot;z&quot;</span> <span class="ot">=</span> mybeta<span class="sc">/</span>mysd, </span>
<span id="cb115-3"><a href="logistic-regression.html#cb115-3" tabindex="-1"></a>          <span class="st">&quot;pvalue&quot;</span> <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pnorm</span>(<span class="fu">abs</span>(mybeta<span class="sc">/</span>mysd)))), <span class="at">dig=</span><span class="dv">5</span>)</span>
<span id="cb115-4"><a href="logistic-regression.html#cb115-4" tabindex="-1"></a><span class="do">##               beta      sd        z  pvalue</span></span>
<span id="cb115-5"><a href="logistic-regression.html#cb115-5" tabindex="-1"></a><span class="do">## intercept -6.15072 1.30826 -4.70145 0.00000</span></span>
<span id="cb115-6"><a href="logistic-regression.html#cb115-6" tabindex="-1"></a><span class="do">## sbp        0.00650 0.00573  1.13500 0.25637</span></span>
<span id="cb115-7"><a href="logistic-regression.html#cb115-7" tabindex="-1"></a><span class="do">## tobacco    0.07938 0.02660  2.98376 0.00285</span></span>
<span id="cb115-8"><a href="logistic-regression.html#cb115-8" tabindex="-1"></a><span class="do">## ldl        0.17392 0.05966  2.91517 0.00355</span></span>
<span id="cb115-9"><a href="logistic-regression.html#cb115-9" tabindex="-1"></a><span class="do">## adiposity  0.01859 0.02929  0.63458 0.52570</span></span>
<span id="cb115-10"><a href="logistic-regression.html#cb115-10" tabindex="-1"></a><span class="do">## famhist    0.92537 0.22789  4.06053 0.00005</span></span>
<span id="cb115-11"><a href="logistic-regression.html#cb115-11" tabindex="-1"></a><span class="do">## typea      0.03960 0.01232  3.21382 0.00131</span></span>
<span id="cb115-12"><a href="logistic-regression.html#cb115-12" tabindex="-1"></a><span class="do">## obesity   -0.06291 0.04425 -1.42176 0.15509</span></span>
<span id="cb115-13"><a href="logistic-regression.html#cb115-13" tabindex="-1"></a><span class="do">## alcohol    0.00012 0.00448  0.02714 0.97835</span></span>
<span id="cb115-14"><a href="logistic-regression.html#cb115-14" tabindex="-1"></a><span class="do">## age        0.04523 0.01213  3.72846 0.00019</span></span>
<span id="cb115-15"><a href="logistic-regression.html#cb115-15" tabindex="-1"></a>      </span>
<span id="cb115-16"><a href="logistic-regression.html#cb115-16" tabindex="-1"></a>    <span class="co"># check that with the glm fitting </span></span>
<span id="cb115-17"><a href="logistic-regression.html#cb115-17" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">summary</span>(heart.full)<span class="sc">$</span>coef, <span class="at">dig=</span><span class="dv">5</span>)</span>
<span id="cb115-18"><a href="logistic-regression.html#cb115-18" tabindex="-1"></a><span class="do">##             Estimate Std. Error  z value Pr(&gt;|z|)</span></span>
<span id="cb115-19"><a href="logistic-regression.html#cb115-19" tabindex="-1"></a><span class="do">## (Intercept) -6.15072    1.30826 -4.70145  0.00000</span></span>
<span id="cb115-20"><a href="logistic-regression.html#cb115-20" tabindex="-1"></a><span class="do">## sbp          0.00650    0.00573  1.13500  0.25637</span></span>
<span id="cb115-21"><a href="logistic-regression.html#cb115-21" tabindex="-1"></a><span class="do">## tobacco      0.07938    0.02660  2.98376  0.00285</span></span>
<span id="cb115-22"><a href="logistic-regression.html#cb115-22" tabindex="-1"></a><span class="do">## ldl          0.17392    0.05966  2.91517  0.00355</span></span>
<span id="cb115-23"><a href="logistic-regression.html#cb115-23" tabindex="-1"></a><span class="do">## adiposity    0.01859    0.02929  0.63458  0.52570</span></span>
<span id="cb115-24"><a href="logistic-regression.html#cb115-24" tabindex="-1"></a><span class="do">## famhist      0.92537    0.22789  4.06053  0.00005</span></span>
<span id="cb115-25"><a href="logistic-regression.html#cb115-25" tabindex="-1"></a><span class="do">## typea        0.03960    0.01232  3.21382  0.00131</span></span>
<span id="cb115-26"><a href="logistic-regression.html#cb115-26" tabindex="-1"></a><span class="do">## obesity     -0.06291    0.04425 -1.42176  0.15509</span></span>
<span id="cb115-27"><a href="logistic-regression.html#cb115-27" tabindex="-1"></a><span class="do">## alcohol      0.00012    0.00448  0.02714  0.97835</span></span>
<span id="cb115-28"><a href="logistic-regression.html#cb115-28" tabindex="-1"></a><span class="do">## age          0.04523    0.01213  3.72846  0.00019</span></span></code></pre></div>
</div>
<div id="penalized-logistic-regression" class="section level2 hasAnchor" number="10.6">
<h2><span class="header-section-number">10.6</span> Penalized Logistic Regression<a href="logistic-regression.html#penalized-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Similar to a linear regression, we can also apply penalties to a logistic regression to address collinearity problems or select variables in a high-dimensional setting. For example, if we use the Lasso penalty, the objective function is</p>
<p><span class="math display">\[\sum_{i=1}^n \log \, p(y_i | x_i, \boldsymbol{\beta}) + \lambda |\boldsymbol{\beta}|_1\]</span>
This can be done using the <code>glmnet</code> package. Specifying <code>family = "binomial"</code> will ensure that a logistic regression is used, even your <code>y</code> is not a factor (but as numerical 0/1).</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="logistic-regression.html#cb116-1" tabindex="-1"></a>  <span class="fu">library</span>(glmnet)</span>
<span id="cb116-2"><a href="logistic-regression.html#cb116-2" tabindex="-1"></a>  lasso.fit <span class="ot">=</span> <span class="fu">cv.glmnet</span>(<span class="at">x =</span> <span class="fu">data.matrix</span>(SAheart[, <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>]), <span class="at">y =</span> SAheart[,<span class="dv">10</span>], </span>
<span id="cb116-3"><a href="logistic-regression.html#cb116-3" tabindex="-1"></a>                        <span class="at">nfold =</span> <span class="dv">10</span>, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb116-4"><a href="logistic-regression.html#cb116-4" tabindex="-1"></a>  </span>
<span id="cb116-5"><a href="logistic-regression.html#cb116-5" tabindex="-1"></a>  <span class="fu">plot</span>(lasso.fit)</span></code></pre></div>
<p><img src="SMLR_files/figure-html/unnamed-chunk-151-1.png" width="45%" style="display: block; margin: auto;" /></p>
<p>The procedure is essentially the same as in a linear regression. And we could obtain the estimated parameters by selecting the best <span class="math inline">\(\lambda\)</span> value.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="logistic-regression.html#cb117-1" tabindex="-1"></a>  <span class="fu">coef</span>(lasso.fit, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</span>
<span id="cb117-2"><a href="logistic-regression.html#cb117-2" tabindex="-1"></a><span class="do">## 10 x 1 sparse Matrix of class &quot;dgCMatrix&quot;</span></span>
<span id="cb117-3"><a href="logistic-regression.html#cb117-3" tabindex="-1"></a><span class="do">##               lambda.min</span></span>
<span id="cb117-4"><a href="logistic-regression.html#cb117-4" tabindex="-1"></a><span class="do">## (Intercept) -6.007447249</span></span>
<span id="cb117-5"><a href="logistic-regression.html#cb117-5" tabindex="-1"></a><span class="do">## sbp          0.002418113</span></span>
<span id="cb117-6"><a href="logistic-regression.html#cb117-6" tabindex="-1"></a><span class="do">## tobacco      0.064475340</span></span>
<span id="cb117-7"><a href="logistic-regression.html#cb117-7" tabindex="-1"></a><span class="do">## ldl          0.126087677</span></span>
<span id="cb117-8"><a href="logistic-regression.html#cb117-8" tabindex="-1"></a><span class="do">## adiposity    .          </span></span>
<span id="cb117-9"><a href="logistic-regression.html#cb117-9" tabindex="-1"></a><span class="do">## famhist      0.735876295</span></span>
<span id="cb117-10"><a href="logistic-regression.html#cb117-10" tabindex="-1"></a><span class="do">## typea        0.023547995</span></span>
<span id="cb117-11"><a href="logistic-regression.html#cb117-11" tabindex="-1"></a><span class="do">## obesity      .          </span></span>
<span id="cb117-12"><a href="logistic-regression.html#cb117-12" tabindex="-1"></a><span class="do">## alcohol      .          </span></span>
<span id="cb117-13"><a href="logistic-regression.html#cb117-13" tabindex="-1"></a><span class="do">## age          0.040806121</span></span></code></pre></div>

<div style="display:none;">
<!-- Conflict \def\bf{\mathbf{f}} -->
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="spline.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="discriminant-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "sepia",
    "family": "serif",
    "size": 1
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
