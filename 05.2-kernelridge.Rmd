```{r, echo=FALSE, results='asis'}
  # import macros
  cat("<div style='display:none;'>")
  cat(paste(scan("mathjax_header.html", what = character(), sep = "\n", quiet = TRUE), collapse = "\n"))
  cat("</div>")
```

```{r include = FALSE}
  options(width = 100)
  knitr::opts_knit$set(global.par = TRUE)
  knitr::opts_chunk$set(collapse = TRUE, fig.dim = c(6, 6),
                        out.width = "45%", fig.align = 'center')
```
```{r include = FALSE}
  par(bg="transparent")
  par(mar=c(2,2,2,2))
  par(mfrow=c(1,1))
```

# Kernel Ridge Regression

## Linear Regression as a Constraint Optimization

This example is motivated from an alternative derivation provided by Prof. Max Welling on his kernel ridge regression [lecture note](https://web2.qatar.cmu.edu/~gdicaro/10315-Fall19/additional/welling-notes-on-kernel-ridge.pdf). This understanding would utilize a primal-dual derivation, which will also be used in SVM. This enables us to switch things to the kernel version through __kernel trick__. Consider a linear regression

\[
\underset{\bbeta}{\text{minimize}} \,\, \frac{1}{n} \lVert \by - \bX \bbeta \rVert^2 + \lambda \lVert \bbeta \rVert^2
\]

Introduce a new set of variables 

\[
z_i = y_i - \bx_i^\T \bbeta,
\]

for $i = 1, \ldots, n$. Then The original problem becomes 

\begin{align}
\underset{\bbeta, \bz}{\text{minimize}} \quad & \frac{1}{2n\lambda} \lVert \bz \rVert^2 + \frac{1}{2} \lVert \bbeta \rVert^2 \nonumber \\
\text{subj. to} \quad & z_i = y_i - \bx_i^\T \bbeta, \,\, i = 1, \ldots, n.
\end{align}

For solving a constrained optimization problem, we utilize the Lagrangian

\[
\cL(\balpha; \bz, \bbeta) = \frac{1}{2n\lambda} \lVert \bz \rVert^2 + \frac{1}{2} \lVert \bbeta \rVert^2 + \sum_{i=1}^n \alpha_i (y_i - \bx_i^\T \bbeta - z_i)
\]

with $\alpha_i \in \RR$. and our problem becomes 

\[
\underset{\bz, \bbeta}{\min} \underset{\balpha}{\max} \cL(\balpha; \bz, \bbeta)
\]

The logic here for the Lagrangian is that if we first maximize w.r.t. $\balpha$, any solution that violates the constraint would give negative infinity of the Lagrangian. Hence, the outside maximization w.r.t. $\bz$ and $\bbeta$ would never those values. This is called the __primal form__ and by swamping the two optimization, we get the __dual form__

\[
\underset{\balpha}{\max} \underset{\bz, \bbeta}{\min} \cL(\balpha; \bz, \bbeta)
\]

Note that the dual form is always a lower bound of the primal form. Under some [regularity conditions](https://en.wikipedia.org/wiki/Duality_(optimization)), they are equal. To proceed with the optimization, we first minimize w.r.t. $\bbeta$ and $\bz$ by taking the derivative and set them to zero.

\begin{align}
\frac{\partial \cal L}{\partial z_i} =&\, \frac{1}{n\lambda}z_i - \alpha_i = 0, \quad \text{for} \,\, i = 1, \ldots, n, \nonumber \\
\text{and}\,\, \frac{\partial \cL}{\partial \bbeta} =&\, \bbeta - \sum_{i=1}^n \alpha_i \bx_i = \mathbf{0}
\end{align}

Hence, we have, the estimated $\widehat{\bbeta}$ is $\sum_{i=1}^n \alpha_i \bx_i$ that matches our previous understanding. Also, if we view the inner product as a linear kernel $K(x_1, x_2) = x_1^\T x_2$, the predicted value of at $\bx$ is 

\begin{align}
f(\bx) =& \,\, \bx^\T \bbeta \nonumber \\
=& \sum_{i=1}^n \alpha_i \bx^\T \bx_i \nonumber \\
=& \sum_{i=1}^n \alpha_i \langle\bx, \bx_i \rangle \nonumber \\
=& \sum_{i=1}^n \alpha_i K(\bx, \bx_i).
\end{align}

Now, to complete our dual solution, we plugin these results, and have 

\begin{align}
\underset{\balpha}{\max} \underset{\bz, \bbeta}{\min} {\cal L} =& \frac{n\lambda}{2} \balpha^\T \balpha + \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j x_i^\T x_j + \sum_{j} \alpha_j \big(y_j - x_j^\T \sum_i \alpha_i \bx_i - n\lambda \alpha_i \big) \nonumber \\
 =& - \frac{n\lambda}{2} \balpha^\T \balpha - \frac{1}{2} \sum_{i, j} \alpha_i \alpha_j K(x_i, x_j) + \sum_{i} \alpha_i y_i \nonumber \\
=& - \frac{n\lambda}{2} \balpha^\T \balpha - \frac{1}{2} \balpha^\T \bK \balpha + \balpha^\T \by
\end{align}

By again taking derivative w.r.t. $\alpha$, we have

$$ - n\lambda \bI \balpha - \bK \balpha + \by = \mathbf{0},$$
and the solution is the same as what we had before 

$$\balpha = (\bK + n\lambda \bI)^{-1} \by$$

## The Kernel Ridge Regression

With our understandings of the RKHS and the representer theorem (introduced in the next Chapter), we can say that for any regression function models, if we want the solution to be more flexible, we may solve it within a RKHS. This is essentially change the linear kernel previously into any other kernel function. In general, we consider the following regression problem:

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \sum_{i=1}^n \Big(y_i - f(x_i) \Big)^2 + \lambda \lVert f \rVert_\cH^2$$
Since we know that the solution has to take the form 

$$\widehat f = \sum_{i=1}^n \alpha_i K(x_i, \cdot),$$
we can instead solve the problem as a ridge regression type of problem:

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \big\lVert \by - \bK \balpha \big\rVert^2 + \lambda \lVert f \rVert_\cH^2,$$
where $\bK$ is an $n \times n$ matrix with $K(x_i, x_j)$ at its $(i,j)$th element. With some simple calculation, we also have 

\begin{align}
\lVert f \rVert_\cH^2 =& \langle f, f \rangle \nonumber \\
=& \langle \sum_{i=1}^n \alpha_i K(x_i, \cdot), \sum_{j=1}^n \alpha_j K(x_j, \cdot) \rangle \nonumber \\
=& \sum_{i, j} \alpha_i \alpha_j \big\langle K(x_i, \cdot), K(x_j, \cdot) \big\rangle \nonumber \\
=& \sum_{i, j} \alpha_i \alpha_j K(x_i, x_j) \nonumber \\
=& \balpha^\T \bK \balpha
\end{align}

Hence, the problem becomes 

$$\widehat f = \underset{f \in \cH}{\arg\min} \,\, \frac{1}{n} \big\lVert \by - \bK \balpha \big\rVert^2 + \lambda \balpha^\T \bK \balpha.$$
By taking the derivative with respect to $\balpha$, we have (note that $\bK$ is symmetric),

\begin{align}
-\frac{1}{n} \bK^\T (\by - \bK \balpha) + \lambda \bK \balpha \overset{\text{set}}{=} \mathbf{0} \nonumber \\
\bK (- \by + \bK \balpha + n\lambda \balpha) = \mathbf{0}.
\end{align}
This implies 

$$ \balpha = (\bK + n\lambda \bI)^{-1} \by.$$
and we obtained the same solution. 

## Ridge Regression as a Linear Kernel Model

When $K(\bx_i, \bx_j) = \bx_i^\T \bx_j$, we also have $\bK = \bX \bX^\T$. We should expect this to match the original ridge regression since this is essentially a linear regression. First, plug this into our previous result, we have 

$$ \balpha = (\bX \bX^\T + n\lambda \bI)^{-1} \by.$$
and the fitted value is 

$$ \widehat{\by} = \bK \balpha = \bX \bX^\T (\bX \bX^\T + n\lambda \bI)^{-1} \by$$
Using a matrix identity $(\bP \bQ + \bI)^{-1}\bP = \bP (\bQ \bP + \bI)^{-1}$, and let $\bQ = \bX = \bP^\T$, we have 

$$ \widehat{\by} = \bK \balpha = \bX (\bX^\T \bX + n\lambda \bI)^{-1} \bX^\T \by$$
and 

$$ \widehat{\by} = \bX \underbrace{\big[ \bX^\T \balpha \big]}_{\bbeta} = \bX \underbrace{\big[ (\bX^\T \bX + n\lambda \bI)^{-1} \bX^\T \by \big]}_{\bbeta}$$


which is simply the Ridge regression solution, and also the corresponding linear regression solution $\widehat{\bbeta} = \bX^\T \widehat{\balpha}$. This makes the penalty term $\balpha^\T \bK \balpha = \balpha^\T \bX \bX^\T \balpha = \bbeta^\T \bbeta$, which maps every thing back to the ridge regression form. In this example, we can see that the functional form of our estimation is 

\begin{align}
\widehat f(\cdot) &= \cdot^\T \widehat{\bbeta} \\
&= \cdot^\T \bX^\T \widehat{\balpha} \\
&= \sum_{i=1}^n \widehat{\alpha}_i \langle \cdot, \bx_i \rangle \\
\end{align}

which falls into the span of the linear kernel functions $\langle \cdot, \bx_i \rangle$.


